<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
    <title>GCP Architect Quiz (10 Question Demo)</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet"/>
    <style>
        /* --- New Light Theme --- */
        @keyframes gradient-animation {
            0% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
            100% { background-position: 0% 50%; }
        }

        html, body {
            height: 100%;
        }

        body {
            font-family: 'Inter', sans-serif;
            background-color: #f8fafc; /* Light Slate Background */
            background-image: radial-gradient(at 27% 37%, hsla(215, 98%, 61%, 0.15) 0px, transparent 50%),
                              radial-gradient(at 97% 21%, hsla(145, 98%, 61%, 0.15) 0px, transparent 50%),
                              radial-gradient(at 52% 99%, hsla(355, 98%, 61%, 0.15) 0px, transparent 50%),
                              radial-gradient(at 10% 29%, hsla(256, 96%, 61%, 0.15) 0px, transparent 50%),
                              radial-gradient(at 97% 96%, hsla(38, 60%, 62%, 0.15) 0px, transparent 50%),
                              radial-gradient(at 33% 50%, hsla(222, 67%, 73%, 0.15) 0px, transparent 50%),
                              radial-gradient(at 79% 53%, hsla(343, 68%, 79%, 0.15) 0px, transparent 50%);
            background-size: 400% 400%;
            animation: gradient-animation 15s ease infinite;
            color: #1e293b; /* Dark Slate Text */
            display: flex;
            flex-direction: column;
        }

        /* --- Light Card Style --- */
        .card {
            background-color: rgba(255, 255, 255, 0.8);
            backdrop-filter: blur(16px) saturate(180%);
            -webkit-backdrop-filter: blur(16px) saturate(180%);
            border: 1px solid rgba(0, 0, 0, 0.05);
            box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.1);
        }

        /* --- Header --- */
        #main-header h1 {
            color: #0f172a; /* Darker Slate for Heading */
        }
        #main-header p {
            color: #475569; /* Medium Slate for Subheading */
        }

        /* --- Light Quiz Option Styles --- */
        .quiz-option {
            transition: all 0.25s ease-in-out;
            cursor: pointer;
            border: 2px solid #e2e8f0; /* Light Gray Border */
            background-color: #ffffff;
        }
        .quiz-option:hover {
            transform: translateY(-4px) scale(1.02);
            border-color: #3b82f6; /* Blue-500 */
            box-shadow: 0 0 20px rgba(59, 130, 246, 0.15);
        }
        .quiz-option.selected {
            background-color: #eff6ff; /* Blue-50 */
            border-color: #2563eb; /* Blue-600 */
        }
        .quiz-option.correct {
            background-color: rgba(34, 197, 94, 0.1);
            border-color: #16a34a; /* Green-600 */
        }
        .quiz-option.incorrect {
            background-color: rgba(239, 68, 68, 0.1);
            border-color: #ef4444; /* Red-500 */
        }
        .status-icon {
            width: 24px;
            height: 24px;
        }

        /* --- Feedback Card --- */
        .feedback-card {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.5s ease-in-out, padding 0.5s ease-in-out, margin 0.5s ease-in-out, opacity 0.5s ease-in-out;
            opacity: 0;
        }
        .feedback-card.show {
            max-height: 800px;
            padding-top: 1.5rem;
            padding-bottom: 1.5rem;
            margin-top: 1.5rem;
            opacity: 1;
        }

        /* --- State for Answered Questions --- */
        .answered .quiz-option:not(.selected):not(.correct) {
            cursor: not-allowed;
            opacity: 0.6;
            filter: grayscale(50%);
        }
        .answered .quiz-option:hover {
            transform: none;
            box-shadow: none;
        }

        /* --- Light Button Styles --- */
        .btn {
            transition: all 0.2s ease-in-out;
            border-radius: 0.5rem;
            font-weight: 600;
            border: 1px solid transparent;
        }
        .btn:hover {
            transform: translateY(-2px) scale(1.02);
        }
        .btn:active {
            transform: translateY(-1px) scale(1.0);
        }
        .btn-primary {
            background-image: linear-gradient(to right, #3b82f6, #6366f1); /* Blue to Indigo */
            color: white;
            box-shadow: 0 4px 15px rgba(59, 130, 246, 0.2);
        }
        .btn-primary:hover {
             filter: brightness(1.1);
             box-shadow: 0 0 25px rgba(59, 130, 246, 0.4);
        }
        .btn-secondary {
            background-color: #e5e7eb; /* Gray-200 */
            color: #374151; /* Gray-700 */
            border-color: #d1d5db; /* Gray-300 */
        }
        .btn-secondary:hover {
            background-color: #d1d5db; /* Gray-300 */
            border-color: #9ca3af; /* Gray-400 */
        }
        .btn-danger {
            background-color: #fca5a5; /* Red-300 */
            color: #991b1b; /* Red-800 */
            border-color: #f87171; /* Red-400 */
        }
        .btn-danger:hover {
            background-color: #f87171; /* Red-400 */
            color: #7f1d1d; /* Red-900 */
        }

        /* --- Light Progress Bar --- */
        #progress-container {
            width: 100%;
            background-color: #e5e7eb; /* Gray-200 */
            border-radius: 9999px;
            height: 12px;
            box-shadow: inset 0 1px 3px rgba(0,0,0,0.1);
        }
        #progress-bar {
            background-image: linear-gradient(to right, #3b82f6, #8b5cf6);
            height: 100%;
            border-radius: 9999px;
            transition: width 0.4s ease-in-out;
            box-shadow: 0 0 10px rgba(59, 130, 246, 0.4);
        }
        
        /* --- Light Results & Review --- */
        #score-circle {
            background: conic-gradient(from 180deg at 50% 50%, #f3f4f6 0%, #ffffff 100%);
            border-color: #e5e7eb;
        }
        #score-text {
            color: #1d4ed8; /* Blue-700 */
        }
        .review-option {
            border-left-width: 4px;
            padding: 0.75rem;
            border-radius: 0.25rem;
            margin-top: 0.5rem;
        }
        .review-option-correct {
            background-color: rgba(34, 197, 94, 0.08);
            border-color: #16a34a;
        }
        .review-option-incorrect {
            background-color: rgba(239, 68, 68, 0.08);
            border-color: #ef4444;
        }
        
        /* --- Scrolling Fixes --- */
        #review-content {
            max-height: 65vh; 
            overflow-y: auto;
            padding-right: 1rem; 
        }

        .feedback-explanations {
            max-height: 300px;
            overflow-y: auto;
            padding-right: 0.5rem;
        }

        /* Custom scrollbar for webkit browsers */
        #review-content::-webkit-scrollbar, .feedback-explanations::-webkit-scrollbar {
            width: 8px;
        }

        #review-content::-webkit-scrollbar-track, .feedback-explanations::-webkit-scrollbar-track {
            background: #e2e8f0;
            border-radius: 10px;
        }

        #review-content::-webkit-scrollbar-thumb, .feedback-explanations::-webkit-scrollbar-thumb {
            background-color: #94a3b8;
            border-radius: 10px;
            border: 2px solid #e2e8f0;
        }

        #review-content::-webkit-scrollbar-thumb:hover, .feedback-explanations::-webkit-scrollbar-thumb:hover {
            background-color: #64748b;
        }
    </style>
</head>
<body class="text-slate-800">

<div class="container mx-auto p-4 sm:p-6 lg:p-8 max-w-6xl flex flex-col flex-grow">
    <header class="text-center mb-8" id="main-header">
        <h1 class="text-3xl sm:text-4xl font-extrabold flex items-center justify-center">
            <span class="inline-block align-middle w-10 h-10 mr-3">
                <svg fill="none" viewBox="0 0 108 72" xmlns="http://www.w3.org/2000/svg">
                    <path d="M85.333 29.333c-2.07-12.2-12.72-21.333-25.333-21.333-8.107 0-15.307 3.84-19.893 9.76C37.333 17.013 33.813 16 30 16c-8.827 0-16 7.173-16 16 0 .613.053.933.16.1.213-3.947 3.52-7.1 7.547-7.1h1.76c.693 0 1.333.32 1.76.853.48-5.387 5.12-9.547 10.773-9.547 6.187 0 11.2 4.907 11.307 11.093.053.853.747 1.547 1.6 1.547h.427c5.44 0 9.866 4.427 9.866 9.867 0 5.173-4.053 9.493-9.28 9.813H37.333c-6.627 0-12 5.373-12 12s5.373 12 12 12h48c9.707 0 17.333-7.947 17.333-17.333 0-9.28-7.253-16.853-16.333-17.333z" fill="#4285F4"></path>
                    <path d="M37.333 53.333c-2.293 0-4.373-1.013-5.813-2.613-1.494-1.6-2.454-3.84-2.454-6.054 0-4.906 3.947-8.853 8.854-8.853h48c4.906 0 8.853 3.947 8.853 8.853 0 4.907-3.947 8.854-8.853 8.854H37.333z" fill="#1A73E8"></path>
                    <path d="M30 23c-4.97 0-9 4.03-9 9s4.03 9 9 9h6.667c.107-1.386.427-2.72.96-4-1.813-.64-3.093-2.4-3.093-4.453 0-2.614 2.133-4.747 4.747-4.747h1.066c.214-3.84 3.467-6.8 7.334-6.8 3.413 0 6.346 2.347 7.146 5.493C55.467 25.12 56.693 24 58 24c3.307 0 6 2.693 6 6 0 .693-.107 1.333-.32 1.973.853.213 1.6.64 2.24 1.173-1.493-4.48-5.653-7.813-10.587-7.813-4.213 0-7.946 2.56-9.653 6.24C44.427 26.133 40.853 23 36.667 23H30z" fill="#E8F0FE"></path>
                </svg>
            </span>
            GCP Architect Quiz üß†
        </h1>
        <p class="mt-2 text-lg">Test your knowledge and prepare for certification!</p>
    </header>

    <main class="flex-grow">
        <!-- The quiz content will be dynamically inserted here by script.js -->
        <div class="card p-6 sm:p-8 rounded-2xl" id="quiz-container">
        </div>

        <!-- Navigation controls for the quiz -->
        <div class="mt-8 flex items-center justify-center gap-4" id="navigation-container">
            <button class="btn btn-secondary py-2 px-4 disabled:opacity-50 disabled:cursor-not-allowed disabled:transform-none" id="prev-btn">Previous</button>
            <div class="flex-grow text-center px-4">
                <div id="progress-container">
                    <div id="progress-bar" style="width: 0%;"></div>
                </div>
                <div class="text-sm font-medium text-slate-500 mt-2" id="progress-indicator">
                    Question <span id="current-q-span">1</span> of <span id="total-q-span">10</span>
                </div>
            </div>
            <button class="btn btn-primary py-2 px-4" id="next-btn">Next</button>
            <button class="btn btn-danger py-2 px-4 ml-4" id="finish-btn">Finish</button>
        </div>

        <!-- Results screen, hidden by default -->
        <div class="hidden text-center card p-8 rounded-2xl" id="results-container">
            <h2 class="text-3xl font-bold mb-4">üèÜ Quiz Complete! üèÜ</h2>
            <p class="text-lg text-slate-700 mb-2">Your Final Score:</p>
            <div class="relative w-40 h-40 mx-auto flex items-center justify-center rounded-full mb-6 border-4" id="score-circle">
                <span class="text-5xl font-extrabold" id="score-text"></span>
            </div>
            <p class="text-slate-600 text-lg mb-8" id="score-summary"></p>
            <div class="flex flex-col sm:flex-row justify-center gap-4">
                <button class="btn btn-secondary py-3 px-6" id="review-all-btn">Review All Answers</button>
                <button class="btn btn-primary py-3 px-6" id="review-focused-btn">Focused Review</button>
                <button class="btn btn-secondary py-3 px-6" id="restart-btn">Restart Quiz</button>
            </div>
        </div>

        <!-- Review screen, hidden by default -->
        <div class="hidden card p-6 sm:p-8 rounded-2xl" id="review-container">
            <h2 class="text-3xl font-bold mb-6 text-center" id="review-title">üìã Answer Review</h2>
            <div class="space-y-8" id="review-content">
                <!-- Review content will be dynamically inserted here -->
            </div>
            <div class="text-center mt-8">
                <button class="btn btn-primary py-2 px-4" id="back-to-score-btn">Back to Score</button>
            </div>
        </div>
    </main>
</div>

<footer class="text-center p-4 mt-4">
    <p class="text-sm text-slate-500">Made with ‚ù§Ô∏è by Arjun</p>
</footer>

<script>
    // --- DATA (First 10 Questions) ---
   const quizData = [
        {
            "title": "Q1: API Versioning (Same DNS/SSL)",
            "description": "Your company has decided to make a major revision of their API in order to create better experiences for their developers. They need to keep the old version of the API available and deployable, while allowing new customers and testers to try out the new API. They want to keep the same SSL and DNS records in place to serve both APIs. What should they do?",
            "options": [
                "Configure a new load balancer",
                "Reconfigure old clients",
                "Have the old API forward traffic to the new API based on the path",
                "Use separate backend pools for each API path behind the load balancer"
            ],
            "correctAnswer": "Use separate backend pools for each API path behind the load balancer",
            "feedback": {
                "Configure a new load balancer": "Incorrect. This would require a new IP address, which in turn would necessitate new DNS/SSL records, violating a key requirement.",
                "Reconfigure old clients": "Incorrect. This is not a scalable or practical solution and would cause significant disruption to existing users of the old API.",
                "Have the old API forward traffic to the new API based on the path": "Incorrect. This approach is risky as it creates a dependency between the two API versions, making independent deployments difficult and increasing the chance of cascading failures.",
                "Use separate backend pools for each API path behind the load balancer": "Correct! An HTTP(S) Load Balancer supports path-based routing. You can configure it to send traffic for '/v1/*' to the old backend pool and '/v2/*' to the new one, all while using the same DNS name and SSL certificate."
            }
        },
        {
            "title": "Q2: Data Storage for Scalable SQL Analysis",
            "description": "Your company plans to migrate a multi-petabyte data set to the cloud. The data set must be available 24 hours a day. Your business analysts have experience only with using a SQL interface. How should you store the data to optimize it for ease of analysis?",
            "options": [
                "Load data into Google BigQuery",
                "Insert data into Google Cloud SQL",
                "Put flat files into Google Cloud Storage",
                "Stream into Google Cloud Datastore"
            ],
            "correctAnswer": "Load data into Google BigQuery",
            "feedback": {
                "Load data into Google BigQuery": "Correct! BigQuery is Google's serverless, petabyte-scale data warehouse, specifically designed for running fast SQL queries over massive datasets. This perfectly matches the requirements.",
                "Insert data into Google Cloud SQL": "Incorrect. Cloud SQL is a managed relational database for transactional (OLTP) workloads and does not scale to the multi-petabyte level required for this kind of analysis.",
                "Put flat files into Google Cloud Storage": "Incorrect. While Cloud Storage is excellent for storing large amounts of data, it is an object store and does not offer a direct SQL interface for analysis.",
                "Stream into Google Cloud Datastore": "Incorrect. Datastore is a NoSQL document database. It does not use a SQL interface and is not designed for petabyte-scale analytical workloads."
            }
        },
        {
            "title": "Q3: Migrating a J2EE Application - Recommended Practices",
            "description": "The operations manager asks you for a list of recommended practices that she should consider when migrating a J2EE application to the cloud. Which three practices should you recommend? (Choose three.)",
            "type": "CHECKBOX",
            "options": [
                "Port the application code to run on Google App Engine",
                "Integrate Cloud Dataflow into the application to capture real-time metrics",
                "Instrument the application with a monitoring tool like Stackdriver Debugger",
                "Select an automation framework to reliably provision the cloud infrastructure",
                "Deploy a continuous integration tool with automated testing in a staging environment",
                "Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable"
            ],
            "correctAnswers": [
                "Instrument the application with a monitoring tool like Stackdriver Debugger",
                "Select an automation framework to reliably provision the cloud infrastructure",
                "Deploy a continuous integration tool with automated testing in a staging environment"
            ],
            "feedback": {
                "Port the application code to run on Google App Engine": "Incorrect. Porting to App Engine is a specific re-platforming strategy. It may not be the best initial practice for all J2EE applications and is not a universal recommendation.",
                "Integrate Cloud Dataflow into the application to capture real-time metrics": "Incorrect. Cloud Dataflow is a data processing service for ETL pipelines, not a tool for capturing application performance metrics.",
                "Instrument the application with a monitoring tool like Stackdriver Debugger": "Correct! Gaining visibility into application performance, logs, and errors is a fundamental best practice for operating any application, especially in a new cloud environment.",
                "Select an automation framework to reliably provision the cloud infrastructure": "Correct! Using Infrastructure as Code (IaC) tools like Terraform or Deployment Manager is a core DevOps practice that ensures infrastructure is provisioned consistently and reliably.",
                "Deploy a continuous integration tool with automated testing in a staging environment": "Correct! A CI/CD pipeline is essential for improving release velocity, ensuring code quality through automated testing, and enabling reliable deployments to the cloud.",
                "Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable": "Incorrect. This represents a major architectural change (re-architecting). It's a significant undertaking and not a general best practice for an initial cloud migration."
            }
        },
        {
            "title": "Q4: App Engine Session State Issue",
            "description": "Your news feed web service has the following code running on Google App Engine. During peak load, users report that they can see news articles they already viewed. (Code shows sessions = {} global variable). What is the most likely cause of this problem?",
            "options": [
                "The session variable is local to just a single instance",
                "The session variable is being overwritten in Cloud Datastore",
                "The URL of the API needs to be modified to prevent caching",
                "The HTTP Expires header needs to be set to -1 stop caching"
            ],
            "correctAnswer": "The session variable is local to just a single instance",
            "feedback": {
                "The session variable is local to just a single instance": "Correct! App Engine automatically scales by creating multiple instances. A global variable in code exists only in the memory of a single instance. When a user's subsequent requests are served by different instances, the session state is lost, causing inconsistent behavior.",
                "The session variable is being overwritten in Cloud Datastore": "Incorrect. The problem description and code snippet do not indicate any use of Cloud Datastore for session management.",
                "The URL of the API needs to be modified to prevent caching": "Incorrect. The issue stems from inconsistent server-side state, not from client-side or CDN caching of responses.",
                "The HTTP Expires header needs to be set to -1 stop caching": "Incorrect. This is a client-side caching control. The problem is with server-side session management across multiple instances."
            }
        },
        {
            "title": "Q5: Logging Tool Assessment",
            "description": "An application development team believes their current logging tool will not meet their needs for their new cloud-based product. They want a better tool to capture errors and help them analyze their historical log data. You want to help them find a solution that meets their needs. What should you do?",
            "options": [
                "Direct them to download and install the Google StackDriver logging agent",
                "Send them a list of online resources about logging best practices",
                "Help them define their requirements and assess viable logging tools",
                "Help them upgrade their current tool to take advantage of any new features"
            ],
            "correctAnswer": "Help them define their requirements and assess viable logging tools",
            "feedback": {
                "Direct them to download and install the Google StackDriver logging agent": "Incorrect. This is jumping to a solution without understanding the problem. The first step should always be to understand the requirements.",
                "Send them a list of online resources about logging best practices": "Incorrect. While helpful, this is passive and doesn't actively help the team solve their specific problem. A Cloud Architect should be a more active partner.",
                "Help them define their requirements and assess viable logging tools": "Correct! This is the most appropriate first step. As a Cloud Architect, you should facilitate a discussion to understand their specific needs (e.g., query language, retention period, alert mechanisms) before recommending or assessing any tool.",
                "Help them upgrade their current tool to take advantage of any new features": "Incorrect. This assumes their current tool is the right choice, but the team has already stated they believe it will not meet their needs."
            }
        },
        {
            "title": "Q6: Compute Engine Instance Migration",
            "description": "Your company wants to move a development application from an on-premises server to Google Cloud. The application runs on a Linux server and stores data in a MySQL database. The application is a three-tier web application, with the web server, application server, and database server each running on a separate virtual machine. The application will be used by 10 developers. You need to migrate the virtual machines (VMs) and establish connectivity between them. What should you do?",
            "options": [
                "Use Compute Engine for each server, and deploy a global load balancer.",
                "Use Compute Engine for each server, and deploy a Cloud VPN.",
                "Use Compute Engine for each server, and create a firewall rule to allow traffic between VMs.",
                "Use Compute Engine for each server, and set up a VPC Network and Firewall rules."
            ],
            "correctAnswer": "Use Compute Engine for each server, and set up a VPC Network and Firewall rules.",
            "feedback": {
                "Use Compute Engine for each server, and deploy a global load balancer.": "Incorrect. A global load balancer is for distributing external traffic, not for establishing basic internal connectivity between VMs in a three-tier application.",
                "Use Compute Engine for each server, and deploy a Cloud VPN.": "Incorrect. A Cloud VPN is used to connect your on-premises network to your GCP VPC. It is not used for communication between VMs within the same VPC.",
                "Use Compute Engine for each server, and create a firewall rule to allow traffic between VMs.": "Incorrect. While you do need firewall rules, you first need the foundational network for them to exist in. This answer is incomplete.",
                "Use Compute Engine for each server, and set up a VPC Network and Firewall rules.": "Correct! This is the complete solution. You need to create VMs on Compute Engine, place them in a VPC Network for logical isolation, and then create Firewall Rules to control the traffic flow between the tiers (e.g., allow the web server to talk to the application server)."
            }
        },
        {
            "title": "Q7: Choosing a Database for Financial Transactions",
            "description": "Your company is developing an application that stores highly sensitive financial transactions. These transactions need to be stored in a database that offers strong ACID compliance. You need to select a database to meet this requirement. Which database should you use?",
            "options": [
                "Google Cloud Datastore",
                "Google Cloud Bigtable",
                "Google Cloud Spanner",
                "Google Cloud Firestore in Native Mode"
            ],
            "correctAnswer": "Google Cloud Spanner",
            "feedback": {
                "Google Cloud Datastore": "Incorrect. This is a NoSQL database and does not offer the strong, global ACID compliance required for sensitive financial transactions.",
                "Google Cloud Bigtable": "Incorrect. This is a NoSQL wide-column store designed for high-throughput analytical workloads, not for transactional ACID compliance.",
                "Google Cloud Spanner": "Correct! Cloud Spanner is a globally distributed, relational database that provides strong, transactional (ACID) consistency at scale. This is its key differentiator and makes it ideal for financial applications.",
                "Google Cloud Firestore in Native Mode": "Incorrect. This is a NoSQL document database. While it supports transactions on documents and batches, it does not provide the global, strong ACID guarantees of a relational system like Spanner."
            }
        },
        {
            "title": "Q8: Automating VM Configuration After Deployment",
            "description": "Your company needs to deploy a large number of virtual machines (VMs) to Google Cloud. These VMs must run a specific set of configurations and software after they are deployed. You need to manage the configuration consistently and efficiently. What should you do?",
            "options": [
                "Write a custom deployment script and run it manually on each VM.",
                "Use a startup script on each VM to configure it after deployment.",
                "Create a custom VM image that includes all required configurations and software.",
                "Develop a Kubernetes Deployment to manage the VM configuration."
            ],
            "correctAnswer": "Create a custom VM image that includes all required configurations and software.",
            "feedback": {
                "Write a custom deployment script and run it manually on each VM.": "Incorrect. This is not efficient or consistent for a large number of VMs and is prone to human error.",
                "Use a startup script on each VM to configure it after deployment.": "Incorrect. While possible, this adds time to every VM's boot process and is less robust than having a pre-configured image. It's better for small, dynamic changes.",
                "Create a custom VM image that includes all required configurations and software.": "Correct! Creating a 'golden image' is the best practice. It ensures that every VM deployed from the image is identical and ready to go, leading to faster startup times and guaranteed consistency.",
                "Develop a Kubernetes Deployment to manage the VM configuration.": "Incorrect. Kubernetes is an orchestrator for containers, not for managing the configuration of individual virtual machines."
            }
        },
        {
            "title": "Q9: High Availability for Web Application Backend",
            "description": "Your company has a multi-tier web application running on Compute Engine. The backend application servers are deployed across multiple zones in a region using managed instance groups. You need to ensure the application remains highly available even if an entire zone becomes unavailable. What should you do?",
            "options": [
                "Configure a regional managed instance group with autoscaling.",
                "Configure an HTTP(S) Load Balancer in front of the managed instance groups.",
                "Configure a health check on the managed instance groups and a Load Balancer.",
                "Configure the application with an internal load balancer across multiple zones."
            ],
            "correctAnswer": "Configure an HTTP(S) Load Balancer in front of the managed instance groups.",
            "feedback": {
                "Configure a regional managed instance group with autoscaling.": "Incorrect. A regional MIG is a crucial part of the HA setup, but it doesn't provide a single entry point or distribute external traffic. It's the backend, not the complete solution.",
                "Configure an HTTP(S) Load Balancer in front of the managed instance groups.": "Correct! The External HTTP(S) Load Balancer provides a single, global IP address. It distributes traffic to healthy instances across multiple zones and will automatically stop sending traffic to a zone if it becomes unavailable, thus ensuring high availability.",
                "Configure a health check on the managed instance groups and a Load Balancer.": "Incorrect. Health checks are a necessary component that the load balancer uses to determine instance health, but they are not the overall solution for providing high availability.",
                "Configure the application with an internal load balancer across multiple zones.": "Incorrect. An internal load balancer is for traffic within your VPC, not for handling external user traffic to a web application."
            }
        },
        {
            "title": "Q10: Database for Financial Transactions with Auditing",
            "description": "Your company stores highly sensitive financial transactions. These transactions need to be stored in a database that offers strong ACID compliance. Additionally, the database must support auditing to track changes to the data. Which database should you use?",
            "options": [
                "Google Cloud Datastore",
                "Google Cloud Bigtable",
                "Google Cloud Spanner",
                "Google Cloud Firestore in Native Mode"
            ],
            "correctAnswer": "Google Cloud Spanner",
            "feedback": {
                "Google Cloud Datastore": "Incorrect. NoSQL databases like Datastore do not provide the strong, global ACID compliance needed for rigorous financial transaction systems.",
                "Google Cloud Bigtable": "Incorrect. Bigtable is a NoSQL database designed for large-scale analytical and operational workloads, not for ACID-compliant transactions.",
                "Google Cloud Spanner": "Correct! Spanner is the only service listed that provides global strong consistency and ACID transactions. Auditing can be implemented by exporting audit logs or using features like change streams to track data modifications.",
                "Google Cloud Firestore in Native Mode": "Incorrect. As a NoSQL database, Firestore lacks the comprehensive ACID guarantees required for this level of financial application."
            }
        },
        {
            "title": "Q11: Selecting a Storage for Analytics Workload",
            "description": "Your company has a large collection of unstructured data (images, videos, documents). They want to store this data in Google Cloud and make it available for future analysis by data scientists. The data volume is expected to grow significantly. Which storage solution should you choose?",
            "options": [
                "Google Cloud Storage (Standard storage class)",
                "Google Cloud Storage (Nearline storage class)",
                "Google Cloud Filestore",
                "Google Cloud SQL"
            ],
            "correctAnswer": "Google Cloud Storage (Standard storage class)",
            "feedback": {
                "Google Cloud Storage (Standard storage class)": "Correct! Cloud Storage is the ideal place for large volumes of unstructured data. The Standard class is best for data that will be accessed frequently, which is typical for data being actively used for analysis.",
                "Google Cloud Storage (Nearline storage class)": "Incorrect. Nearline is for data accessed less than once a month. While cheaper for storage, the retrieval costs would be higher for an active analytics workload.",
                "Google Cloud Filestore": "Incorrect. This is a managed file storage (NFS) service. It's not as scalable or cost-effective for petabyte-scale object storage as Cloud Storage.",
                "Google Cloud SQL": "Incorrect. This is a relational database for structured data, not suitable for storing unstructured files like images and videos."
            }
        },
        {
            "title": "Q12: Network Design for On-Prem to Cloud Migration (VPC)",
            "description": "Your company plans to migrate its on-premises data center to Google Cloud. They want to ensure seamless network connectivity between their existing on-premises network and their new Google Cloud resources. They need a logically isolated, high-performance network in the cloud. Which network construct should you set up in Google Cloud?",
            "options": [
                "Shared VPC",
                "VPC Network",
                "Firewall Rules",
                "VPN Gateway"
            ],
            "correctAnswer": "VPC Network",
            "feedback": {
                "Shared VPC": "Incorrect. Shared VPC is a feature that allows multiple projects to use a common VPC network, but the fundamental construct is the VPC Network itself.",
                "VPC Network": "Correct! A Virtual Private Cloud (VPC) Network is the foundational, logically isolated private network space within Google Cloud where you deploy your resources.",
                "Firewall Rules": "Incorrect. Firewall rules control traffic within a VPC, but they don't create the network itself.",
                "VPN Gateway": "Incorrect. A VPN Gateway is a component used to connect an existing network to a VPC; it is not the network itself."
            }
        },
        {
            "title": "Q13: Cloud Cost Optimization for Compute Engine",
            "description": "Your company is running several non-critical Compute Engine instances that process batch jobs. These jobs can tolerate some interruptions and have flexible start/stop times. You want to optimize costs for these instances. What type of Compute Engine instance should you recommend?",
            "options": [
                "Standard instances",
                "Preemptible instances",
                "Custom machine types",
                "Sole-tenant nodes"
            ],
            "correctAnswer": "Preemptible instances",
            "feedback": {
                "Standard instances": "Incorrect. These are the default, full-priced instances and do not offer specific cost optimization for interruptible workloads.",
                "Preemptible instances": "Correct! Preemptible VMs (PVMs) offer massive cost savings (up to 80% discount) in exchange for the possibility of being shut down (preempted) by Google. This makes them perfect for fault-tolerant and interruptible batch jobs.",
                "Custom machine types": "Incorrect. Custom machine types help optimize costs by letting you choose the exact amount of vCPU and memory, but they don't provide the deep discounts that PVMs do for interruptible tasks.",
                "Sole-tenant nodes": "Incorrect. These are significantly more expensive and are used for workloads with specific compliance or licensing requirements that need dedicated physical hardware."
            }
        },
        {
            "title": "Q14: Securing Access to Cloud Storage (Specific Users)",
            "description": "Your company wants to grant specific individuals in their data science team read-only access to a Google Cloud Storage bucket containing sensitive research data. You need to ensure access is tightly controlled and follows the principle of least privilege. What is the most secure way to grant this access?",
            "options": [
                "Make the bucket publicly readable",
                "Grant the roles/storage.objectViewer role to individual users",
                "Grant the roles/storage.admin role to the data science team's service account",
                "Set up signed URLs for each object they need to access"
            ],
            "correctAnswer": "Grant the roles/storage.objectViewer role to individual users",
            "feedback": {
                "Make the bucket publicly readable": "Incorrect. This is extremely insecure and would expose sensitive data to the entire internet.",
                "Grant the roles/storage.objectViewer role to individual users": "Correct! This follows the principle of least privilege by granting only the necessary permission (read-only) directly to the specific users who need it.",
                "Grant the roles/storage.admin role to the data science team's service account": "Incorrect. This violates the principle of least privilege by granting full administrative control. Also, service accounts are for applications, not for granting direct access to human users.",
                "Set up signed URLs for each object they need to access": "Incorrect. This is impractical for ongoing access to a large dataset and is better suited for providing temporary, one-off access to a specific file."
            }
        },
        {
            "title": "Q15: Choosing a Database for High-Throughput IoT Data",
            "description": "Your company is developing an IoT application that collects sensor data from millions of devices. This data needs to be ingested at extremely high throughput and made available for near real-time analytics. The data is mostly time-series based. Which database should you use?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Datastore"
            ],
            "correctAnswer": "Google Cloud Bigtable",
            "feedback": {
                "Google Cloud SQL": "Incorrect. A relational database like Cloud SQL is not designed to handle the massive write throughput and scale required for millions of IoT devices.",
                "Google Cloud Spanner": "Incorrect. Spanner is for globally consistent transactional workloads, not the ideal choice for raw, high-throughput time-series data ingestion.",
                "Google Cloud Bigtable": "Correct! Bigtable is a wide-column NoSQL database specifically designed for massive-scale, high-throughput ingestion and analytics on time-series and operational data, making it a perfect fit for IoT.",
                "Google Cloud Datastore": "Incorrect. Datastore is a document database suitable for web and mobile app backends but does not scale to the extreme throughput and volume of a large IoT use case."
            }
        },
        {
            "title": "Q16: Data Transfer for Large On-Premises Dataset to GCP",
            "description": "Your company needs to migrate a large on-premises dataset (several petabytes) to Google Cloud Storage. The internet connection at the data center is slow and unreliable. You need a reliable and fast method for this initial bulk data transfer. What should you do?",
            "options": [
                "Use gsutil rsync to synchronize data to Cloud Storage.",
                "Use Transfer Appliance to transfer the data.",
                "Use Storage Transfer Service to transfer the data.",
                "Use a Direct Peering connection to transfer the data."
            ],
            "correctAnswer": "Use Transfer Appliance to transfer the data.",
            "feedback": {
                "Use gsutil rsync to synchronize data to Cloud Storage.": "Incorrect. This command relies on the internet connection. Given that the connection is slow and unreliable, this method would be extremely time-consuming and likely fail.",
                "Use Transfer Appliance to transfer the data.": "Correct! Transfer Appliance is a physical hardware device that you rent from Google. You load your data onto it on-premises and then ship it to a Google data center. This is the recommended solution for petabyte-scale transfers with poor network connectivity.",
                "Use Storage Transfer Service to transfer the data.": "Incorrect. This is a managed service for online data transfers. It still depends on having a reliable network connection.",
                "Use a Direct Peering connection to transfer the data.": "Incorrect. This is a networking service to improve connectivity to Google's public services, not a data transfer method for bypassing a slow link for a bulk migration."
            }
        },
        {
            "title": "Q17: Managing Configuration for Multiple Applications on GKE",
            "description": "Your company is running multiple containerized applications on Google Kubernetes Engine (GKE). Each application has its own set of configurations (e.g., API keys, database connection strings, environment variables) that need to be managed securely and updated easily without redeploying the applications. What should you use to manage these configurations?",
            "options": [
                "Kubernetes Secrets for sensitive data, and ConfigMaps for non-sensitive data.",
                "Store all configurations as environment variables directly in the Deployment manifest.",
                "Store all configurations in a Cloud Storage bucket and retrieve them at runtime.",
                "Use a custom solution to store and inject configurations into pods."
            ],
            "correctAnswer": "Kubernetes Secrets for sensitive data, and ConfigMaps for non-sensitive data.",
            "feedback": {
                "Kubernetes Secrets for sensitive data, and ConfigMaps for non-sensitive data.": "Correct! This is the standard, native Kubernetes way. ConfigMaps are for non-sensitive configuration data, and Secrets are specifically designed to store and manage sensitive information like passwords and API keys.",
                "Store all configurations as environment variables directly in the Deployment manifest.": "Incorrect. This is inflexible and insecure. It exposes sensitive data in version control and requires a full redeployment to update any value.",
                "Store all configurations in a Cloud Storage bucket and retrieve them at runtime.": "Incorrect. This requires writing custom logic within the application to fetch configurations and manage access, adding unnecessary complexity compared to the native Kubernetes solution.",
                "Use a custom solution to store and inject configurations into pods.": "Incorrect. Building a custom solution is unnecessary when Kubernetes provides robust, built-in objects (Secrets and ConfigMaps) for this exact purpose."
            }
        },
        {
            "title": "Q18: Deploying a Scalable Microservices Architecture",
            "description": "Your company wants to develop a new application using a microservices architecture. They need a platform that can automatically scale individual microservices, handle service discovery, and provide load balancing between them. Which Google Cloud service should you choose?",
            "options": [
                "Google Compute Engine",
                "Google Kubernetes Engine (GKE)",
                "Google App Engine Standard",
                "Google Cloud Functions"
            ],
            "correctAnswer": "Google Kubernetes Engine (GKE)",
            "feedback": {
                "Google Compute Engine": "Incorrect. Using plain VMs would require you to manually build and manage all the orchestration features like scaling, service discovery, and load balancing, which is a significant operational burden.",
                "Google Kubernetes Engine (GKE)": "Correct! GKE is a managed Kubernetes service, which is the industry standard for orchestrating containerized microservices. It provides all the required features natively: autoscaling, service discovery, internal load balancing, and rolling updates.",
                "Google App Engine Standard": "Incorrect. While App Engine supports microservices (as separate services), it offers less flexibility and control over the container environment compared to GKE, which is generally preferred for complex microservices architectures.",
                "Google Cloud Functions": "Incorrect. Cloud Functions are for event-driven, single-purpose functions (FaaS), not for hosting complex, long-running microservices systems."
            }
        },
        {
            "title": "Q19: Disaster Recovery for Cloud SQL (RPO/RTO)",
            "description": "Your company is running a critical application using Google Cloud SQL for MySQL. They need a disaster recovery strategy that ensures a Recovery Point Objective (RPO) of near zero and a Recovery Time Objective (RTO) in minutes. What should you do?",
            "options": [
                "Enable automated daily backups.",
                "Configure Point-in-Time Recovery.",
                "Set up a read replica in a different region.",
                "Configure High Availability (HA) with cross-region replication."
            ],
            "correctAnswer": "Configure High Availability (HA) with cross-region replication.",
            "feedback": {
                "Enable automated daily backups.": "Incorrect. Daily backups mean an RPO of up to 24 hours (a full day of data loss) and an RTO of many minutes or hours to restore.",
                "Configure Point-in-Time Recovery.": "Incorrect. This helps recover from logical errors but still involves a restoration process, leading to a higher RTO than required.",
                "Set up a read replica in a different region.": "Incorrect. A standard read replica uses asynchronous replication, which means there can be some data loss (non-zero RPO). Failover is also a manual process, increasing the RTO.",
                "Configure High Availability (HA) with cross-region replication.": "Correct! A Cloud SQL HA configuration provides a synchronous standby in another zone for near-zero RPO and automatic failover in minutes (low RTO). Combining this with a cross-region replica for disaster recovery provides the most robust solution."
            }
        },
        {
            "title": "Q20: Firewall Rule for Specific IP Address Range",
            "description": "You need to create a firewall rule in your GCP VPC network that allows incoming HTTP traffic from 192.168.1.0/24. Which configuration should you use for the firewall rule?",
            "options": [
                "Direction: Egress, Source IP ranges: 192.168.1.0/24, Protocols and ports: tcp:80",
                "Direction: Ingress, Source IP ranges: 192.168.1.0/24, Protocols and ports: tcp:80",
                "Direction: Egress, Destination IP ranges: 192.168.1.0/24, Protocols and ports: tcp:80",
                "Direction: Ingress, Destination IP ranges: 192.168.1.0/24, Protocols and ports: tcp:80"
            ],
            "correctAnswer": "Direction: Ingress, Source IP ranges: 192.168.1.0/24, Protocols and ports: tcp:80",
            "feedback": {
                "Direction: Egress, Source IP ranges: 192.168.1.0/24, Protocols and ports: tcp:80": "Incorrect. 'Egress' refers to outgoing traffic from your VMs. The request is for incoming traffic.",
                "Direction: Ingress, Source IP ranges: 192.168.1.0/24, Protocols and ports: tcp:80": "Correct! 'Ingress' means incoming traffic. 'Source IP ranges' specifies where the traffic is coming from. This rule correctly allows incoming TCP traffic on port 80 (HTTP) from the 192.168.1.0/24 network.",
                "Direction: Egress, Destination IP ranges: 192.168.1.0/24, Protocols and ports: tcp:80": "Incorrect. 'Egress' refers to outgoing traffic.",
                "Direction: Ingress, Destination IP ranges: 192.168.1.0/24, Protocols and ports: tcp:80": "Incorrect. For an ingress rule, 'Destination' refers to the target within your VPC. The IP range given is the origin of the traffic, so it should be the 'Source'."
            }
        },
        {
            "title": "Q21: Data Retention Policy for Regulatory Compliance",
            "description": "Your company has a regulatory requirement to retain specific log data for seven years. This data is rarely accessed after the first 30 days. You need a cost-effective storage solution that meets this retention policy. Which Cloud Storage class should you use?",
            "options": [
                "Standard",
                "Nearline",
                "Coldline",
                "Archive"
            ],
            "correctAnswer": "Archive",
            "feedback": {
                "Standard": "Incorrect. This class is for frequently accessed data and would be unnecessarily expensive for long-term archival.",
                "Nearline": "Incorrect. Nearline is cost-effective for data accessed about once a month. This data is accessed even less frequently.",
                "Coldline": "Incorrect. Coldline is designed for data accessed about once a quarter. Archive storage is even more cost-effective for the required access pattern.",
                "Archive": "Correct! Archive storage has the lowest storage cost and is designed for long-term (years) data retention where data is accessed less than once a year. This perfectly matches the compliance requirement."
            }
        },
        {
            "title": "Q22: Serverless Application Development with Event-Driven Functions",
            "description": "Your company wants to develop a new application that processes events from various sources (e.g., file uploads, database changes, messages on a queue). They want to use a fully managed, serverless platform that automatically scales and requires minimal operational overhead for individual pieces of logic. Which service should you choose?",
            "options": [
                "Google App Engine",
                "Google Kubernetes Engine",
                "Google Cloud Functions",
                "Google Compute Engine"
            ],
            "correctAnswer": "Google Cloud Functions",
            "feedback": {
                "Google App Engine": "Incorrect. App Engine is a Platform-as-a-Service (PaaS) for hosting entire applications, which is more than what's needed for processing individual events.",
                "Google Kubernetes Engine": "Incorrect. GKE is a container orchestration platform. It is not serverless and has significant operational overhead compared to Cloud Functions.",
                "Google Cloud Functions": "Correct! This is the definition of a Function-as-a-Service (FaaS) platform. Cloud Functions are designed to run short-lived, event-driven pieces of code in a fully managed, serverless environment.",
                "Google Compute Engine": "Incorrect. This is Infrastructure-as-a-Service (IaaS), requiring you to manage VMs, and it is the opposite of a serverless, minimal overhead solution."
            }
        },
        {
            "title": "Q23: Managing Secrets and Certificates in GKE",
            "description": "Your application running on Google Kubernetes Engine (GKE) requires access to sensitive information like API keys and SSL/TLS certificates. You need a secure and manageable way to store and inject these secrets into your application pods. Which method should you use?",
            "options": [
                "Store secrets directly in Docker images.",
                "Use Kubernetes Secrets.",
                "Mount secrets from a Cloud Storage bucket.",
                "Hardcode secrets in application code."
            ],
            "correctAnswer": "Use Kubernetes Secrets.",
            "feedback": {
                "Store secrets directly in Docker images.": "Incorrect. This is a major security risk, as anyone who can access the image can extract the secrets. It also makes secret rotation very difficult.",
                "Use Kubernetes Secrets.": "Correct! Kubernetes Secrets are the native, secure, and standard way to store and manage sensitive information. They can be mounted as files or exposed as environment variables to pods, and they are not stored in version control.",
                "Mount secrets from a Cloud Storage bucket.": "Incorrect. This requires custom application logic and careful management of IAM permissions, making it more complex and less integrated than using native Kubernetes Secrets.",
                "Hardcode secrets in application code.": "Incorrect. This is extremely insecure. Secrets would be exposed in your source code repository, and updating them would require a code change and redeployment."
            }
        },
        {
            "title": "Q24: Migrating On-Premise Relational Database (Minimal Change)",
            "description": "Your company wants to migrate an existing on-premises relational database (PostgreSQL) to Google Cloud with minimal changes to the application code. They need a fully managed service. Which service should you choose?",
            "options": [
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud SQL",
                "Google Cloud Datastore"
            ],
            "correctAnswer": "Google Cloud SQL",
            "feedback": {
                "Google Cloud Spanner": "Incorrect. Spanner is a globally distributed database with a different architecture and API from standard PostgreSQL. Migrating would likely require significant application changes.",
                "Google Cloud Bigtable": "Incorrect. This is a NoSQL wide-column database and is fundamentally different from a relational database like PostgreSQL.",
                "Google Cloud SQL": "Correct! Cloud SQL is a fully managed relational database service that supports PostgreSQL. It provides a highly compatible environment, allowing for a 'lift-and-shift' migration with minimal or no changes to the application code.",
                "Google Cloud Datastore": "Incorrect. This is a NoSQL document database and is not compatible with a PostgreSQL application."
            }
        },
        {
            "title": "Q25: Network Performance for Direct Traffic to GCE VMs",
            "description": "Your company has an application running on Compute Engine VMs that experiences high network latency for direct inbound traffic from the internet. They need to optimize network performance for these connections. What should they do?",
            "options": [
                "Deploy a Global Load Balancer in front of the VMs.",
                "Use a Cloud VPN connection.",
                "Increase the network bandwidth of the VMs.",
                "Enable VPC Service Controls."
            ],
            "correctAnswer": "Deploy a Global Load Balancer in front of the VMs.",
            "feedback": {
                "Deploy a Global Load Balancer in front of the VMs.": "Correct! The Global External HTTP(S) Load Balancer leverages Google's global network and edge locations. It terminates user connections at an edge location close to the user and then carries the traffic over Google's high-performance backbone network to the backend VMs, significantly reducing latency.",
                "Use a Cloud VPN connection.": "Incorrect. A VPN is for creating a secure tunnel from another network (like on-premises) to your VPC, not for optimizing public internet traffic.",
                "Increase the network bandwidth of the VMs.": "Incorrect. This might help if the VM itself is the bottleneck, but it won't improve the latency of the network path between the user and the VM.",
                "Enable VPC Service Controls.": "Incorrect. This is a security feature for creating a service perimeter to prevent data exfiltration; it does not improve network performance."
            }
        },
        {
            "title": "Q26: Storing and Querying High-Volume Log Data",
            "description": "Your application generates a very high volume of log data that needs to be stored and efficiently queried for debugging and auditing purposes. The data will grow over time, and you need a scalable solution. Which Google Cloud services should you use?",
            "options": [
                "Cloud Storage for logs, and Cloud SQL for queries.",
                "Cloud Logging for ingestion and storage, and BigQuery for analytical queries.",
                "Cloud Datastore for logs, and Cloud Functions for queries.",
                "Cloud Bigtable for logs, and Dataflow for queries."
            ],
            "correctAnswer": "Cloud Logging for ingestion and storage, and BigQuery for analytical queries.",
            "feedback": {
                "Cloud Storage for logs, and Cloud SQL for queries.": "Incorrect. Cloud SQL is not designed for analyzing high volumes of unstructured log data and would not perform well.",
                "Cloud Logging for ingestion and storage, and BigQuery for analytical queries.": "Correct! This is a standard and powerful pattern. Cloud Logging is designed for high-volume log ingestion and storage. You can then create a sink to export logs to BigQuery, which is built for scalable, SQL-based analysis of massive datasets.",
                "Cloud Datastore for logs, and Cloud Functions for queries.": "Incorrect. This combination is not scalable or efficient for querying large volumes of log data.",
                "Cloud Bigtable for logs, and Dataflow for queries.": "Incorrect. While Bigtable can store logs, Dataflow is a data processing service, not an interactive query engine. BigQuery is the superior tool for analytical queries."
            }
        },
        {
            "title": "Q27: Managing User Access to Multiple Projects",
            "description": "Your company has multiple Google Cloud projects, each for a different team (e.g., Development, Staging, Production). You need to manage user access centrally and consistently across these projects. Which IAM feature should you use?",
            "options": [
                "Service Accounts",
                "Custom Roles",
                "Organizations and Folders",
                "Project IAM policies"
            ],
            "correctAnswer": "Organizations and Folders",
            "feedback": {
                "Service Accounts": "Incorrect. Service accounts are identities for applications or VMs, not for managing human user access across projects.",
                "Custom Roles": "Incorrect. Custom roles define sets of permissions, but they don't provide a way to apply those permissions hierarchically across multiple projects.",
                "Organizations and Folders": "Correct! The resource hierarchy (Organization -> Folders -> Projects) is designed for this. You can apply IAM policies at the Organization or Folder level, and they will be inherited by all the projects within, ensuring consistent, centralized access management.",
                "Project IAM policies": "Incorrect. Managing IAM policies on each project individually is not centralized and becomes unmanageable at scale."
            }
        },
        {
            "title": "Q28: Designing for High Availability for a Stateful Application",
            "description": "You are designing a critical, stateful application that needs to be highly available across multiple zones within a region. If an instance fails, it should quickly fail over to another healthy instance, retaining its state. Which design pattern should you use for state?",
            "options": [
                "Store state in local disk and use persistent disk snapshots for backup.",
                "Store state in an external, highly available database service.",
                "Store state in a distributed cache like Redis and replicate it.",
                "Store state in a Google Cloud Storage bucket."
            ],
            "correctAnswer": "Store state in an external, highly available database service.",
            "feedback": {
                "Store state in local disk and use persistent disk snapshots for backup.": "Incorrect. Local disk state is lost if the instance fails. Restoring from a snapshot takes time and results in a high RTO (Recovery Time Objective).",
                "Store state in an external, highly available database service.": "Correct! By externalizing the state to a managed, multi-zone database (like Cloud SQL in HA mode or Spanner), any application instance can access the current state. If an instance fails, a new one can start and immediately connect to the database, ensuring state is retained and failover is fast.",
                "Store state in a distributed cache like Redis and replicate it.": "Incorrect. While a cache can be part of the solution for performance, a database provides stronger durability and consistency guarantees, which are essential for the primary state of a critical application.",
                "Store state in a Google Cloud Storage bucket.": "Incorrect. Cloud Storage is an object store with higher latency than a database and is not suitable for managing the live, transactional state of an application."
            }
        },
        {
            "title": "Q29: Network Connectivity for Hybrid Cloud (High Bandwidth)",
            "description": "Your company needs to establish high-bandwidth, low-latency, and secure connectivity between its on-premises data center and Google Cloud for mission-critical applications. Which networking solution should you choose?",
            "options": [
                "Cloud VPN",
                "Direct Peering",
                "Partner Interconnect",
                "Dedicated Interconnect"
            ],
            "correctAnswer": "Dedicated Interconnect",
            "feedback": {
                "Cloud VPN": "Incorrect. Cloud VPN operates over the public internet. It is not suitable for applications that require guaranteed high bandwidth and low latency.",
                "Direct Peering": "Incorrect. Direct Peering is for connecting to Google's public services (like YouTube or Google Search), not for creating a private connection to your VPC.",
                "Partner Interconnect": "Incorrect. This is a good option for a private connection, but Dedicated Interconnect offers higher bandwidth options and a direct physical connection, which is best for the most demanding 'mission-critical' applications.",
                "Dedicated Interconnect": "Correct! Dedicated Interconnect provides a direct, private, physical connection between your on-premises network and Google's network. It offers the highest bandwidth (up to 100 Gbps per link) and lowest latency, making it the best choice for mission-critical workloads."
            }
        },
        {
            "title": "Q30: Migrating a Monolithic Application to Containers (Minimal Refactoring)",
            "description": "Your company has a monolithic application running on VMs. They want to migrate it to a containerized environment on Google Cloud with minimal refactoring to gain scalability and portability benefits. Which service should you use?",
            "options": [
                "Google App Engine",
                "Google Kubernetes Engine (GKE)",
                "Google Cloud Functions",
                "Google Compute Engine"
            ],
            "correctAnswer": "Google Kubernetes Engine (GKE)",
            "feedback": {
                "Google App Engine": "Incorrect. Migrating a typical monolith to App Engine Standard often requires significant refactoring to fit its sandboxed environment and specific runtimes.",
                "Google Kubernetes Engine (GKE)": "Correct! You can package the existing monolithic application into a container with minimal changes and deploy it to GKE. This 'lift-and-shift' approach allows you to immediately benefit from container orchestration, scalability, and portability without a large upfront refactoring effort.",
                "Google Cloud Functions": "Incorrect. Cloud Functions are for small, single-purpose functions, not for running large monolithic applications.",
                "Google Compute Engine": "Incorrect. This is just moving from one VM environment to another. It doesn't provide the benefits of a containerized environment like orchestration and portability."
            }
        },
        {
            "title": "Q31: Securing Internal HTTP Traffic between Microservices",
            "description": "Your microservices application runs on Google Kubernetes Engine (GKE). You need to secure the internal HTTP communication between your microservices, ensuring that only authorized services can communicate with each other. Which approach should you use?",
            "options": [
                "Use Network Policies to restrict traffic.",
                "Implement mTLS (mutual TLS) using a service mesh like Istio.",
                "Configure firewall rules within the GKE cluster.",
                "Use internal load balancing to encrypt traffic."
            ],
            "correctAnswer": "Implement mTLS (mutual TLS) using a service mesh like Istio.",
            "feedback": {
                "Use Network Policies to restrict traffic.": "Incorrect. Network Policies operate at Layer 3/4 (IP/port) to control which pods can connect to each other. They provide segmentation but do not encrypt the traffic or verify service identity.",
                "Implement mTLS (mutual TLS) using a service mesh like Istio.": "Correct! A service mesh like Istio can automatically enforce mutual TLS (mTLS) for all service-to-service communication. This provides both strong encryption of traffic and strong identity verification (authentication) for each service.",
                "Configure firewall rules within the GKE cluster.": "Incorrect. GCP firewall rules operate at the VM (node) level, not at the granular pod or service level required for microservices security.",
                "Use internal load balancing to encrypt traffic.": "Incorrect. An internal load balancer distributes traffic but does not inherently encrypt the traffic between the services themselves."
            }
        },
        {
            "title": "Q32: Real-time Data Streaming and Processing",
            "description": "Your company has a new product that generates continuous streams of data from user interactions. You need to ingest this data in real time, process it, and load it into a data warehouse for immediate analysis. Which combination of services should you use?",
            "options": [
                "Cloud Storage and Dataflow.",
                "Pub/Sub and Dataflow.",
                "Cloud SQL and Dataproc.",
                "Cloud Bigtable and BigQuery."
            ],
            "correctAnswer": "Pub/Sub and Dataflow.",
            "feedback": {
                "Cloud Storage and Dataflow.": "Incorrect. Cloud Storage is for batch data, not real-time ingestion. Triggering a Dataflow job for every small file would be inefficient.",
                "Pub/Sub and Dataflow.": "Correct! This is the classic Google Cloud pattern for stream processing. Pub/Sub provides a scalable, real-time ingestion point for the data streams. Dataflow then consumes from Pub/Sub to perform real-time processing (like transformations or aggregations) before loading to a warehouse like BigQuery.",
                "Cloud SQL and Dataproc.": "Incorrect. This combination is not suited for real-time streaming. Cloud SQL is a transactional database, and Dataproc is primarily for batch processing on Spark/Hadoop.",
                "Cloud Bigtable and BigQuery.": "Incorrect. This pair is missing the real-time ingestion and processing components. You need a service like Pub/Sub to get the data into the cloud in real-time."
            }
        },
        {
            "title": "Q33: Monitoring Application Performance and Logs",
            "description": "Your development team needs a comprehensive solution to monitor application performance, track errors, and analyze logs in real time across your Google Cloud environment. Which service should they primarily use?",
            "options": [
                "Cloud Source Repositories",
                "Cloud Build",
                "Cloud Monitoring",
                "Cloud Trace"
            ],
            "correctAnswer": "Cloud Monitoring",
            "feedback": {
                "Cloud Source Repositories": "Incorrect. This is a service for hosting private Git repositories for source code.",
                "Cloud Build": "Incorrect. This is a CI/CD service for building, testing, and deploying applications.",
                "Cloud Monitoring": "Correct! Cloud Monitoring (part of the Google Cloud's operations suite) is the centralized service for collecting and visualizing metrics, events, and metadata. It integrates logging (Cloud Logging) and tracing (Cloud Trace) to provide a comprehensive view of application performance.",
                "Cloud Trace": "Incorrect. Cloud Trace is a specific tool for distributed tracing to analyze latency. It's part of the overall monitoring solution but not the comprehensive service itself."
            }
        },
        {
            "title": "Q34: Cost-Effective Storage for Infrequently Accessed Archival Data",
            "description": "Your company has a large volume of historical data that needs to be stored for compliance reasons. This data is rarely accessed (less than once a year) but must be retained for decades. You need the most cost-effective storage option. Which Cloud Storage class should you use?",
            "options": [
                "Standard",
                "Nearline",
                "Coldline",
                "Archive"
            ],
            "correctAnswer": "Archive",
            "feedback": {
                "Standard": "Incorrect. This is the most expensive storage class, designed for frequently accessed data.",
                "Nearline": "Incorrect. This is for data accessed roughly once a month. Archive is more cost-effective for even less frequent access.",
                "Coldline": "Incorrect. This is for data accessed roughly once a quarter. Archive offers even lower storage costs.",
                "Archive": "Correct! Archive storage provides the lowest at-rest storage cost, making it the most economical choice for long-term data archival where access is extremely rare (less than once a year)."
            }
        },
        {
            "title": "Q35: Automating VM Deployment and Scaling",
            "description": "You need to deploy a fleet of Compute Engine VMs that run identical web servers. The number of VMs needs to automatically scale up or down based on traffic load. You also want to automate the deployment of these VMs. Which Compute Engine feature should you use?",
            "options": [
                "Custom images and startup scripts.",
                "Instance templates and managed instance groups.",
                "Sole-tenant nodes and autoscaling.",
                "Snapshots and instance templates."
            ],
            "correctAnswer": "Instance templates and managed instance groups.",
            "feedback": {
                "Custom images and startup scripts.": "Incorrect. These are components, but they don't provide the scaling and management capabilities on their own.",
                "Instance templates and managed instance groups.": "Correct! This is the perfect combination. An Instance Template defines the configuration for each VM. A Managed Instance Group (MIG) uses that template to create, manage, and (with an autoscaler) automatically scale a group of identical VMs.",
                "Sole-tenant nodes and autoscaling.": "Incorrect. Sole-tenant nodes are for dedicated hardware and are not typically used for general web server scaling.",
                "Snapshots and instance templates.": "Incorrect. Snapshots are for backing up disks. While you can create an image from a snapshot, the MIG is the key component for automated deployment and scaling."
            }
        },
        {
            "title": "Q36: Centralized Log Collection and Aggregation",
            "description": "Your company operates applications across multiple Google Cloud projects. You need to centralize all application and infrastructure logs from these projects into a single location for easier analysis and compliance. Which service should you use?",
            "options": [
                "Cloud Storage",
                "Cloud Logging",
                "BigQuery",
                "Cloud SQL"
            ],
            "correctAnswer": "Cloud Logging",
            "feedback": {
                "Cloud Storage": "Incorrect. Cloud Storage can be a destination for logs, but it is not the service that collects and aggregates them from multiple sources.",
                "Cloud Logging": "Correct! Cloud Logging is the centralized log management service. You can configure Log Sinks at the folder or organization level to route all logs from child projects to a central destination like a Cloud Storage bucket or a BigQuery dataset.",
                "BigQuery": "Incorrect. BigQuery is a powerful tool for analyzing logs, but it is a destination for the logs, not the aggregation service itself.",
                "Cloud SQL": "Incorrect. A relational database is not a suitable tool for aggregating high volumes of unstructured log data."
            }
        },
        {
            "title": "Q37: Data Ingestion for Real-time Dashboards",
            "description": "You are building a real-time analytics dashboard for your application. User clickstream data needs to be ingested continuously, transformed minimally, and then made available for immediate display on the dashboard. Which service should you use for ingestion?",
            "options": [
                "Cloud Storage",
                "Cloud Pub/Sub",
                "Cloud Bigtable",
                "Cloud SQL"
            ],
            "correctAnswer": "Cloud Pub/Sub",
            "feedback": {
                "Cloud Storage": "Incorrect. Cloud Storage is designed for batch data and object storage, not for ingesting continuous, low-latency streams of data.",
                "Cloud Pub/Sub": "Correct! Cloud Pub/Sub is a fully managed, real-time messaging service. It's designed to reliably ingest and deliver high volumes of streaming data with low latency, making it the ideal entry point for a real-time analytics pipeline.",
                "Cloud Bigtable": "Incorrect. Bigtable is a NoSQL database for storing and serving large analytical datasets; it's a potential destination for the data, not the ingestion service.",
                "Cloud SQL": "Incorrect. A transactional, relational database is not suited for the high throughput and streaming nature of clickstream data ingestion."
            }
        },
        {
            "title": "Q38: Managing API Endpoints and Security",
            "description": "Your company has multiple internal and external APIs that need to be centrally managed, secured, and monitored. You need a solution that can handle authentication, authorization, rate limiting, and analytics for all your APIs. Which service should you use?",
            "options": [
                "Cloud Load Balancing",
                "API Gateway",
                "Cloud DNS",
                "Identity-Aware Proxy (IAP)"
            ],
            "correctAnswer": "API Gateway",
            "feedback": {
                "Cloud Load Balancing": "Incorrect. A load balancer distributes traffic but lacks the comprehensive API management features like authentication, rate limiting, and developer portals.",
                "API Gateway": "Correct! API Gateway is a fully managed service specifically designed to create, secure, and monitor APIs. It provides a single point of entry and enforces policies like authentication, authorization, rate limiting, and provides usage analytics.",
                "Cloud DNS": "Incorrect. This service is for managing domain names and DNS records, not for managing API endpoints.",
                "Identity-Aware Proxy (IAP)": "Incorrect. IAP is for securing access to web applications and VMs based on user identity, not for general-purpose API management."
            }
        },
        {
            "title": "Q39: Migrating a Data Warehouse for Scalability",
            "description": "Your company currently runs an on-premises data warehouse that is struggling to scale with increasing data volumes and complex analytical queries. You need to migrate it to Google Cloud to achieve high scalability and performance for analytical workloads. Which service should you use?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud BigQuery",
                "Google Cloud Bigtable"
            ],
            "correctAnswer": "Google Cloud BigQuery",
            "feedback": {
                "Google Cloud SQL": "Incorrect. This is a transactional (OLTP) database and does not have the scale or columnar storage architecture needed for a large data warehouse.",
                "Google Cloud Spanner": "Incorrect. Spanner is a globally consistent transactional database, not an analytical data warehouse.",
                "Google Cloud BigQuery": "Correct! BigQuery is Google's serverless, highly scalable, and cost-effective enterprise data warehouse. It is specifically designed to handle petabyte-scale data and execute complex analytical SQL queries with high performance.",
                "Google Cloud Bigtable": "Incorrect. This is a NoSQL database optimized for high-throughput operational workloads, not a relational data warehouse for analytical queries."
            }
        },
        {
            "title": "Q40: Setting Up CI/CD for GKE Application",
            "description": "You are deploying a new application to Google Kubernetes Engine (GKE). You need to set up a Continuous Integration/Continuous Delivery (CI/CD) pipeline that automatically builds your container images, tests them, and deploys them to GKE when code changes are committed. Which combination of services should you use?",
            "options": [
                "Cloud Source Repositories, Cloud Build, and Cloud Deploy.",
                "Cloud Source Repositories, Cloud Functions, and Cloud Run.",
                "Cloud Storage, Cloud Build, and Cloud Scheduler.",
                "Cloud Code, Cloud Build, and Compute Engine."
            ],
            "correctAnswer": "Cloud Source Repositories, Cloud Build, and Cloud Deploy.",
            "feedback": {
                "Cloud Source Repositories, Cloud Build, and Cloud Deploy.": "Correct! This combination provides a complete, native CI/CD solution. Cloud Source Repositories hosts the code, Cloud Build handles the CI (build and test), and Cloud Deploy manages the CD (progressive delivery to GKE).",
                "Cloud Source Repositories, Cloud Functions, and Cloud Run.": "Incorrect. Cloud Functions and Cloud Run are deployment targets, not tools for building a CI/CD pipeline for GKE.",
                "Cloud Storage, Cloud Build, and Cloud Scheduler.": "Incorrect. Cloud Storage and Cloud Scheduler are not primary components of a CI/CD pipeline for code.",
                "Cloud Code, Cloud Build, and Compute Engine.": "Incorrect. Cloud Code is an IDE extension, and Compute Engine is for VMs. Cloud Deploy is the specific tool for continuous delivery to GKE."
            }
        },
        {
            "title": "Q41: Optimizing Object Storage for Cost and Access (Variable Frequency)",
            "description": "Your company stores large amounts of unstructured data (e.g., historical reports, media files) in Google Cloud Storage. The access patterns for this data vary: some files are accessed frequently for a short period, while others are accessed rarely after initial upload. You need to optimize storage costs while ensuring data is accessible when needed. What should you do?",
            "options": [
                "Use the Standard storage class for all data.",
                "Use a Lifecycle Management policy to transition data between storage classes.",
                "Use the Archive storage class for all data.",
                "Use Nearline storage class for all data."
            ],
            "correctAnswer": "Use a Lifecycle Management policy to transition data between storage classes.",
            "feedback": {
                "Use the Standard storage class for all data.": "Incorrect. This would not be cost-effective for the data that is rarely accessed.",
                "Use a Lifecycle Management policy to transition data between storage classes.": "Correct! Lifecycle Management policies allow you to automatically transition objects to cheaper storage classes (e.g., from Standard to Nearline, then to Coldline) based on rules like age. This is the perfect way to automate cost optimization for data with variable access patterns.",
                "Use the Archive storage class for all data.": "Incorrect. This would result in high retrieval costs and delays for the data that is accessed frequently in its early life.",
                "Use Nearline storage class for all data.": "Incorrect. This is a middle ground that is not optimal for either the frequently accessed or the very rarely accessed data."
            }
        },
        {
            "title": "Q42: Data Processing for Big Data Batch Jobs",
            "description": "Your data science team needs to process multi-terabyte datasets using Apache Spark batch jobs. They want a fully managed service that can quickly provision and scale Spark clusters without manual infrastructure management. Which service should you recommend?",
            "options": [
                "Google Cloud Dataflow",
                "Google Cloud Dataproc",
                "Google Compute Engine",
                "Google Kubernetes Engine"
            ],
            "correctAnswer": "Google Cloud Dataproc",
            "feedback": {
                "Google Cloud Dataflow": "Incorrect. Dataflow uses the Apache Beam model and is not for running native Apache Spark jobs without rewriting them.",
                "Google Cloud Dataproc": "Correct! Dataproc is Google's fully managed service for running Apache Spark and Hadoop clusters. It handles cluster provisioning, management, and scaling, allowing the team to focus on their Spark jobs.",
                "Google Compute Engine": "Incorrect. This would require the team to manually install, configure, and manage a Spark cluster on VMs, which contradicts the 'fully managed' requirement.",
                "Google Kubernetes Engine": "Incorrect. While you can run Spark on GKE, it requires more complex setup and management than the purpose-built Dataproc service."
            }
        },
        {
            "title": "Q43: Managing Network Access to Services within a VPC",
            "description": "You have a multi-tier application deployed across different Compute Engine VMs within a single VPC Network. You need to restrict inbound network access to specific application services based on their internal IP addresses and ports, allowing only authorized communication between tiers. Which networking feature should you use?",
            "options": [
                "Network Tags",
                "Firewall Rules",
                "VPC Service Controls",
                "Service Accounts"
            ],
            "correctAnswer": "Firewall Rules",
            "feedback": {
                "Network Tags": "Incorrect. Network tags are labels you apply to VMs, but they don't enforce any rules on their own. They are used by firewall rules to identify which VMs a rule applies to.",
                "Firewall Rules": "Correct! VPC Firewall Rules are the primary mechanism for controlling traffic between VMs within a VPC. You can create rules that allow or deny traffic based on protocol, port, and source/destination (using IPs, tags, or service accounts).",
                "VPC Service Controls": "Incorrect. This is for creating a security perimeter around managed Google services to prevent data exfiltration, not for controlling VM-to-VM traffic.",
                "Service Accounts": "Incorrect. Service accounts are identities for services. While they can be used as sources/targets in firewall rules, the feature itself is 'Firewall Rules'."
            }
        },
        {
            "title": "Q44: Storing Session State for Highly Scalable Web App",
            "description": "Your web application is experiencing high traffic and needs to scale horizontally across many instances. You need to store user session state securely and ensure it's accessible by any instance, even if an instance restarts or fails. Which solution should you choose?",
            "options": [
                "In-memory session on each web server instance.",
                "Google Cloud Memorystore for Redis.",
                "Client-side cookies with encrypted session data.",
                "Persistent Disk attached to a single web server."
            ],
            "correctAnswer": "Google Cloud Memorystore for Redis.",
            "feedback": {
                "In-memory session on each web server instance.": "Incorrect. This is not scalable. If a user's request hits a different instance, their session state will be lost. If an instance fails, all sessions on it are lost.",
                "Google Cloud Memorystore for Redis.": "Correct! Externalizing session state to a fast, managed, in-memory store like Memorystore for Redis is the standard solution. It's highly available and allows any application instance to access any user's session data, enabling seamless horizontal scaling.",
                "Client-side cookies with encrypted session data.": "Incorrect. Storing large amounts of session data in cookies is not recommended due to size limits and potential security concerns if not implemented perfectly.",
                "Persistent Disk attached to a single web server.": "Incorrect. This creates a single point of failure and does not allow multiple instances to access the session state simultaneously."
            }
        },
        {
            "title": "Q45: Centralized User Directory for Hybrid Cloud",
            "description": "Your company has an existing on-premises Active Directory for user management. They are migrating applications to Google Cloud and need to maintain a single source of truth for user identities. Users in the cloud should authenticate against the existing Active Directory. Which solution should you implement?",
            "options": [
                "Set up Cloud Identity.",
                "Use Identity Platform.",
                "Implement Managed Microsoft AD.",
                "Configure Directory Sync."
            ],
            "correctAnswer": "Configure Directory Sync.",
            "feedback": {
                "Set up Cloud Identity.": "Incorrect. Cloud Identity is where the users will be synced to, but it's not the mechanism that performs the synchronization.",
                "Use Identity Platform.": "Incorrect. Identity Platform is for managing customer identities for your own applications (CIAM), not for your company's workforce identities.",
                "Implement Managed Microsoft AD.": "Incorrect. This service creates a new, managed AD domain in the cloud. The requirement is to use the *existing* on-premises Active Directory.",
                "Configure Directory Sync.": "Correct! Google Cloud Directory Sync (GCDS) is a tool that synchronizes user and group information from your existing Active Directory or LDAP server to your Cloud Identity domain, allowing you to manage identities in one place."
            }
        },
        {
            "title": "Q46: Event-Driven Architecture for Data Processing",
            "description": "You need to design an architecture that processes incoming data files (e.g., CSVs, images) as soon as they are uploaded to a Cloud Storage bucket. The processing involves data validation and transformation. You want a serverless solution that is cost-effective and scales automatically. Which combination of services should you use?",
            "options": [
                "Cloud Storage with Cloud Functions.",
                "Cloud Pub/Sub with Cloud Dataflow.",
                "Cloud Storage with Compute Engine.",
                "Cloud Pub/Sub with Cloud Run."
            ],
            "correctAnswer": "Cloud Storage with Cloud Functions.",
            "feedback": {
                "Cloud Storage with Cloud Functions.": "Correct! This is a classic serverless, event-driven pattern. Cloud Functions can be configured with a Cloud Storage trigger that automatically executes your processing code whenever a new file is uploaded to a specific bucket.",
                "Cloud Pub/Sub with Cloud Dataflow.": "Incorrect. This is a pattern for streaming data, not for processing discrete file uploads.",
                "Cloud Storage with Compute Engine.": "Incorrect. This is not a serverless solution. You would have to manage the VM and create a mechanism to watch the bucket for changes.",
                "Cloud Pub/Sub with Cloud Run.": "Incorrect. This is also for streaming data. Cloud Run is not directly triggered by Cloud Storage events in the same way Cloud Functions are."
            }
        },
        {
            "title": "Q47: Database for Complex Analytics with Geospatial Data",
            "description": "Your company needs to perform complex analytical queries on a large dataset that includes geospatial information. Data scientists need to run ad-hoc queries and combine geospatial data with other business data. Which database service should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google BigQuery"
            ],
            "correctAnswer": "Google BigQuery",
            "feedback": {
                "Google Cloud SQL": "Incorrect. While it supports some geospatial data types, Cloud SQL is a transactional database and is not optimized for complex, ad-hoc analytical queries on large datasets.",
                "Google Cloud Spanner": "Incorrect. Spanner is a globally consistent transactional database, not an analytical data warehouse.",
                "Google Cloud Bigtable": "Incorrect. This is a NoSQL database and does not support complex SQL or geospatial queries.",
                "Google BigQuery": "Correct! BigQuery is an analytical data warehouse that excels at this. It has built-in support for geospatial data types and a rich set of GIS functions, allowing data scientists to run complex ad-hoc queries combining location data with other business metrics at massive scale."
            }
        },
        {
            "title": "Q48: Disaster Recovery for Stateless Web Application",
            "description": "You have a critical, stateless web application deployed on Google Compute Engine instances using Managed Instance Groups in a single region. You need a disaster recovery strategy that minimizes downtime in case of a regional outage. What should you do?",
            "options": [
                "Create regular snapshots of the persistent disks.",
                "Set up a Multi-Region Load Balancer and deploy MIGs in another region.",
                "Configure a Cloud VPN connection to an on-premises DR site.",
                "Implement a manual failover process to a standby application in another zone."
            ],
            "correctAnswer": "Set up a Multi-Region Load Balancer and deploy MIGs in another region.",
            "feedback": {
                "Create regular snapshots of the persistent disks.": "Incorrect. This is a backup strategy, not a disaster recovery strategy for minimizing downtime. Restoring from snapshots would lead to a very high RTO.",
                "Set up a Multi-Region Load Balancer and deploy MIGs in another region.": "Correct! This provides a robust, active-active or active-passive DR setup. The Global Load Balancer can automatically direct traffic to the healthy region if one region fails, ensuring minimal downtime.",
                "Configure a Cloud VPN connection to an on-premises DR site.": "Incorrect. This describes a hybrid cloud setup, not a cloud-native multi-region disaster recovery solution.",
                "Implement a manual failover process to a standby application in another zone.": "Incorrect. A regional outage would affect all zones in that region, so failing over to another zone is not sufficient. Also, manual failover is slow and error-prone."
            }
        },
        {
            "title": "Q49: Secure Access to Internal Applications (No VPN)",
            "description": "Your development team needs to securely access several internal web applications running on Compute Engine VMs, without using a VPN. Access should be controlled based on user identity. Which service should you use?",
            "options": [
                "Cloud VPN",
                "Cloud Interconnect",
                "Identity-Aware Proxy (IAP)",
                "Cloud Armor"
            ],
            "correctAnswer": "Identity-Aware Proxy (IAP)",
            "feedback": {
                "Cloud VPN": "Incorrect. The requirement explicitly states 'without using a VPN'.",
                "Cloud Interconnect": "Incorrect. This provides network connectivity from on-premises, but doesn't solve secure access for remote users without a VPN.",
                "Identity-Aware Proxy (IAP)": "Correct! IAP is a zero-trust security service that allows you to manage access to your internal web applications based on a user's identity and context, without the need for a traditional VPN.",
                "Cloud Armor": "Incorrect. Cloud Armor is a Web Application Firewall (WAF) and DDoS protection service. It protects applications from external attacks but doesn't provide identity-based access for users."
            }
        },
        {
            "title": "Q50: Centralized Metrics and Monitoring for Hybrid Environment",
            "description": "Your company has applications running both on-premises and on Google Cloud. You need a single pane of glass to monitor performance metrics and logs from both environments. Which service should you use?",
            "options": [
                "Cloud Logging",
                "Cloud Monitoring",
                "Cloud Trace",
                "Cloud Audit Logs"
            ],
            "correctAnswer": "Cloud Monitoring",
            "feedback": {
                "Cloud Logging": "Incorrect. Cloud Logging is primarily for logs. While part of the overall solution, Cloud Monitoring is the service that provides the 'single pane of glass' for both metrics and logs.",
                "Cloud Monitoring": "Correct! Cloud Monitoring is designed for this hybrid use case. By installing the Ops Agent on your on-premises servers, you can send metrics and logs to Cloud Monitoring, giving you a unified view of your entire infrastructure in one place.",
                "Cloud Trace": "Incorrect. This is a specialized tool for analyzing application latency, not for general infrastructure monitoring.",
                "Cloud Audit Logs": "Incorrect. This service records administrative activities and data access events within GCP, not performance metrics from applications."
            }
        },
        {
            "title": "Q51: Choosing a Database for Financial Ledger (Append-Only)",
            "description": "Your company needs to store a financial ledger that requires append-only operations and extremely high write throughput. Data is typically accessed via range queries on the ledger entries. Which database should you use?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Firestore"
            ],
            "correctAnswer": "Google Cloud Bigtable",
            "feedback": {
                "Google Cloud SQL": "Incorrect. Relational databases are not optimized for the extremely high append-only write throughput of a large-scale ledger.",
                "Google Cloud Spanner": "Incorrect. Spanner is designed for strong ACID transactions. While it can handle the load, it might be overkill and less cost-effective than Bigtable for a pure append-only use case without complex transactional requirements.",
                "Google Cloud Bigtable": "Correct! Bigtable is a wide-column NoSQL database optimized for very high write throughput and fast range scans on a row key. This makes it ideal for time-series or ledger-like data where entries are appended and then read in sequence.",
                "Google Cloud Firestore": "Incorrect. Firestore is a document database for application data and does not scale to the extreme write throughput required for a massive financial ledger."
            }
        },
        {
            "title": "Q52: Data Ingestion for Batch Analytics (Cost-Effective)",
            "description": "You need to ingest large batches of data files (e.g., daily CSV reports from partners) into Google Cloud Storage. This process should be cost-effective and reliable. Which method should you use?",
            "options": [
                "Use gsutil cp from a Compute Engine VM.",
                "Use Storage Transfer Service.",
                "Use Cloud Pub/Sub.",
                "Use Cloud Dataflow."
            ],
            "correctAnswer": "Use Storage Transfer Service.",
            "feedback": {
                "Use gsutil cp from a Compute Engine VM.": "Incorrect. This requires you to manage a VM and write scripts for scheduling, making it less managed and potentially less reliable for large recurring transfers.",
                "Use Storage Transfer Service.": "Correct! This is a fully managed, serverless service designed for large-scale, scheduled, and reliable data transfers into Cloud Storage from various sources. It is the most cost-effective and reliable solution for this use case.",
                "Use Cloud Pub/Sub.": "Incorrect. Pub/Sub is for streaming data, not for transferring large batches of files.",
                "Use Cloud Dataflow.": "Incorrect. Dataflow is a data processing service. While it can write to Cloud Storage, it is not the primary tool for simply transferring existing files."
            }
        },
        {
            "title": "Q53: Automating Infrastructure Deployment in Production",
            "description": "Your company wants to automate the deployment of its production infrastructure (VPC, subnets, Compute Engine instances, load balancers) in Google Cloud. They need a declarative, repeatable, and version-controlled approach. Which tool should you use?",
            "options": [
                "Cloud Console",
                "gcloud CLI",
                "Deployment Manager",
                "Cloud Shell"
            ],
            "correctAnswer": "Deployment Manager",
            "feedback": {
                "Cloud Console": "Incorrect. The Cloud Console is a manual, UI-based tool and is not suitable for automated, repeatable, or version-controlled deployments.",
                "gcloud CLI": "Incorrect. The gcloud CLI uses imperative commands ('create this, then create that'). It is not a declarative approach where you define the desired end state.",
                "Deployment Manager": "Correct! Deployment Manager (or Terraform) is Google Cloud's native Infrastructure as Code (IaC) service. It allows you to declaratively define your infrastructure in templates, which can be version-controlled and used for automated, consistent deployments.",
                "Cloud Shell": "Incorrect. Cloud Shell is a browser-based command-line environment; it is not an IaC tool itself."
            }
        },
        {
            "title": "Q54: Optimizing Web Application Latency Globally",
            "description": "Your web application serves users globally. You need to minimize latency for users accessing the application from different geographic locations. Which load balancing solution should you use?",
            "options": [
                "Regional HTTP(S) Load Balancer",
                "Internal HTTP(S) Load Balancer",
                "External HTTP(S) Load Balancer (Global)",
                "Network Load Balancer (External)"
            ],
            "correctAnswer": "External HTTP(S) Load Balancer (Global)",
            "feedback": {
                "Regional HTTP(S) Load Balancer": "Incorrect. As the name implies, this load balancer operates within a single region and cannot route traffic to backends in other regions.",
                "Internal HTTP(S) Load Balancer": "Incorrect. This is for traffic within your VPC network, not for external users on the internet.",
                "External HTTP(S) Load Balancer (Global)": "Correct! The Global External HTTP(S) Load Balancer uses a single Anycast IP address and Google's global network to route users to the geographically closest healthy backend, which is the ideal solution for minimizing latency for a global audience.",
                "Network Load Balancer (External)": "Incorrect. This is a regional, Layer 4 load balancer that is not designed for global HTTP/HTTPS traffic routing and latency optimization."
            }
        },
        {
            "title": "Q55: Multi-Factor Authentication for GCP Console",
            "description": "Your company wants to enhance the security of access to the Google Cloud Console for administrators. They need to implement a strong authentication method beyond just passwords. Which security feature should you enable?",
            "options": [
                "VPC Service Controls",
                "Context-Aware Access",
                "Two-Step Verification",
                "Identity Platform"
            ],
            "correctAnswer": "Two-Step Verification",
            "feedback": {
                "VPC Service Controls": "Incorrect. This is a network security feature to prevent data exfiltration, not a user authentication method.",
                "Context-Aware Access": "Incorrect. This allows you to create granular access policies based on user identity and context, but it relies on an underlying authentication method like 2-Step Verification.",
                "Two-Step Verification": "Correct! Two-Step Verification (also known as Multi-Factor Authentication or 2FA/MFA) is the standard feature for adding a second layer of security to user logins, requiring something the user knows (password) and something the user has (a phone prompt or security key).",
                "Identity Platform": "Incorrect. This is for managing customer identities for applications you build, not for securing access to the GCP console for your own administrators."
            }
        },
        {
            "title": "Q56: Managing Multiple Cloud Resources as a Single Unit",
            "description": "You are deploying a new application composed of several interdependent Google Cloud resources (e.g., Compute Engine instances, Cloud SQL databases, Cloud Storage buckets, Pub/Sub topics). You need to manage these resources as a single unit, ensuring consistent deployment and updates. Which concept should you use?",
            "options": [
                "Separate projects for each resource type.",
                "Use a single project for all resources and manage manually.",
                "Define them using a resource hierarchy (Organization, Folders, Projects).",
                "Group them logically into a Cloud Deployment Manager template."
            ],
            "correctAnswer": "Group them logically into a Cloud Deployment Manager template.",
            "feedback": {
                "Separate projects for each resource type.": "Incorrect. This would create unnecessary complexity and make managing the interdependent resources of a single application very difficult.",
                "Use a single project for all resources and manage manually.": "Incorrect. Manual management is prone to errors, lacks consistency, and is not a scalable or repeatable process.",
                "Define them using a resource hierarchy (Organization, Folders, Projects).": "Incorrect. The resource hierarchy is for organizing projects, not for managing the individual resources within a project as a single logical unit.",
                "Group them logically into a Cloud Deployment Manager template.": "Correct! Using an Infrastructure as Code tool like Cloud Deployment Manager or Terraform allows you to define all the related resources for an application in a single template. This template can then be used to deploy, update, and delete the entire application stack as a single, consistent unit."
            }
        },
        {
            "title": "Q57: Cost-Effective Storage for Infrequently Accessed Backups",
            "description": "Your company performs daily backups of its on-premises databases and needs to store them in Google Cloud for disaster recovery. These backups are rarely accessed (only in case of disaster recovery), but must be retrieved quickly when needed. You need a cost-effective storage solution. Which Cloud Storage class should you choose?",
            "options": [
                "Standard",
                "Nearline",
                "Coldline",
                "Archive"
            ],
            "correctAnswer": "Coldline",
            "feedback": {
                "Standard": "Incorrect. This is too expensive for rarely accessed backup data.",
                "Nearline": "Incorrect. Nearline is designed for data accessed about once a month. Since these backups are accessed even less frequently, Coldline is more cost-effective.",
                "Coldline": "Correct! Coldline storage is designed for data accessed at most once a quarter. It offers low storage costs and fast retrieval times (seconds), making it a good balance for disaster recovery backups that are rarely accessed but need to be available quickly.",
                "Archive": "Incorrect. Archive has the lowest storage cost but retrieval takes minutes to hours, which does not meet the requirement to be 'retrieved quickly'."
            }
        },
        {
            "title": "Q58: Data Processing for Stream Analytics",
            "description": "Your company has a system that generates real-time sensor data. You need to perform continuous, real-time analytics on this data to detect anomalies. Which Google Cloud service should you use for processing?",
            "options": [
                "Google Cloud Dataproc",
                "Google Cloud Dataflow",
                "Google Cloud Composer",
                "Google BigQuery"
            ],
            "correctAnswer": "Google Cloud Dataflow",
            "feedback": {
                "Google Cloud Dataproc": "Incorrect. Dataproc is primarily for batch processing with Spark and Hadoop, though it has streaming capabilities, Dataflow is the fully managed, purpose-built service for stream analytics.",
                "Google Cloud Dataflow": "Correct! Dataflow is a fully managed service designed for large-scale, real-time stream processing and analytics using the Apache Beam model. It is the ideal choice for continuous analysis of sensor data.",
                "Google Cloud Composer": "Incorrect. This is a workflow orchestration service for scheduling and managing batch workflows, not for real-time stream processing.",
                "Google BigQuery": "Incorrect. BigQuery is a data warehouse for storing and querying data. While it has streaming ingest capabilities, the actual real-time processing and anomaly detection logic would be built in a service like Dataflow."
            }
        },
        {
            "title": "Q59: Network Design for Service-to-Service Communication (Internal)",
            "description": "You are designing a microservices application on Google Kubernetes Engine (GKE). You need to enable secure and efficient communication between different microservices (pods) within the cluster. Which networking concept should you leverage?",
            "options": [
                "External Load Balancer",
                "Service Mesh",
                "Shared VPC",
                "Kubernetes Services"
            ],
            "correctAnswer": "Kubernetes Services",
            "feedback": {
                "External Load Balancer": "Incorrect. This is for exposing an application to traffic from outside the cluster, not for internal communication between pods.",
                "Service Mesh": "Incorrect. A service mesh (like Istio) builds on top of Kubernetes Services to provide advanced features like mTLS and traffic management. However, the fundamental concept enabling communication is the Kubernetes Service.",
                "Shared VPC": "Incorrect. This is a networking construct for sharing a VPC across multiple projects, not for managing communication within a single Kubernetes cluster.",
                "Kubernetes Services": "Correct! The Kubernetes Service is the fundamental abstraction that provides a stable IP address and DNS name for a set of pods. This allows microservices to discover and communicate with each other reliably, even as pods are created and destroyed."
            }
        },
        {
            "title": "Q60: Multi-Project Resource Organization",
            "description": "Your company wants to organize its Google Cloud resources (projects, billing, IAM) into a hierarchical structure for better management and policy enforcement. They have multiple departments, each with its own set of projects. Which resource hierarchy component should you establish at the top level?",
            "options": [
                "Folders",
                "Projects",
                "Organization",
                "Resource Groups"
            ],
            "correctAnswer": "Organization",
            "feedback": {
                "Folders": "Incorrect. Folders are used to group projects within an Organization; they are not the top-level root element.",
                "Projects": "Incorrect. Projects contain the actual resources and are themselves contained within Folders or an Organization.",
                "Organization": "Correct! The Organization node is the root of the Google Cloud resource hierarchy. It represents the company and allows for centralized control and policy inheritance over all folders and projects.",
                "Resource Groups": "Incorrect. This is not a standard component of the Google Cloud resource hierarchy."
            }
        },
        {
            "title": "Q61: Global Network Connectivity for Microservices",
            "description": "Your company is deploying a globally distributed microservices application on Google Kubernetes Engine (GKE). You need to ensure efficient and low-latency communication between services running in different regions. Which networking service should you use?",
            "options": [
                "VPC Network Peering",
                "Cloud VPN",
                "Dedicated Interconnect",
                "Shared VPC"
            ],
            "correctAnswer": "VPC Network Peering",
            "feedback": {
                "VPC Network Peering": "Correct! VPC Peering allows you to connect two VPC networks so that resources in each network can communicate as if they were in the same network. This communication happens over Google's internal backbone, providing low-latency and high-bandwidth connectivity between regions.",
                "Cloud VPN": "Incorrect. Cloud VPN uses the public internet and is not suitable for high-performance, low-latency communication between microservices in different regions.",
                "Dedicated Interconnect": "Incorrect. This is used to connect an on-premises network to GCP, not for connecting two different GCP VPCs.",
                "Shared VPC": "Incorrect. This allows multiple projects to use a single VPC, but doesn't solve the problem of connecting different VPCs across regions."
            }
        },
        {
            "title": "Q62: Data Pipeline for Batch Processing and Analysis",
            "description": "Your company receives large daily batches of data from external partners. This data needs to be extracted, transformed, and loaded into BigQuery for analytical purposes. You need a fully managed service to build and execute this batch data pipeline. Which service should you choose?",
            "options": [
                "Google Cloud Pub/Sub",
                "Google Cloud Dataflow",
                "Google Cloud Dataproc",
                "Google Cloud Functions"
            ],
            "correctAnswer": "Google Cloud Dataflow",
            "feedback": {
                "Google Cloud Pub/Sub": "Incorrect. Pub/Sub is a messaging service for real-time data ingestion, not for processing large batches of existing data.",
                "Google Cloud Dataflow": "Correct! Dataflow is a fully managed service for executing data processing pipelines. It excels at large-scale ETL (Extract, Transform, Load) jobs for both batch and stream data, making it ideal for this scenario.",
                "Google Cloud Dataproc": "Incorrect. Dataproc is for running Spark/Hadoop clusters. While it can handle batch processing, Dataflow is the more serverless and fully managed option for ETL pipelines.",
                "Google Cloud Functions": "Incorrect. Cloud Functions are for small, event-driven tasks and are not suitable for large-scale batch data processing pipelines."
            }
        },
        {
            "title": "Q63: Cost Optimization for Infrequently Used Development VMs",
            "description": "Your development team uses several Compute Engine VMs for occasional testing and development. These VMs are often idle for long periods (e.g., overnight, weekends). You want to minimize costs while allowing developers to quickly resume work when needed. What should you do?",
            "options": [
                "Use Preemptible VMs.",
                "Manually stop and start VMs as needed.",
                "Use custom machine types.",
                "Set up a Managed Instance Group with autoscaling."
            ],
            "correctAnswer": "Manually stop and start VMs as needed.",
            "feedback": {
                "Use Preemptible VMs.": "Incorrect. Preemptible VMs can be terminated by Google at any time, which would be disruptive for a developer who needs to resume their work.",
                "Manually stop and start VMs as needed.": "Correct! When a VM is stopped, you are not billed for vCPU or memory, only for the storage of the persistent disk. This is the most direct and effective way to save costs on VMs that are idle for predictable periods.",
                "Use custom machine types.": "Incorrect. This helps to right-size the VMs to avoid paying for unused resources while they are running, but it doesn't save money when they are completely idle.",
                "Set up a Managed Instance Group with autoscaling.": "Incorrect. Autoscaling is designed to handle fluctuating load, not to manage costs for individual development VMs that have predictable idle times."
            }
        },
        {
            "title": "Q64: Securing Access to Cloud Storage (Specific IP Ranges)",
            "description": "You have a Cloud Storage bucket containing sensitive marketing data. You need to restrict access to this bucket so that objects can only be uploaded or downloaded from specific IP address ranges belonging to your corporate network. Which feature should you use?",
            "options": [
                "IAM Conditions",
                "VPC Service Controls",
                "Firewall Rules",
                "Signed URLs"
            ],
            "correctAnswer": "VPC Service Controls",
            "feedback": {
                "IAM Conditions": "Incorrect. While IAM Conditions can filter access based on IP address, VPC Service Controls provides a stronger, network-perimeter-based security model that is better suited for preventing data exfiltration.",
                "VPC Service Controls": "Correct! VPC Service Controls allows you to create a service perimeter around your Google Cloud projects and services. You can configure this perimeter to deny access from outside a specified set of IP ranges, effectively isolating your sensitive data.",
                "Firewall Rules": "Incorrect. Firewall rules apply to traffic to and from Compute Engine VMs, not to API-based services like Cloud Storage.",
                "Signed URLs": "Incorrect. Signed URLs are for providing temporary, limited access to specific objects, not for enforcing a persistent IP-based restriction on an entire bucket."
            }
        },
        {
            "title": "Q65: Migrating SQL Server Database to Cloud (Managed, Compatibility)",
            "description": "Your company needs to migrate an on-premises SQL Server database to Google Cloud. They require a fully managed service that offers high compatibility with their existing SQL Server tools and applications. Which service should you choose?",
            "options": [
                "Google Cloud SQL for PostgreSQL",
                "Google Cloud Spanner",
                "Google Cloud SQL for SQL Server",
                "Google Cloud Bigtable"
            ],
            "correctAnswer": "Google Cloud SQL for SQL Server",
            "feedback": {
                "Google Cloud SQL for PostgreSQL": "Incorrect. This is for a different database engine (PostgreSQL) and would require a complex schema and code migration from SQL Server.",
                "Google Cloud Spanner": "Incorrect. Spanner is not a direct replacement for SQL Server and migrating would require significant application re-architecture.",
                "Google Cloud SQL for SQL Server": "Correct! Cloud SQL for SQL Server is a fully managed database service that provides a highly compatible environment for Microsoft SQL Server, allowing for a straightforward migration with minimal changes.",
                "Google Cloud Bigtable": "Incorrect. This is a NoSQL database and is not compatible with a relational SQL Server database."
            }
        },
        {
            "title": "Q66: Orchestrating Complex Multi-Step Workflows",
            "description": "Your data processing workflow involves multiple interdependent steps (e.g., ingest data, transform it, run a machine learning model, load results). You need to orchestrate these steps, handle retries, and manage dependencies efficiently. Which service should you use?",
            "options": [
                "Google Cloud Dataflow",
                "Google Cloud Pub/Sub",
                "Google Cloud Composer",
                "Google Cloud Functions"
            ],
            "correctAnswer": "Google Cloud Composer",
            "feedback": {
                "Google Cloud Dataflow": "Incorrect. Dataflow is a service for executing a single data processing pipeline, not for orchestrating a series of different, interdependent steps.",
                "Google Cloud Pub/Sub": "Incorrect. This is a messaging service used for communication between services, not for workflow orchestration.",
                "Google Cloud Composer": "Correct! Cloud Composer is a fully managed workflow orchestration service built on Apache Airflow. It is specifically designed to schedule, monitor, and manage complex workflows (represented as DAGs - Directed Acyclic Graphs) with dependencies, retries, and alerting.",
                "Google Cloud Functions": "Incorrect. While you could chain functions together, it becomes very difficult to manage dependencies, retries, and monitoring for a complex workflow. It's not the right tool for orchestration."
            }
        },
        {
            "title": "Q67: Centralized Control for Resource Deployment and Policy",
            "description": "Your company has a large organizational structure with multiple departments and projects. You need to enforce consistent resource deployment rules (e.g., allowed VM types, required labeling) and IAM policies across all projects. Which resource hierarchy component should be used to achieve this?",
            "options": [
                "Projects",
                "Folders",
                "Organization",
                "Billing Accounts"
            ],
            "correctAnswer": "Organization",
            "feedback": {
                "Projects": "Incorrect. Policies applied at the project level only affect that single project and are not inherited.",
                "Folders": "Incorrect. While you can apply policies at the folder level, the Organization is the root node above folders, providing the most centralized point of control.",
                "Organization": "Correct! The Organization is the root of the resource hierarchy. Policies (both IAM and Organization Policies) applied at the Organization level are inherited by all folders and projects below it, ensuring consistent enforcement across the entire company.",
                "Billing Accounts": "Incorrect. Billing accounts are for managing payment and costs, not for enforcing resource or security policies."
            }
        },
        {
            "title": "Q68: Performance Optimization for Read-Heavy Relational DB",
            "description": "Your application uses a Cloud SQL for MySQL database which is experiencing high read load, leading to performance bottlenecks. Write operations are infrequent. You need to improve read performance without affecting write performance. What should you do?",
            "options": [
                "Upgrade the Cloud SQL instance to a larger machine type.",
                "Add read replicas to the Cloud SQL instance.",
                "Migrate to Cloud Spanner.",
                "Migrate to Cloud Bigtable."
            ],
            "correctAnswer": "Add read replicas to the Cloud SQL instance.",
            "feedback": {
                "Upgrade the Cloud SQL instance to a larger machine type.": "Incorrect. This is 'vertical scaling'. It can help, but for a read-heavy workload, 'horizontal scaling' with read replicas is often more effective and scalable.",
                "Add read replicas to the Cloud SQL instance.": "Correct! Read replicas are copies of the primary instance that handle read requests. By directing read traffic to one or more replicas, you offload the primary instance, allowing it to handle writes efficiently and significantly improving overall read performance.",
                "Migrate to Cloud Spanner.": "Incorrect. This is a major and complex migration that is not necessary for solving a simple read performance issue.",
                "Migrate to Cloud Bigtable.": "Incorrect. This is a NoSQL database and not a suitable replacement for a relational MySQL database."
            }
        },
        {
            "title": "Q69: Securely Transferring Sensitive Data to Cloud Storage",
            "description": "Your company needs to transfer highly sensitive data from on-premises to a Cloud Storage bucket. The data must be encrypted in transit and at rest, and the transfer needs to be secure and auditable. Which method should you use?",
            "options": [
                "Use gsutil cp with customer-managed encryption keys.",
                "Use Storage Transfer Service with default encryption.",
                "Use gsutil rsync over HTTP.",
                "Use Transfer Appliance."
            ],
            "correctAnswer": "Use Storage Transfer Service with default encryption.",
            "feedback": {
                "Use gsutil cp with customer-managed encryption keys.": "Incorrect. While `gsutil` uses HTTPS for transit encryption, Storage Transfer Service provides a more robust, managed, and auditable solution for large-scale, sensitive transfers.",
                "Use Storage Transfer Service with default encryption.": "Correct! Storage Transfer Service is a managed service that uses HTTPS for encryption in transit, supports various encryption-at-rest options, and provides detailed transfer logs for auditing, making it ideal for secure and auditable transfers.",
                "Use gsutil rsync over HTTP.": "Incorrect. Transferring sensitive data over unencrypted HTTP is a major security risk.",
                "Use Transfer Appliance.": "Incorrect. This is for offline transfers when network connectivity is poor. For online transfers, Storage Transfer Service is the appropriate managed solution."
            }
        },
        {
            "title": "Q70: Identifying Performance Bottlenecks in App Engine",
            "description": "Your application on Google App Engine is experiencing intermittent performance issues (high latency). You need to identify where the bottlenecks are occurring within your application's distributed services. Which tool should you primarily use?",
            "options": [
                "Cloud Logging",
                "Cloud Monitoring",
                "Cloud Trace",
                "Cloud Audit Logs"
            ],
            "correctAnswer": "Cloud Trace",
            "feedback": {
                "Cloud Logging": "Incorrect. Logs can show you errors, but they don't provide a visual breakdown of where time is being spent in a request.",
                "Cloud Monitoring": "Incorrect. Monitoring provides high-level metrics like CPU and request count, but doesn't pinpoint the specific function or service call causing a slowdown within a request.",
                "Cloud Trace": "Correct! Cloud Trace is specifically designed for this purpose. It captures latency information for requests as they travel through your application's services, presenting a detailed waterfall graph that makes it easy to identify which part of the process is the bottleneck.",
                "Cloud Audit Logs": "Incorrect. This is for auditing administrative access and changes, not for application performance analysis."
            }
        },
        {
            "title": "Q71: Choosing Storage for Large Archives with Rare Access",
            "description": "Your company has an extremely large archive of old scientific research data that needs to be retained for regulatory compliance. This data is rarely, if ever, accessed (less than once every few years). Cost-effectiveness for storage is the highest priority. Which Cloud Storage class should you choose?",
            "options": [
                "Standard",
                "Nearline",
                "Coldline",
                "Archive"
            ],
            "correctAnswer": "Archive",
            "feedback": {
                "Standard": "Incorrect. This would be prohibitively expensive for data that is almost never accessed.",
                "Nearline": "Incorrect. This is for data accessed monthly. Archive is far more cost-effective for this use case.",
                "Coldline": "Incorrect. This is for data accessed quarterly. Archive offers even lower storage costs.",
                "Archive": "Correct! Archive storage has the absolute lowest cost for data at rest, making it the perfect choice for long-term retention of data that is very rarely accessed."
            }
        },
        {
            "title": "Q72: Automating Software Updates for Compute Engine Fleet",
            "description": "You have a fleet of Compute Engine VMs running an application. You need to automate the process of applying operating system patches and application updates to these VMs in a controlled and consistent manner. Which feature should you use?",
            "options": [
                "Managed Instance Groups with rolling updates.",
                "Startup scripts and custom images.",
                "SSH into each VM and run update commands.",
                "Cloud Build and custom container images."
            ],
            "correctAnswer": "Managed Instance Groups with rolling updates.",
            "feedback": {
                "Managed Instance Groups with rolling updates.": "Correct! You can create a new instance template with the updated image and then use the MIG's rolling update feature to safely and gradually replace old VMs with new ones, ensuring service availability.",
                "Startup scripts and custom images.": "Incorrect. Startup scripts are for initial configuration, not for applying ongoing updates to a running fleet of VMs.",
                "SSH into each VM and run update commands.": "Incorrect. This is a manual, error-prone, and unscalable process.",
                "Cloud Build and custom container images.": "Incorrect. This applies to containerized applications, but the question refers to updating the OS and applications on Compute Engine VMs."
            }
        },
        {
            "title": "Q73: Securely Connecting On-Prem to GCP (Limited Bandwidth)",
            "description": "Your company needs to connect its small on-premises office network to Google Cloud. The office has limited internet bandwidth, but data is sensitive and needs to be encrypted. Cost-effectiveness is a major concern. Which networking solution should you choose?",
            "options": [
                "Dedicated Interconnect",
                "Partner Interconnect",
                "Cloud VPN",
                "Direct Peering"
            ],
            "correctAnswer": "Cloud VPN",
            "feedback": {
                "Dedicated Interconnect": "Incorrect. This provides a dedicated physical connection and is very expensive, making it unsuitable for a small office with limited bandwidth and a focus on cost-effectiveness.",
                "Partner Interconnect": "Incorrect. This is also a private connection but is generally more expensive and for higher bandwidth needs than a simple Cloud VPN.",
                "Cloud VPN": "Correct! Cloud VPN provides a secure, encrypted (IPsec) tunnel over the public internet. It is the most cost-effective solution and is well-suited for scenarios with lower bandwidth requirements like a small office.",
                "Direct Peering": "Incorrect. This is for connecting to Google's public services, not for creating a private, encrypted connection to your VPC."
            }
        },
        {
            "title": "Q74: Managing API Keys for Cloud Services",
            "description": "Your application uses several Google Cloud services and requires API keys for authentication to these services. You need a secure and centralized way to manage and rotate these API keys. Which service should you use?",
            "options": [
                "Kubernetes Secrets",
                "Cloud Firestore",
                "Secret Manager",
                "Cloud Identity"
            ],
            "correctAnswer": "Secret Manager",
            "feedback": {
                "Kubernetes Secrets": "Incorrect. This is for managing secrets within a Kubernetes cluster, not a centralized solution for all Google Cloud services.",
                "Cloud Firestore": "Incorrect. Storing secrets in a database is not a recommended security practice. It lacks the built-in access control, versioning, and rotation features of a dedicated secret management service.",
                "Secret Manager": "Correct! Secret Manager is a fully managed service designed specifically for securely storing, managing, and accessing secrets like API keys. It provides versioning, IAM-based access control, and audit logging.",
                "Cloud Identity": "Incorrect. This is for managing user and group identities, not for storing application secrets like API keys."
            }
        },
        {
            "title": "Q75: Data Migration for Large Relational Database (Minimal Downtime)",
            "description": "Your company needs to migrate a large (multi-terabyte) on-premises relational database to Google Cloud SQL with minimal downtime. The database is actively used by applications 24/7. Which migration approach should you use?",
            "options": [
                "Export data to CSV and import into Cloud SQL.",
                "Use Database Migration Service (DMS) for continuous replication.",
                "Perform a logical backup and restore to Cloud SQL.",
                "Create a new Cloud SQL instance and manually sync data."
            ],
            "correctAnswer": "Use Database Migration Service (DMS) for continuous replication.",
            "feedback": {
                "Export data to CSV and import into Cloud SQL.": "Incorrect. This method would require significant application downtime while the export and import processes are running.",
                "Use Database Migration Service (DMS) for continuous replication.": "Correct! DMS is a serverless service designed for minimal downtime migrations. It performs an initial load and then uses continuous replication (Change Data Capture) to keep the target database in sync with the source, allowing you to cut over with only a few minutes of downtime.",
                "Perform a logical backup and restore to Cloud SQL.": "Incorrect. Similar to a CSV export, this would involve taking the database offline for a long period to perform the backup and restore, resulting in significant downtime.",
                "Create a new Cloud SQL instance and manually sync data.": "Incorrect. This is a manual, error-prone, and slow process that would not meet the minimal downtime requirement."
            }
        },
        {
            "title": "Q76: Real-time User Event Tracking for Analytics",
            "description": "Your mobile application generates a continuous stream of user interaction events (e.g., clicks, views, searches). You need to capture these events in real time and push them to an analytics system. Which service is best suited for ingesting these events?",
            "options": [
                "Cloud Storage",
                "Cloud Pub/Sub",
                "Cloud Bigtable",
                "Cloud SQL"
            ],
            "correctAnswer": "Cloud Pub/Sub",
            "feedback": {
                "Cloud Storage": "Incorrect. Cloud Storage is for object storage, not for ingesting a high-volume, real-time stream of small events.",
                "Cloud Pub/Sub": "Correct! Cloud Pub/Sub is a highly scalable, global messaging service designed for real-time event ingestion. It can easily handle millions of events per second from distributed sources like mobile applications.",
                "Cloud Bigtable": "Incorrect. Bigtable is a database where you might store the events for analysis, but it's not the ingestion service itself.",
                "Cloud SQL": "Incorrect. A relational database cannot handle the write throughput of a large-scale event tracking system."
            }
        },
        {
            "title": "Q77: Centralized Audit Logging and Compliance",
            "description": "Your company needs to meet strict compliance requirements by centrally collecting and analyzing audit logs for all administrative activities and data access across your Google Cloud organization. Which service primarily enables this?",
            "options": [
                "Cloud Logging",
                "Cloud Monitoring",
                "Cloud Audit Logs",
                "Security Command Center"
            ],
            "correctAnswer": "Cloud Audit Logs",
            "feedback": {
                "Cloud Logging": "Incorrect. Cloud Logging is the broader service that stores logs, but Cloud Audit Logs is the specific feature that generates the required compliance and security logs.",
                "Cloud Monitoring": "Incorrect. This service is for performance metrics, not for auditing administrative activities.",
                "Cloud Audit Logs": "Correct! Cloud Audit Logs provides a detailed, immutable trail of who did what, when, and where for all administrative actions and data access events within your Google Cloud environment. This is essential for compliance and security auditing.",
                "Security Command Center": "Incorrect. This is a security management and threat detection platform that consumes data from sources like Cloud Audit Logs, but it is not the source of the audit logs itself."
            }
        },
        {
            "title": "Q78: Automating VM Creation for Development Environment",
            "description": "Your development team frequently needs to spin up and tear down new Compute Engine VMs for testing new features. They need a quick and automated way to create these VMs with a consistent base configuration. Which approach should you use?",
            "options": [
                "Manual VM creation via Cloud Console.",
                "Use instance templates to define consistent VM configurations.",
                "Write a gcloud compute instances create script for each VM.",
                "Create custom VM images and launch from them."
            ],
            "correctAnswer": "Use instance templates to define consistent VM configurations.",
            "feedback": {
                "Manual VM creation via Cloud Console.": "Incorrect. This is slow, inconsistent, and not automated.",
                "Use instance templates to define consistent VM configurations.": "Correct! An instance template is a reusable resource that defines the properties of a VM instance (machine type, image, disks, etc.). It's the ideal way to ensure that all development VMs are created with a consistent, automated configuration.",
                "Write a gcloud compute instances create script for each VM.": "Incorrect. While this is a form of automation, it's less manageable and reusable than a centrally defined instance template.",
                "Create custom VM images and launch from them.": "Incorrect. A custom image is a good idea for the base configuration, but the instance template is the resource that defines how to launch a VM from that image, including machine type and other settings."
            }
        },
        {
            "title": "Q79: Network Access Control to Specific Service Ports",
            "description": "You have an application running on Compute Engine where multiple services listen on different ports (e.g., web on 80, admin on 8080). You need to allow external HTTP traffic only to the web service (port 80) and block all other incoming traffic by default. Which networking feature is best for this?",
            "options": [
                "Network Tags",
                "Firewall Rules",
                "VPC Service Controls",
                "Cloud Armor"
            ],
            "correctAnswer": "Firewall Rules",
            "feedback": {
                "Network Tags": "Incorrect. Tags are used to apply firewall rules to instances, but they are not the rules themselves.",
                "Firewall Rules": "Correct! VPC Firewall Rules are the primary tool for controlling network traffic at the port and protocol level. You would create an ingress rule to allow TCP traffic on port 80 from the desired source IPs, and the default implicit deny rule would block all other traffic.",
                "VPC Service Controls": "Incorrect. This is for creating a security perimeter around managed services, not for controlling port-level access to VMs.",
                "Cloud Armor": "Incorrect. This is a WAF and DDoS protection service that operates at a higher level. Basic port filtering is handled by firewall rules."
            }
        },
        {
            "title": "Q80: Multi-Region Data Consistency for Global App",
            "description": "Your company is deploying a global application with users across multiple continents. This application requires strong transactional consistency for its data, even across different geographic regions. Which database should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud BigQuery",
                "Google Cloud Spanner",
                "Google Cloud Bigtable"
            ],
            "correctAnswer": "Google Cloud Spanner",
            "feedback": {
                "Google Cloud SQL": "Incorrect. Cloud SQL is a regional service and cannot provide strong transactional consistency across different regions.",
                "Google Cloud BigQuery": "Incorrect. This is an analytical data warehouse, not a transactional (OLTP) database.",
                "Google Cloud Spanner": "Correct! Cloud Spanner is a unique, globally distributed relational database that is specifically designed to provide strong, transactional (ACID) consistency across multiple regions, making it the perfect choice for this requirement.",
                "Google Cloud Bigtable": "Incorrect. This is a NoSQL database that does not provide strong transactional consistency."
            }
        },
        {
            "title": "Q81: Data Archiving and Long-Term Retention (Extremely Rare Access)",
            "description": "Your company has petabytes of old operational data that must be kept for legal and compliance reasons for 10+ years. This data is almost never accessed (less than once per decade). Cost-effectiveness for storage is the absolute highest priority. Which Cloud Storage class should you use?",
            "options": [
                "Standard",
                "Nearline",
                "Coldline",
                "Archive"
            ],
            "correctAnswer": "Archive",
            "feedback": {
                "Standard": "Incorrect. This would be prohibitively expensive for data that is almost never accessed.",
                "Nearline": "Incorrect. This is for data accessed monthly. Archive is far more cost-effective for this use case.",
                "Coldline": "Incorrect. This is for data accessed quarterly. Archive offers even lower storage costs.",
                "Archive": "Correct! Archive storage has the absolute lowest cost for data at rest, making it the perfect choice for long-term retention of data that is very rarely accessed."
            }
        },
        {
            "title": "Q82: Automating Infrastructure Provisioning for New Projects",
            "description": "Your organization frequently spins up new Google Cloud projects for different teams and applications. You need to standardize and automate the provisioning of common infrastructure (e.g., VPC networks, subnets, basic IAM roles) in these new projects. Which tool or service should you use?",
            "options": [
                "Cloud Console",
                "gcloud CLI scripts",
                "Cloud Deployment Manager",
                "Cloud Functions"
            ],
            "correctAnswer": "Cloud Deployment Manager",
            "feedback": {
                "Cloud Console": "Incorrect. The Cloud Console is a manual, UI-based tool and is not suitable for automated, standardized provisioning.",
                "gcloud CLI scripts": "Incorrect. While scripts can provide automation, they are imperative ('do this, then do that') rather than declarative. IaC tools are better for defining a standard state.",
                "Cloud Deployment Manager": "Correct! Deployment Manager (or Terraform) is an Infrastructure as Code (IaC) tool that allows you to create reusable templates for your standard infrastructure. This ensures every new project is provisioned consistently and automatically.",
                "Cloud Functions": "Incorrect. Cloud Functions are for running event-driven code, not for provisioning infrastructure."
            }
        },
        {
            "title": "Q83: Performance Bottleneck in High-Traffic Web App (Database)",
            "description": "Your high-traffic web application is experiencing performance bottlenecks. Monitoring shows high latency and CPU utilization on the Cloud SQL database instance, especially during peak hours. The application is read-heavy. What is the most effective way to address this?",
            "options": [
                "Upgrade the Cloud SQL instance machine type.",
                "Add read replicas to the Cloud SQL instance.",
                "Migrate the database to Cloud Spanner.",
                "Implement a caching layer with Cloud Memorystore."
            ],
            "correctAnswer": "Add read replicas to the Cloud SQL instance.",
            "feedback": {
                "Upgrade the Cloud SQL instance machine type.": "Incorrect. This is 'vertical scaling' and while it might help temporarily, 'horizontal scaling' with replicas is a more scalable and often more cost-effective solution for a read-heavy workload.",
                "Add read replicas to the Cloud SQL instance.": "Correct! Since the application is read-heavy, adding read replicas allows you to distribute the read load across multiple database instances. This directly reduces the CPU utilization on the primary instance and improves read performance.",
                "Migrate the database to Cloud Spanner.": "Incorrect. This is a very complex and costly solution for a problem that can be easily solved with read replicas.",
                "Implement a caching layer with Cloud Memorystore.": "Incorrect. While adding a cache is an excellent strategy for improving performance, the question specifically points to high CPU on the database instance itself. Addressing the database load directly with replicas is the most effective first step."
            }
        },
        {
            "title": "Q84: Securing Access to Cloud Resources for Developers",
            "description": "Your development team needs access to various Google Cloud resources (Compute Engine, Cloud Storage, Cloud SQL) in a specific project. You want to grant them only the necessary permissions (least privilege) and ensure their access is regularly reviewed. Which IAM feature should you use?",
            "options": [
                "Grant them Project Owner role.",
                "Assign predefined roles or create custom roles.",
                "Use service accounts for each developer.",
                "Share your personal Google account with them."
            ],
            "correctAnswer": "Assign predefined roles or create custom roles.",
            "feedback": {
                "Grant them Project Owner role.": "Incorrect. The Owner role grants full control over all resources in the project, which is a major violation of the principle of least privilege.",
                "Assign predefined roles or create custom roles.": "Correct! This is the best practice. You should grant specific, predefined roles (e.g., `roles/compute.instanceAdmin`, `roles/storage.objectAdmin`) or create custom roles that bundle only the exact permissions the developers need.",
                "Use service accounts for each developer.": "Incorrect. Service accounts are intended for applications and services to authenticate, not for individual human users.",
                "Share your personal Google account with them.": "Incorrect. This is extremely insecure and violates all security and auditing best practices."
            }
        },
        {
            "title": "Q85: Real-time Data Ingestion for IoT Analytics",
            "description": "Your IoT solution generates millions of sensor readings per second. You need a scalable, high-throughput service to ingest this continuous stream of data for immediate processing and storage. Which service is best suited for raw data ingestion?",
            "options": [
                "Cloud Storage",
                "Cloud Pub/Sub",
                "Cloud SQL",
                "Cloud Bigtable"
            ],
            "correctAnswer": "Cloud Pub/Sub",
            "feedback": {
                "Cloud Storage": "Incorrect. Cloud Storage is not designed for ingesting millions of small, real-time events per second.",
                "Cloud Pub/Sub": "Correct! Cloud Pub/Sub is a global, fully managed messaging service designed for high-throughput, real-time data ingestion. It can easily scale to handle millions of messages per second from distributed IoT devices.",
                "Cloud SQL": "Incorrect. A relational database like Cloud SQL cannot handle the massive write throughput of a large-scale IoT solution.",
                "Cloud Bigtable": "Incorrect. Bigtable is a database where the data might eventually be stored, but Pub/Sub is the service used for the initial ingestion of the real-time stream."
            }
        },
        {
            "title": "Q86: Managing Container Images for CI/CD Pipeline",
            "description": "Your CI/CD pipeline builds new Docker container images for your application deployed on GKE. You need a secure, private, and managed registry to store these images, integrated with Cloud Build. Which service should you use?",
            "options": [
                "Docker Hub Public Registry",
                "Container Registry (Artifact Registry)",
                "Cloud Storage",
                "GitHub Packages"
            ],
            "correctAnswer": "Container Registry (Artifact Registry)",
            "feedback": {
                "Docker Hub Public Registry": "Incorrect. This is a public registry and is not suitable for storing private, proprietary container images.",
                "Container Registry (Artifact Registry)": "Correct! Artifact Registry is Google Cloud's managed repository for all types of artifacts, including Docker images. It is private, secure, integrates seamlessly with Cloud Build and GKE, and provides features like vulnerability scanning.",
                "Cloud Storage": "Incorrect. While you can technically store image layers in a bucket, it is not a container registry and lacks the necessary APIs and features for managing images.",
                "GitHub Packages": "Incorrect. This is a third-party service. The native, integrated GCP solution is Artifact Registry."
            }
        },
        {
            "title": "Q87: Distributed Transaction Management (Globally Consistent)",
            "description": "Your application requires transactional updates across multiple geographically dispersed databases with strong consistency. You need a database that ensures all parts of a distributed transaction commit or rollback together globally. Which database should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Firestore"
            ],
            "correctAnswer": "Google Cloud Spanner",
            "feedback": {
                "Google Cloud SQL": "Incorrect. Cloud SQL is a regional database and does not support globally distributed transactions with strong consistency.",
                "Google Cloud Spanner": "Correct! This is the primary use case for Cloud Spanner. It is a globally distributed, relational database that provides external consistency and ACID transactions across multiple regions and continents.",
                "Google Cloud Bigtable": "Incorrect. As a NoSQL database, Bigtable does not support multi-row transactions or global strong consistency.",
                "Google Cloud Firestore": "Incorrect. Firestore supports transactions, but they are limited in scope and do not provide the global, strong ACID guarantees of Spanner."
            }
        },
        {
            "title": "Q88: Cost Optimization for Infrequent Batch Processing",
            "description": "You run a large batch processing job on Compute Engine that is executed only once a month and takes several hours to complete. This job can tolerate interruptions. You want to minimize the cost of running this job. What should you use?",
            "options": [
                "Standard Compute Engine VMs",
                "Preemptible Compute Engine VMs",
                "Custom machine types",
                "Sole-tenant nodes"
            ],
            "correctAnswer": "Preemptible Compute Engine VMs",
            "feedback": {
                "Standard Compute Engine VMs": "Incorrect. These are full-priced and not the most cost-effective option for a workload that can be interrupted.",
                "Preemptible Compute Engine VMs": "Correct! Preemptible VMs (PVMs) offer very large discounts (up to 80%) because Google can reclaim them at any time. Since the batch job is fault-tolerant and can be interrupted, PVMs provide the most significant cost savings.",
                "Custom machine types": "Incorrect. This helps avoid paying for unused resources but does not provide the deep discounts that PVMs offer for interruptible jobs.",
                "Sole-tenant nodes": "Incorrect. These are extremely expensive and are used for specific compliance or licensing needs."
            }
        },
        {
            "title": "Q89: Network Security for Inbound Traffic to Web Servers",
            "description": "You have web servers running on Compute Engine behind an HTTP(S) Load Balancer. You need to protect these web servers from common web exploits (e.g., SQL injection, cross-site scripting) and DDoS attacks. Which service should you use?",
            "options": [
                "VPC Firewall Rules",
                "Cloud VPN",
                "Cloud Armor",
                "Identity-Aware Proxy (IAP)"
            ],
            "correctAnswer": "Cloud Armor",
            "feedback": {
                "VPC Firewall Rules": "Incorrect. Firewall rules operate at Layer 3/4 (IP address, port) and cannot inspect traffic for Layer 7 web exploits like SQL injection.",
                "Cloud VPN": "Incorrect. This is for secure network connectivity, not for protecting a web application from attacks.",
                "Cloud Armor": "Correct! Cloud Armor is Google's Web Application Firewall (WAF) and DDoS mitigation service. It integrates with the Global HTTP(S) Load Balancer to protect applications from common web attacks and provides robust DDoS protection.",
                "Identity-Aware Proxy (IAP)": "Incorrect. IAP is for controlling access based on user identity, not for protecting against web exploits."
            }
        },
        {
            "title": "Q90: Real-time Data Analytics on Streaming Data",
            "description": "Your application generates high-volume, real-time data streams. You need to perform immediate analysis on this data to derive insights and trigger alerts. Which services are best suited for building such a real-time analytics pipeline?",
            "options": [
                "Cloud Storage and BigQuery",
                "Pub/Sub and Dataflow",
                "Cloud SQL and Dataproc",
                "Cloud Bigtable and Cloud Functions"
            ],
            "correctAnswer": "Pub/Sub and Dataflow",
            "feedback": {
                "Cloud Storage and BigQuery": "Incorrect. This is a pattern for batch analytics, not for real-time analysis of streaming data.",
                "Pub/Sub and Dataflow": "Correct! This is the canonical Google Cloud architecture for stream analytics. Pub/Sub ingests the real-time stream, and Dataflow consumes from it to perform complex analysis, transformations, and aggregations in real time.",
                "Cloud SQL and Dataproc": "Incorrect. This combination is not suited for real-time stream processing.",
                "Cloud Bigtable and Cloud Functions": "Incorrect. While Cloud Functions can process events, Dataflow is the service designed for building complex, scalable, and resilient stream analytics pipelines."
            }
        },
        {
            "title": "Q91: Migrating from Oracle Database to Cloud (Managed, Compatibility)",
            "description": "Your company needs to migrate an on-premises Oracle database to Google Cloud. They are looking for a fully managed relational database service that supports Oracle compatibility and tools. Which service should you choose?",
            "options": [
                "Google Cloud SQL for MySQL",
                "Google Cloud Spanner",
                "Google Cloud SQL for PostgreSQL",
                "Bare Metal Solution for Oracle"
            ],
            "correctAnswer": "Bare Metal Solution for Oracle",
            "feedback": {
                "Google Cloud SQL for MySQL": "Incorrect. This is a different database engine.",
                "Google Cloud Spanner": "Incorrect. This would require a major re-architecture of the application and database schema.",
                "Google Cloud SQL for PostgreSQL": "Incorrect. While there are tools to migrate from Oracle to PostgreSQL, it's a complex process and doesn't offer native Oracle compatibility.",
                "Bare Metal Solution for Oracle": "Correct! This service provides Oracle-certified bare metal hardware in Google's data centers, allowing you to lift-and-shift your existing Oracle databases with minimal changes. It offers the highest level of compatibility for complex Oracle workloads."
            }
        },
        {
            "title": "Q92: Implementing DevOps Principles for Rapid Development",
            "description": "Your development team wants to adopt DevOps principles to achieve faster release cycles, improved collaboration, and automated deployments. Which Google Cloud services align with these goals?",
            "options": [
                "Cloud Source Repositories, Cloud Build, and Cloud Deploy.",
                "Compute Engine, Cloud Storage, and Cloud SQL.",
                "Cloud Functions, Cloud Run, and App Engine.",
                "Cloud Identity, Cloud DNS, and Cloud VPN."
            ],
            "correctAnswer": "Cloud Source Repositories, Cloud Build, and Cloud Deploy.",
            "feedback": {
                "Cloud Source Repositories, Cloud Build, and Cloud Deploy.": "Correct! This combination represents a complete, cloud-native DevOps toolchain. Cloud Source Repositories for version control, Cloud Build for continuous integration, and Cloud Deploy for continuous delivery.",
                "Compute Engine, Cloud Storage, and Cloud SQL.": "Incorrect. These are core infrastructure services, not tools specifically for implementing DevOps practices.",
                "Cloud Functions, Cloud Run, and App Engine.": "Incorrect. These are serverless compute platforms, which can be part of a DevOps workflow, but they are not the tools that manage the pipeline itself.",
                "Cloud Identity, Cloud DNS, and Cloud VPN.": "Incorrect. These are identity and networking services, not part of the core application development and deployment pipeline."
            }
        },
        {
            "title": "Q93: Managing Network Access to VMs by Service Accounts",
            "description": "You have Compute Engine VMs running different microservices. You want to control network access between these VMs based on the service accounts attached to them (e.g., service A can talk to service B, but not service C). Which feature should you use?",
            "options": [
                "Network Tags",
                "Firewall Rules",
                "Service Accounts for Firewall Rules",
                "VPC Service Controls"
            ],
            "correctAnswer": "Service Accounts for Firewall Rules",
            "feedback": {
                "Network Tags": "Incorrect. Tags are simple string labels. Service accounts provide a stronger, more secure identity for a VM.",
                "Firewall Rules": "Incorrect. This is too general. The specific feature that enables identity-based control is using service accounts within the rules.",
                "Service Accounts for Firewall Rules": "Correct! You can create firewall rules that specify a source or target service account. This allows you to create fine-grained, identity-based network policies that are more secure and manageable than rules based on IP addresses or tags.",
                "VPC Service Controls": "Incorrect. This is for creating a security perimeter around managed services, not for controlling VM-to-VM traffic based on identity."
            }
        },
        {
            "title": "Q94: Choosing Storage for User-Generated Content (Web App)",
            "description": "Your web application allows users to upload images and videos. You need a scalable and cost-effective storage solution for this user-generated content, which will be accessed frequently from the web app. Which service should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Bigtable",
                "Google Cloud Storage (Standard)",
                "Google Cloud Filestore"
            ],
            "correctAnswer": "Google Cloud Storage (Standard)",
            "feedback": {
                "Google Cloud SQL": "Incorrect. A relational database is not designed or cost-effective for storing large binary files like images and videos.",
                "Google Cloud Bigtable": "Incorrect. This is a NoSQL database for structured or semi-structured data, not for storing unstructured blobs like media files.",
                "Google Cloud Storage (Standard)": "Correct! Cloud Storage is a highly scalable and durable object store, perfect for user-generated content. The Standard storage class ensures low latency and high availability for frequently accessed files.",
                "Google Cloud Filestore": "Incorrect. This is a managed NFS file share, which is more expensive and less scalable for this web-based use case than Cloud Storage."
            }
        },
        {
            "title": "Q95: Disaster Recovery for Cloud SQL (Cross-Region)",
            "description": "Your critical application uses Cloud SQL for MySQL. You need a disaster recovery strategy that protects against a regional outage, ensuring high availability and minimal data loss. What should you configure?",
            "options": [
                "Automated daily backups.",
                "Point-in-Time Recovery.",
                "Cross-region read replica.",
                "High Availability (HA) with a cross-region failover replica."
            ],
            "correctAnswer": "High Availability (HA) with a cross-region failover replica.",
            "feedback": {
                "Automated daily backups.": "Incorrect. This would result in significant data loss (high RPO) and downtime (high RTO) in a regional disaster.",
                "Point-in-Time Recovery.": "Incorrect. This is for recovering from logical errors, not for fast recovery from a regional outage.",
                "Cross-region read replica.": "Incorrect. While this places data in another region, the replication is asynchronous (meaning some data loss is possible) and the failover process is manual, leading to a higher RTO.",
                "High Availability (HA) with a cross-region failover replica.": "Correct! For regional DR, configuring a cross-region replica and promoting it during a disaster provides the best RPO/RTO for minimal data loss and rapid recovery."
            }
        },
        {
            "title": "Q96: Real-time Dashboards for Application Metrics",
            "description": "Your operations team needs a real-time dashboard to visualize key performance metrics (CPU, memory, network, custom application metrics) for your services running on Compute Engine and GKE. Which Google Cloud service should you primarily use to build this?",
            "options": [
                "Cloud Logging",
                "Cloud Monitoring",
                "Cloud Trace",
                "Cloud Dataflow"
            ],
            "correctAnswer": "Cloud Monitoring",
            "feedback": {
                "Cloud Logging": "Incorrect. For logs, not metrics visualization.",
                "Cloud Monitoring": "Correct! Cloud Monitoring collects and visualizes metrics from all GCP services (Compute Engine, GKE, etc.), allowing custom dashboards for real-time performance monitoring.",
                "Cloud Trace": "Incorrect. For distributed tracing (latency analysis), not broad metric dashboards.",
                "Cloud Dataflow": "Incorrect. For data processing, not monitoring dashboards."
            }
        },
        {
            "title": "Q97: Choosing a Database for Customer Profiles (Flexible Schema)",
            "description": "Your new application needs to store customer profile data. This data has a flexible schema (new attributes may be added frequently) and needs to be highly scalable for millions of users. It requires low-latency reads and writes. Which database should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Firestore"
            ],
            "correctAnswer": "Google Cloud Firestore",
            "feedback": {
                "Google Cloud SQL": "Incorrect. Relational, requires fixed schema, less scalable for millions of flexible profiles.",
                "Google Cloud Spanner": "Incorrect. Relational, strong ACID for complex transactions, less flexible schema.",
                "Google Cloud Bigtable": "Incorrect. Wide-column, best for very high throughput analytical/time-series data, not typical for flexible customer profiles.",
                "Google Cloud Firestore": "Correct! Firestore is a flexible, scalable NoSQL document database with low-latency reads/writes, automatic scaling, and is ideal for storing user profiles with evolving (flexible) schemas."
            }
        },
        {
            "title": "Q98: Automating VM Deployment with Custom Images",
            "description": "You need to deploy new Compute Engine VMs that come pre-installed with specific software and configurations unique to your environment. You want to automate this process to ensure consistency. What is the most efficient way to achieve this?",
            "options": [
                "Create VMs from public images and run a startup script.",
                "Manually install software on each VM.",
                "Create a custom image from a configured VM.",
                "Use Cloud Build to deploy VMs."
            ],
            "correctAnswer": "Create a custom image from a configured VM.",
            "feedback": {
                "Create VMs from public images and run a startup script.": "Incorrect. Startup scripts add boot time; less efficient for pre-installed software than a custom image.",
                "Manually install software on each VM.": "Incorrect. Inefficient, error-prone, not automated or consistent.",
                "Create a custom image from a configured VM.": "Correct. A custom (golden) image captures a VM's specific OS, software, and configurations. New VMs launched from this image will be identical and consistent.",
                "Use Cloud Build to deploy VMs.": "Incorrect. Cloud Build can trigger VM deployments, but doesn't define the VM's contents. It would still rely on images/scripts."
            }
        },
        {
            "title": "Q99: Network Design for Hybrid Connectivity (High Traffic, Multiple VPCs)",
            "description": "Your company has a large on-premises data center with high traffic volume. You have multiple Google Cloud projects, each with its own VPC network, and you need high-bandwidth, private connectivity from on-premises to all these VPCs. Which networking solution should you implement?",
            "options": [
                "Cloud VPN to each VPC.",
                "Dedicated Interconnect with VPC Network Peering.",
                "Partner Interconnect with Cloud VPN.",
                "Shared VPC with Cloud VPN."
            ],
            "correctAnswer": "Dedicated Interconnect with VPC Network Peering.",
            "feedback": {
                "Cloud VPN to each VPC.": "Incorrect. Cloud VPN uses public internet (not private) and often doesn't provide the 'high bandwidth' required.",
                "Dedicated Interconnect with VPC Network Peering.": "Correct! Dedicated Interconnect provides high-bandwidth, private connection from on-premises to a single VPC. VPC Network Peering then allows that single interconnected VPC to privately communicate with other VPCs across different projects, providing high-bandwidth private connectivity to all.",
                "Partner Interconnect with Cloud VPN.": "Incorrect. Partner Interconnect is private, but combining with Cloud VPN is redundant and less scalable for 'high traffic, multiple VPCs.'",
                "Shared VPC with Cloud VPN.": "Incorrect. Shared VPC shares one VPC to multiple projects; it doesn't solve the high-bandwidth private connection from on-premises if that VPC is connected via VPN."
            }
        },
        {
            "title": "Q100: Choosing a Database for User Activity Feed (Time-Series)",
            "description": "Your application needs to store a high-volume, real-time user activity feed (e.g., likes, comments, shares) that needs to be quickly queryable by user ID and timestamp. This data is primarily append-only. Which database should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Firestore"
            ],
            "correctAnswer": "Google Cloud Bigtable",
            "feedback": {
                "Google Cloud SQL": "Incorrect. Relational, struggles with extreme scale of activity feeds.",
                "Google Cloud Spanner": "Incorrect. Designed for strong ACID transactions, overkill for a high-volume append-only activity feed.",
                "Google Cloud Bigtable": "Correct! Bigtable is a wide-column NoSQL database optimized for very high write throughput, low-latency reads on time-series data, and is excellent for append-only data like activity feeds queryable by row key (user ID + timestamp).",
                "Google Cloud Firestore": "Incorrect. Document database, typically for smaller transactions/app data, not the extreme throughput of activity feeds."
            }
        },
        {
            "title": "Q101: Centralized Log Management for Multiple GCP Projects",
            "description": "Your company has several Google Cloud projects for different environments (dev, test, prod). You need to centralize all logs (including custom application logs and GCP service logs) from these projects into a single location for unified analysis and long-term storage. Which service should you primarily use for collection and routing?",
            "options": [
                "Cloud Storage",
                "Cloud Logging",
                "BigQuery",
                "Cloud SQL"
            ],
            "correctAnswer": "Cloud Logging",
            "feedback": {
                "Cloud Storage": "Incorrect. Can store logs, but is a storage service, not the primary collection/routing service.",
                "Cloud Logging": "Correct. Cloud Logging (specifically Log Sinks/aggregated exports) is designed to collect, centralize, and route logs from multiple projects to a single destination (e.g., another project's Logging bucket, BigQuery, or Pub/Sub).",
                "BigQuery": "Incorrect. For analysis of logs, not primary collection/routing.",
                "Cloud SQL": "Incorrect. Relational DB, not for high-volume log aggregation."
            }
        },
        {
            "title": "Q102: Data Warehousing for Business Intelligence (SQL)",
            "description": "Your business intelligence team needs to perform complex analytical queries using standard SQL on petabytes of historical sales data. They require a fully managed, highly scalable, and cost-effective data warehouse solution for their BI tools. Which service should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google BigQuery"
            ],
            "correctAnswer": "Google BigQuery",
            "feedback": {
                "Google Cloud SQL": "Incorrect. For OLTP, limited scale for petabyte warehousing.",
                "Google Cloud Spanner": "Incorrect. For global transactions, not core data warehousing analytics.",
                "Google Cloud Bigtable": "Incorrect. NoSQL for operational/time-series, not a relational data warehouse for BI.",
                "Google BigQuery": "Correct. Serverless, petabyte-scale data warehouse optimized for complex SQL analytical queries, making it ideal for BI workloads."
            }
        },
        {
            "title": "Q103: Automating Continuous Deployment to GKE",
            "description": "You have a CI pipeline that builds Docker images and pushes them to Artifact Registry. You need to automate the deployment of these new images to your Google Kubernetes Engine (GKE) clusters in a controlled and progressive manner (e.g., canary deployments, rollbacks). Which service should you use for continuous delivery?",
            "options": [
                "Cloud Build",
                "Cloud Deploy",
                "Cloud Functions",
                "Compute Engine"
            ],
            "correctAnswer": "Cloud Deploy",
            "feedback": {
                "Cloud Build": "Incorrect. For Continuous Integration (building/testing images), not Continuous Deployment to GKE clusters.",
                "Cloud Deploy": "Correct! Cloud Deploy is a fully managed service for continuous delivery to GKE (and Cloud Run), providing automated multi-environment deployments, progressive rollouts (canary/blue-green), and rollback capabilities.",
                "Cloud Functions": "Incorrect. For event-driven functions, not managing GKE deployments.",
                "Compute Engine": "Incorrect. IaaS, doesn't provide managed deployment automation for GKE."
            }
        },
        {
            "title": "Q104: Secure Access for On-Prem Users to Internal Web Apps",
            "description": "Your on-premises employees need secure and identity-aware access to internal web applications hosted on Google Cloud, without requiring them to use a traditional VPN client. Which Google Cloud service enables this?",
            "options": [
                "Cloud VPN",
                "Direct Peering",
                "Identity-Aware Proxy (IAP)",
                "Cloud Interconnect"
            ],
            "correctAnswer": "Identity-Aware Proxy (IAP)",
            "feedback": {
                "Cloud VPN": "Incorrect. Requires VPN client setup.",
                "Direct Peering": "Incorrect. For public Google services, not private internal app access.",
                "Identity-Aware Proxy (IAP)": "Correct! IAP provides secure, identity-based access to internal web applications/resources, allowing users to connect from anywhere without a VPN client.",
                "Cloud Interconnect": "Incorrect. For network connectivity, not identity-aware application access."
            }
        },
        {
            "title": "Q105: Choosing Storage for Streaming Data Buffering",
            "description": "You have a high-volume streaming data pipeline. Data is ingested continuously and needs to be temporarily buffered before being processed by a stream processing engine. You need a highly scalable and reliable buffering solution. Which service should you use for buffering?",
            "options": [
                "Cloud Storage",
                "Cloud SQL",
                "Cloud Pub/Sub",
                "Cloud Bigtable"
            ],
            "correctAnswer": "Cloud Pub/Sub",
            "feedback": {
                "Cloud Storage": "Incorrect. For static file storage, not real-time stream buffering.",
                "Cloud SQL": "Incorrect. Transactional DB, not for high-volume stream buffering.",
                "Cloud Pub/Sub": "Correct. Fully managed, highly scalable, and durable messaging service perfect for buffering high-volume real-time data streams before consumption by downstream processors.",
                "Cloud Bigtable": "Incorrect. For operational/time-series data storage, not a message queue/buffer."
            }
        },
        {
            "title": "Q106: Database for Mobile Game Leaderboards (High Write, Fast Read)",
            "description": "You are developing a mobile game and need a database for a global leaderboard. This database will experience extremely high write throughput (player scores) and fast read access for displaying top scores. Schema is simple (player ID, score, timestamp). Which database should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Firestore"
            ],
            "correctAnswer": "Google Cloud Bigtable",
            "feedback": {
                "Google Cloud SQL": "Incorrect. Relational, struggles with extreme high write throughput for leaderboards.",
                "Google Cloud Spanner": "Incorrect. Designed for strong ACID transactions, overkill and not cost-optimized for pure high-throughput leaderboard writes.",
                "Google Cloud Bigtable": "Correct. Optimized for very high write and read throughput for large datasets, making it ideal for time-series-like data such as leaderboards.",
                "Google Cloud Firestore": "Incorrect. Document database, typically for smaller transactions/app data, not extreme scale leaderboards."
            }
        },
        {
            "title": "Q107: Optimizing Costs for Development Environment (Idle Time)",
            "description": "Your development team uses Compute Engine VMs for various projects. These VMs are only active during working hours (9 AM - 5 PM, weekdays) and can be turned off outside these times. You want to automate cost optimization for these VMs. What should you do?",
            "options": [
                "Use Preemptible VMs for all dev environments.",
                "Implement an autoscaling policy for each VM.",
                "Schedule VM instance start/stop using Cloud Scheduler and Cloud Functions.",
                "Manually stop VMs at the end of each day."
            ],
            "correctAnswer": "Schedule VM instance start/stop using Cloud Scheduler and Cloud Functions.",
            "feedback": {
                "Use Preemptible VMs for all dev environments.": "Incorrect. Preemptible VMs can be interrupted randomly, unsuitable for active development.",
                "Implement an autoscaling policy for each VM.": "Incorrect. Autoscaling is for fluctuating load, not for scheduled idle time.",
                "Schedule VM instance start/stop using Cloud Scheduler and Cloud Functions.": "Correct. Cloud Scheduler can trigger Cloud Functions (or gcloud commands) to automatically start VMs at 9 AM and stop them at 5 PM, minimizing billing for idle time.",
                "Manually stop VMs at the end of each day.": "Incorrect. Not automated, relies on human action."
            }
        },
        {
            "title": "Q108: Centralized Policy Enforcement for Multi-Project Organization",
            "description": "Your organization has a strict security policy requiring all Compute Engine VMs to have specific network tags for firewall rule application. You need to enforce this policy consistently across all projects created by different teams. Which IAM or Organization Policy feature should you use?",
            "options": [
                "Custom IAM roles on projects.",
                "Conditional IAM policies.",
                "Organization Policy Constraints.",
                "Service Account impersonation."
            ],
            "correctAnswer": "Organization Policy Constraints.",
            "feedback": {
                "Custom IAM roles on projects.": "Incorrect. IAM roles define permissions, not mandatory resource configurations like network tags.",
                "Conditional IAM policies.": "Incorrect. Control when a role applies, but don't enforce resource properties like network tags.",
                "Organization Policy Constraints.": "Correct! Organization Policies allow setting constraints at the Organization or Folder level (e.g., constraints/compute.requireOsLogin, constraints/compute.requireNetworkTags) to enforce consistent configurations and security rules across all projects.",
                "Service Account impersonation.": "Incorrect. For delegated authority, not policy enforcement."
            }
        },
        {
            "title": "Q109: Secure Internal VM Communication (Encryption)",
            "description": "You have multiple microservices running on Compute Engine VMs within your VPC. You need to ensure all internal communication between these microservices is encrypted and mutually authenticated. Which solution provides this?",
            "options": [
                "VPC Firewall Rules.",
                "Shared VPC.",
                "Internal Load Balancer with SSL.",
                "Service Mesh (e.g., Istio) with mTLS."
            ],
            "correctAnswer": "Service Mesh (e.g., Istio) with mTLS.",
            "feedback": {
                "VPC Firewall Rules.": "Incorrect. Control traffic flow, but don't provide encryption or mutual authentication for internal VM communication.",
                "Shared VPC.": "Incorrect. For sharing a VPC network across projects, not for encrypting internal communications.",
                "Internal Load Balancer with SSL.": "Incorrect. Encrypts traffic to the load balancer, but not necessarily point-to-point between all internal VMs/services.",
                "Service Mesh (e.g., Istio) with mTLS.": "Correct! A service mesh like Istio provides features like mutual TLS (mTLS) to automatically encrypt and mutually authenticate all internal service-to-service communication."
            }
        },
        {
            "title": "Q110: Migrating On-Prem Hadoop/Spark Workloads (Managed)",
            "description": "Your data analytics team uses an on-premises Hadoop and Spark cluster for various batch and real-time processing jobs. They want to migrate to a fully managed cloud service to reduce operational burden and enable dynamic scaling. Which Google Cloud service should you choose?",
            "options": [
                "Google Cloud Dataflow",
                "Google Cloud Composer",
                "Google Cloud Dataproc",
                "Google BigQuery"
            ],
            "correctAnswer": "Google Cloud Dataproc",
            "feedback": {
                "Google Cloud Dataflow": "Incorrect. For Apache Beam pipelines, not a direct managed service for existing Hadoop/Spark jobs without re-writing.",
                "Google Cloud Composer": "Incorrect. For workflow orchestration, not a managed Hadoop/Spark service.",
                "Google Cloud Dataproc": "Correct. Fully managed service for running Apache Hadoop, Spark, Flink, and Presto clusters. It's designed for migrating existing Hadoop/Spark workloads to a managed, scalable cloud environment.",
                "Google BigQuery": "Incorrect. Data warehouse for analytics, not a processing engine for Hadoop/Spark jobs."
            }
        },
        {
            "title": "Q111: Managing Customer Identities for a Web Application",
            "description": "Your new web application needs to allow users to sign up, sign in, and manage their profiles. It should support authentication via various providers (e.g., Google, Facebook) and be highly scalable for millions of users. Which service should you use for customer identity management?",
            "options": [
                "Cloud Identity",
                "Identity-Aware Proxy (IAP)",
                "Identity Platform",
                "Cloud IAM"
            ],
            "correctAnswer": "Identity Platform",
            "feedback": {
                "Cloud Identity": "Incorrect. For enterprise workforce identity and device management, not customer-facing.",
                "Identity-Aware Proxy (IAP)": "Incorrect. Provides secure access to applications based on identity, but not a full customer identity management system itself.",
                "Identity Platform": "Correct! Identity Platform is a fully managed customer identity and access management (CIAM) platform built on Firebase Authentication. It supports various sign-in methods, scales for millions of users, and is designed for customer-facing applications.",
                "Cloud IAM": "Incorrect. For managing access to GCP resources, not customer user authentication for applications."
            }
        },
        {
            "title": "Q112: High-Performance Caching for Web Application",
            "description": "Your web application is highly read-heavy and accesses a database frequently. You want to reduce database load and improve response times for repeated data requests. Which caching service should you implement?",
            "options": [
                "Cloud Storage",
                "Cloud SQL",
                "Cloud Memorystore for Redis",
                "Cloud Bigtable"
            ],
            "correctAnswer": "Cloud Memorystore for Redis",
            "feedback": {
                "Cloud Storage": "Incorrect. Object storage, not a caching service.",
                "Cloud SQL": "Incorrect. Relational database, not a caching layer.",
                "Cloud Memorystore for Redis": "Correct! Fully managed in-memory data store for Redis or Memcached. It provides very high performance and low-latency caching capabilities, ideal for reducing database load in read-heavy applications.",
                "Cloud Bigtable": "Incorrect. NoSQL database for large operational/time-series data, not a caching solution."
            }
        },
        {
            "title": "Q113: Automating Infrastructure Updates with Zero Downtime",
            "description": "You manage a critical application on Compute Engine managed instance groups. You need to update the application versions regularly without any downtime to users. Which update strategy should you implement?",
            "options": [
                "Manual update of each instance.",
                "Restart all instances simultaneously.",
                "Rolling update strategy.",
                "Blue/Green deployment."
            ],
            "correctAnswer": "Rolling update strategy.",
            "feedback": {
                "Manual update of each instance.": "Incorrect. Error-prone, not zero-downtime.",
                "Restart all instances simultaneously.": "Incorrect. Causes significant downtime.",
                "Rolling update strategy.": "Correct. Rolling updates (a feature of Managed Instance Groups) allow you to gradually update instances in a group, replacing old instances with new ones while maintaining service availability, ensuring zero downtime.",
                "Blue/Green deployment.": "Correct. Blue/Green is an advanced strategy that can achieve zero-downtime by running two identical environments and switching traffic. While a valid answer, Rolling Update is a more direct feature of MIGs for continuous updates. Given the context of 'update strategy' for MIGs, rolling update is primary."
            }
        },
        {
            "title": "Q114: Data Processing for Stream Processing with Pub/Sub Input",
            "description": "You have a data pipeline that ingests continuous data from Cloud Pub/Sub. This data needs to be processed (e.g., filtered, aggregated) in real-time and then loaded into BigQuery for analysis. Which service is best suited for the real-time processing step?",
            "options": [
                "Google Cloud Dataproc",
                "Google Cloud Dataflow",
                "Google Cloud Composer",
                "Google Cloud Functions"
            ],
            "correctAnswer": "Google Cloud Dataflow",
            "feedback": {
                "Google Cloud Dataproc": "Incorrect. Primarily for Spark/Hadoop batch processing.",
                "Google Cloud Dataflow": "Correct. Dataflow is a fully managed service for executing Apache Beam pipelines, making it ideal for scalable, real-time stream processing from Pub/Sub to destinations like BigQuery.",
                "Google Cloud Composer": "Incorrect. For workflow orchestration, not real-time stream processing.",
                "Google Cloud Functions": "Incorrect. For individual event-driven functions, not continuous stream processing."
            }
        },
        {
            "title": "Q115: Centralized DNS Management for Global Applications",
            "description": "Your company has multiple applications deployed globally across different GCP projects and regions. You need a centralized and highly available DNS service to manage all your domain names and ensure low-latency DNS resolution for users worldwide. Which service should you use?",
            "options": [
                "On-premises DNS servers",
                "Google Cloud DNS",
                "Public DNS providers (e.g., Cloudflare)",
                "Custom DNS server on Compute Engine"
            ],
            "correctAnswer": "Google Cloud DNS",
            "feedback": {
                "On-premises DNS servers": "Incorrect. Introduces single points of failure, higher latency for global users, and management overhead.",
                "Google Cloud DNS": "Correct. Cloud DNS is a highly available, globally distributed, and low-latency DNS service that integrates natively with Google Cloud, making it ideal for managing domain names for global applications.",
                "Public DNS providers (e.g., Cloudflare)": "Incorrect. A valid option but Cloud DNS offers tighter integration with GCP resources.",
                "Custom DNS server on Compute Engine": "Incorrect. Requires manual management of infrastructure, not fully managed or as globally distributed."
            }
        },
        {
            "title": "Q116: Secure API Access Control (Identity-Based)",
            "description": "Your internal APIs are accessed by various client applications and internal users. You need to control access to these APIs based on the identity of the calling application or user, ensuring only authorized identities can invoke specific API methods. Which security mechanism should you primarily use?",
            "options": [
                "API Keys",
                "OAuth 2.0 and Service Accounts",
                "Network Firewall Rules",
                "VPC Service Controls"
            ],
            "correctAnswer": "OAuth 2.0 and Service Accounts",
            "feedback": {
                "API Keys": "Incorrect. Provide project-level identification, but not user/service identity or fine-grained authorization.",
                "OAuth 2.0 and Service Accounts": "Correct. OAuth 2.0 provides a secure framework for delegated authorization. Service accounts (for applications) or user credentials (for humans) authenticated via OAuth 2.0 tokens allow fine-grained, identity-based access control to APIs via IAM.",
                "Network Firewall Rules": "Incorrect. Control network traffic at IP/port level, not identity-based API access.",
                "VPC Service Controls": "Incorrect. Create security perimeters to prevent data exfiltration, not for API access control based on identity."
            }
        },
        {
            "title": "Q117: Cost-Effective Storage for Infrequent Access (Compliance)",
            "description": "Your company needs to store compliance records that are accessed approximately once a month. Retrieval time needs to be in seconds. You need the most cost-effective storage solution. Which Cloud Storage class should you choose?",
            "options": [
                "Standard",
                "Nearline",
                "Coldline",
                "Archive"
            ],
            "correctAnswer": "Nearline",
            "feedback": {
                "Standard": "Incorrect. Too expensive for data accessed monthly.",
                "Nearline": "Correct. Nearline storage is designed for data accessed at most once a month, offering a balance of low storage cost and fast retrieval (seconds), making it ideal for monthly accessed compliance records.",
                "Coldline": "Incorrect. More cost-effective for even less frequent access (quarterly), with slightly higher retrieval cost.",
                "Archive": "Incorrect. For extremely rare access (<1x/year), with highest retrieval cost/latency."
            }
        },
        {
            "title": "Q118: Automating Infrastructure Provisioning with Version Control",
            "description": "Your infrastructure team wants to define their Google Cloud infrastructure (e.g., VMs, networks, load balancers) in code, manage it through version control (e.g., Git), and automate deployments consistently. Which Google Cloud service should they use?",
            "options": [
                "Cloud Console",
                "gcloud CLI",
                "Cloud Deployment Manager",
                "Cloud Functions"
            ],
            "correctAnswer": "Cloud Deployment Manager",
            "feedback": {
                "Cloud Console": "Incorrect. Manual, not code-based or version-controlled.",
                "gcloud CLI": "Incorrect. Imperative; requires scripting, not declarative or inherently version-controlled for infrastructure state.",
                "Cloud Deployment Manager": "Correct. Google Cloud's native Infrastructure as Code (IaC) service allows you to define resources declaratively in templates (code), integrate with version control, and automate deployments for consistent infrastructure.",
                "Cloud Functions": "Incorrect. For event-driven applications, not infrastructure provisioning."
            }
        },
        {
            "title": "Q119: Secure Access to Internal HTTP/S Applications via Web Browser",
            "description": "Your employees need to access internal web applications securely over the public internet using only a web browser, without relying on VPN software. Access should be based on their corporate identity. Which Google Cloud service enables this?",
            "options": [
                "Cloud VPN",
                "Cloud Load Balancing",
                "Identity-Aware Proxy (IAP)",
                "Cloud DNS"
            ],
            "correctAnswer": "Identity-Aware Proxy (IAP)",
            "feedback": {
                "Cloud VPN": "Incorrect. Requires VPN software/client.",
                "Cloud Load Balancing": "Incorrect. Distributes traffic, doesn't provide identity-based access control for internal apps.",
                "Identity-Aware Proxy (IAP)": "Correct. IAP allows secure, identity-aware access to internal web applications (and other resources) directly from the internet using a browser, eliminating VPN client requirements.",
                "Cloud DNS": "Incorrect. Manages domain names."
            }
        },
        {
            "title": "Q120: Managing Multiple Google Cloud Projects (Hierarchical Structure)",
            "description": "Your organization has grown to manage many Google Cloud projects. You need to organize these projects into a logical hierarchy to apply policies, manage billing, and control access more efficiently across different departments and teams. Which feature should you use?",
            "options": [
                "Project Labels",
                "Resource Manager Folders",
                "Billing Accounts",
                "Service Accounts"
            ],
            "correctAnswer": "Resource Manager Folders",
            "feedback": {
                "Project Labels": "Incorrect. For tagging resources within projects for filtering/cost analysis, not hierarchical organization.",
                "Resource Manager Folders": "Correct! Folders (part of the Resource Hierarchy) allow you to group projects into logical units, enabling policy inheritance, centralized management, and organized billing across multiple projects.",
                "Billing Accounts": "Incorrect. Manages billing relationships, not hierarchical project organization.",
                "Service Accounts": "Incorrect. For application identity, not for organizing projects."
            }
        },
        {
            "title": "Q121: Data Retention for Compliance (Infrequent Access, Long Term)",
            "description": "Your company has a large volume of historical operational data that needs to be retained for compliance reasons for several decades. This data is rarely, if ever, accessed (less than once every few years). Cost minimization for storage is the primary concern. Which Cloud Storage class should you use?",
            "options": [
                "Standard",
                "Nearline",
                "Coldline",
                "Archive"
            ],
            "correctAnswer": "Archive",
            "feedback": {
                "Standard": "Incorrect. Too expensive for rarely accessed data.",
                "Nearline": "Incorrect. For data accessed monthly.",
                "Coldline": "Incorrect. For data accessed quarterly.",
                "Archive": "Correct! Offers the absolute lowest storage cost for data accessed extremely rarely (<1x/year), ideal for very long-term, cold archives."
            }
        },
        {
            "title": "Q122: Multi-Region High Availability for Compute Engine (Web App)",
            "description": "You have a critical web application running on Compute Engine instances behind a regional HTTP(S) Load Balancer. You need to expand its availability to a second region to protect against regional outages and improve latency for users in that new region. How should you design for this?",
            "options": [
                "Set up a Multi-Region Load Balancer and deploy managed instance groups in the second region.",
                "Create snapshots of disks and restore them to VMs in the second region during an outage.",
                "Configure Cloud VPN connections between the two regions.",
                "Manually create VMs and deploy the application in the second region when needed."
            ],
            "correctAnswer": "Set up a Multi-Region Load Balancer and deploy managed instance groups in the second region.",
            "feedback": {
                "Set up a Multi-Region Load Balancer and deploy managed instance groups in the second region.": "Correct. A Multi-Region Load Balancer distributes traffic across regions for global availability and disaster recovery. Deploying MIGs in the second region provides the necessary backend capacity.",
                "Create snapshots of disks and restore them to VMs in the second region during an outage.": "Incorrect. Higher RTO; not for minimizing downtime or improving latency for the second region proactively.",
                "Configure Cloud VPN connections between the two regions.": "Incorrect. VPNs are for secure private connections, not for load balancing or global application availability.",
                "Manually create VMs and deploy the application in the second region when needed.": "Incorrect. Not automated, high RTO, not for continuous availability."
            }
        },
        {
            "title": "Q123: Secure Access to Internal HTTP/S App without VPN",
            "description": "Your internal employees need to securely access a web application running on Compute Engine instances that are in a private subnet, without using a VPN. Access needs to be authenticated using their corporate identities. Which service enables this?",
            "options": [
                "Cloud VPN",
                "Cloud Interconnect",
                "Identity-Aware Proxy (IAP)",
                "Cloud Armor"
            ],
            "correctAnswer": "Identity-Aware Proxy (IAP)",
            "feedback": {
                "Cloud VPN": "Incorrect. Requires VPN client.",
                "Cloud Interconnect": "Incorrect. For network connectivity, not identity-based application access.",
                "Identity-Aware Proxy (IAP)": "Correct! IAP provides secure, identity-based access to internal web applications (and other GCP resources) over the public internet, eliminating VPN requirements for authenticated users.",
                "Cloud Armor": "Incorrect. DDoS protection and WAF, doesn't provide identity-based access without VPN."
            }
        },
        {
            "title": "Q124: Centralized Performance Monitoring and Alerting",
            "description": "Your operations team needs a unified view of performance metrics (CPU, memory, network, custom app metrics) and the ability to set up alerts for your applications across Compute Engine, GKE, and potentially on-premises servers. Which service should you primarily use?",
            "options": [
                "Cloud Logging",
                "Cloud Monitoring",
                "Cloud Trace",
                "Cloud Audit Logs"
            ],
            "correctAnswer": "Cloud Monitoring",
            "feedback": {
                "Cloud Logging": "Incorrect. Primarily for collecting logs, not comprehensive metrics or alerts.",
                "Cloud Monitoring": "Correct. Cloud Monitoring collects metrics from GCP and on-premises/hybrid environments, enabling unified dashboards, alerting, and performance analysis.",
                "Cloud Trace": "Incorrect. For distributed tracing (latency analysis), not overall performance monitoring.",
                "Cloud Audit Logs": "Incorrect. For auditing changes to GCP resources, not application performance metrics/logs."
            }
        },
        {
            "title": "Q125: Choosing a Database for Real-time Inventory Management (High Throughput)",
            "description": "You are developing a real-time inventory management system. This system will experience extremely high read and write throughput for individual product lookups and stock updates. Data schema is relatively simple (product ID, quantity, location). Which database should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Firestore"
            ],
            "correctAnswer": "Google Cloud Bigtable",
            "feedback": {
                "Google Cloud SQL": "Incorrect. Relational, struggles with extreme high throughput for this use case.",
                "Google Cloud Spanner": "Incorrect. Designed for strong ACID transactions globally, overkill and not cost-optimized for pure high read/write throughput.",
                "Google Cloud Bigtable": "Correct! Optimized for very high read/write throughput and low latency on large datasets with simple key-value or wide-column access patterns, making it ideal for real-time inventory.",
                "Google Cloud Firestore": "Incorrect. Document database, typically for smaller transactions/app data, not extreme inventory throughput."
            }
        },
        {
            "title": "Q126: Data Ingestion for Large Data Lakes (Various Sources)",
            "description": "Your company needs to build a data lake in Google Cloud, accepting large volumes of data from various sources (databases, applications, files) in different formats. You need a scalable and flexible service to ingest this data into Cloud Storage. Which service is best suited for this ingestion?",
            "options": [
                "Cloud Pub/Sub",
                "Storage Transfer Service",
                "Cloud SQL",
                "Cloud Dataflow"
            ],
            "correctAnswer": "Storage Transfer Service",
            "feedback": {
                "Cloud Pub/Sub": "Incorrect. Primarily for streaming data, not for bulk transfer of existing data sources/files.",
                "Storage Transfer Service": "Correct! Fully managed service for large-scale data transfers from diverse sources (on-premises, S3, HTTP/HTTPS) into Cloud Storage, ideal for building data lakes.",
                "Cloud SQL": "Incorrect. Relational database, not for data lake ingestion.",
                "Cloud Dataflow": "Incorrect. For data processing pipelines, not primary data ingestion into a raw data lake."
            }
        },
        {
            "title": "Q127: Orchestrating Data Transformation and ML Workflows",
            "description": "Your data science team has a complex workflow that involves extracting data from various sources, cleaning and transforming it, running machine learning models, and then loading results into BigQuery. They need to schedule and manage these interdependent steps efficiently. Which service should they use?",
            "options": [
                "Google Cloud Dataflow",
                "Google Cloud Pub/Sub",
                "Google Cloud Composer",
                "Google Cloud Functions"
            ],
            "correctAnswer": "Google Cloud Composer",
            "feedback": {
                "Google Cloud Dataflow": "Incorrect. For data processing pipelines, not general workflow orchestration.",
                "Google Cloud Pub/Sub": "Incorrect. Messaging service, not for orchestrating workflows.",
                "Google Cloud Composer": "Correct. Cloud Composer (Apache Airflow) is a fully managed workflow orchestration service that excels at defining, scheduling, and monitoring complex multi-step data pipelines and ML workflows with dependencies.",
                "Google Cloud Functions": "Incorrect. For individual event-driven functions, not complex workflow orchestration."
            }
        },
        {
            "title": "Q128: Cost-Effective Storage for Infrequent Access (Compliance Records)",
            "description": "Your company needs to store financial transaction logs for 5 years for regulatory compliance. These logs are accessed approximately once a month for audits. Retrieval time needs to be in seconds. You want the most cost-effective solution. Which Cloud Storage class should you choose?",
            "options": [
                "Standard",
                "Nearline",
                "Coldline",
                "Archive"
            ],
            "correctAnswer": "Nearline",
            "feedback": {
                "Standard": "Incorrect. Too expensive for data accessed monthly.",
                "Nearline": "Correct. Designed for data accessed at most once a month, offering a balance of low storage cost and fast retrieval (seconds), ideal for monthly accessed compliance records.",
                "Coldline": "Incorrect. More cost-effective for even less frequent access (quarterly), with slightly higher retrieval cost.",
                "Archive": "Incorrect. For extremely rare access (<1x/year), with highest retrieval cost/latency."
            }
        },
        {
            "title": "Q129: Securely Accessing VMs from On-Premises via Private IP",
            "description": "Your developers need to securely connect to Compute Engine VMs using their private IP addresses from your on-premises data center. Data exchanged needs to be encrypted, and you need a high-bandwidth connection. Which networking solution should you choose?",
            "options": [
                "Cloud VPN",
                "Direct Peering",
                "Dedicated Interconnect",
                "Shared VPC"
            ],
            "correctAnswer": "Dedicated Interconnect",
            "feedback": {
                "Cloud VPN": "Incorrect. Uses public internet (even if encrypted), lower bandwidth compared to Interconnect.",
                "Direct Peering": "Incorrect. For public Google services, not private VPC traffic.",
                "Dedicated Interconnect": "Correct! Provides a direct, private physical connection with high bandwidth and encryption to your VPC, allowing secure access to VMs via private IPs from on-premises.",
                "Shared VPC": "Incorrect. For sharing VPCs across projects, not for on-premises connectivity."
            }
        },
        {
            "title": "Q130: Containerization Strategy for Existing Applications (Minimal Refactor)",
            "description": "Your company has several existing applications (monoliths and microservices) running on VMs. You want to move them to a containerized platform on Google Cloud to improve portability and resource utilization with minimal refactoring. Which service should you choose?",
            "options": [
                "Google App Engine",
                "Google Kubernetes Engine (GKE)",
                "Google Cloud Functions",
                "Google Compute Engine"
            ],
            "correctAnswer": "Google Kubernetes Engine (GKE)",
            "feedback": {
                "Google App Engine": "Incorrect. Often requires significant refactoring for existing applications (especially Standard environment).",
                "Google Kubernetes Engine (GKE)": "Correct! GKE (Kubernetes) is ideal for containerizing existing applications with minimal refactoring ('lift-and-shift' into containers), offering orchestration, scaling, and portability benefits.",
                "Google Cloud Functions": "Incorrect. For micro-functions, requires significant refactoring for existing apps.",
                "Google Compute Engine": "Incorrect. Just VMs; doesn't provide the benefits of container orchestration itself."
            }
        },
        {
            "title": "Q131: Real-time Anomaly Detection on Streaming Data",
            "description": "Your financial application generates continuous transaction data streams. You need to implement a real-time anomaly detection system to flag suspicious transactions immediately. Which combination of services should you use for this?",
            "options": [
                "Cloud Storage and BigQuery ML.",
                "Pub/Sub and Dataflow.",
                "Cloud SQL and Dataproc.",
                "Cloud Bigtable and Cloud Functions."
            ],
            "correctAnswer": "Pub/Sub and Dataflow.",
            "feedback": {
                "Cloud Storage and BigQuery ML.": "Incorrect. Primarily for batch processing and ML training on historical data, not real-time anomaly detection on streams.",
                "Pub/Sub and Dataflow.": "Correct. Pub/Sub ingests real-time transaction streams. Dataflow processes these streams in real-time (e.g., using a Beam pipeline with ML models) to detect and flag anomalies immediately.",
                "Cloud SQL and Dataproc.": "Incorrect. Cloud SQL is transactional; Dataproc is mostly for batch processing.",
                "Cloud Bigtable and Cloud Functions.": "Incorrect. Bigtable for operational data; Cloud Functions for event-driven logic, not a full stream processing pipeline for continuous anomaly detection."
            }
        },
        {
            "title": "Q132: Centralized User Directory Sync for GCP and On-Prem",
            "description": "Your organization uses an on-premises LDAP directory for user identities. You want to manage user access to Google Cloud centrally through these existing identities. You need to synchronize user information from LDAP to Google Cloud. Which component enables this?",
            "options": [
                "Cloud IAM",
                "Cloud Identity",
                "Directory Sync",
                "Identity Platform"
            ],
            "correctAnswer": "Directory Sync",
            "feedback": {
                "Cloud IAM": "Incorrect. For managing access to GCP resources using identities, not for syncing identities themselves.",
                "Cloud Identity": "Incorrect. Google's identity management platform, but needs Directory Sync to connect to on-premises LDAP.",
                "Directory Sync": "Correct. Directory Sync (e.g., Google Cloud Directory Sync - GCDS) is the tool used to synchronize user and group data from an on-premises LDAP or Active Directory to Cloud Identity, enabling centralized user management.",
                "Identity Platform": "Incorrect. For customer-facing identity management, not enterprise workforce identity."
            }
        },
        {
            "title": "Q133: Monitoring and Alerting on Cloud SQL Performance",
            "description": "You are managing a critical Cloud SQL instance. You need to monitor its performance metrics (e.g., CPU utilization, disk I/O, active connections) and receive alerts if any metric exceeds a predefined threshold. Which Google Cloud service should you use?",
            "options": [
                "Cloud Logging",
                "Cloud Monitoring",
                "Cloud Trace",
                "Cloud Audit Logs"
            ],
            "correctAnswer": "Cloud Monitoring",
            "feedback": {
                "Cloud Logging": "Incorrect. For collecting logs (events, errors), not performance metrics.",
                "Cloud Monitoring": "Correct. Cloud Monitoring is the primary service for collecting, visualizing, and alerting on performance metrics (CPU, disk I/O, network) for GCP services like Cloud SQL.",
                "Cloud Trace": "Incorrect. For distributed tracing (latency analysis of application requests), not database metrics.",
                "Cloud Audit Logs": "Incorrect. For auditing GCP resource access, not performance metrics."
            }
        },
        {
            "title": "Q134: Securing External Access to GCP Resources by IP (Granular)",
            "description": "Your company has highly sensitive data in Cloud Storage buckets and BigQuery datasets. You need to restrict access to these resources so that they can only be accessed from specific, trusted IP address ranges (e.g., your corporate offices). Which security control should you implement?",
            "options": [
                "VPC Firewall Rules",
                "IAM Conditions",
                "Shared VPC",
                "VPC Service Controls"
            ],
            "correctAnswer": "VPC Service Controls",
            "feedback": {
                "VPC Firewall Rules": "Incorrect. Control traffic to/from VMs, not direct access to managed services like GCS/BigQuery APIs.",
                "IAM Conditions": "Incorrect. While they can filter by IP, VPC Service Controls offer a stronger, network-perimeter-based security layer specifically for managed services.",
                "Shared VPC": "Incorrect. For sharing a VPC network across projects, not for restricting access by IP.",
                "VPC Service Controls": "Correct. VPC Service Controls create security perimeters around sensitive services (like GCS and BigQuery) to restrict access to only authorized networks/IPs, preventing data exfiltration and providing granular control over access from specific IP ranges."
            }
        },
        {
            "title": "Q135: Data Migration for Large Object Storage (S3 to GCS)",
            "description": "Your company needs to migrate petabytes of unstructured data (files) from an Amazon S3 bucket to a Google Cloud Storage bucket. You need a fully managed service that handles the transfer reliably and efficiently. Which service should you use?",
            "options": [
                "gsutil rsync",
                "Transfer Appliance",
                "Storage Transfer Service",
                "Cloud Dataflow"
            ],
            "correctAnswer": "Storage Transfer Service",
            "feedback": {
                "gsutil rsync": "Incorrect. Command-line tool, requires running on a VM, less managed/reliable for petabyte-scale.",
                "Transfer Appliance": "Incorrect. Physical appliance for on-premises to cloud transfers, not cloud-to-cloud.",
                "Storage Transfer Service": "Correct! A fully managed service designed for large-scale online data transfers between various cloud storage providers (including Amazon S3) and Google Cloud Storage. It handles scheduling, reliability, and data integrity.",
                "Cloud Dataflow": "Incorrect. For data processing pipelines, not primarily for direct cloud-to-cloud storage migration."
            }
        },
        {
            "title": "Q136: Automating Image Updates for Managed Instance Groups",
            "description": "You manage a critical application running on Compute Engine instances that are part of a Managed Instance Group (MIG). You frequently update the base VM image (e.g., with new OS patches, application versions). You need to apply these new images to the running MIG instances with minimal disruption. Which feature should you use?",
            "options": [
                "Manual SSH to each instance and run updates.",
                "Update the instance template and apply a rolling update.",
                "Create new MIGs for each update.",
                "Use a startup script to pull new images on boot."
            ],
            "correctAnswer": "Update the instance template and apply a rolling update.",
            "feedback": {
                "Manual SSH to each instance and run updates.": "Incorrect. Not automated, causes disruption, error-prone.",
                "Update the instance template and apply a rolling update.": "Correct. This is the standard, automated, and minimal-downtime way to update VMs in a MIG. Update the template to point to the new image, then initiate a rolling update.",
                "Create new MIGs for each update.": "Incorrect. Inefficient, complicates management.",
                "Use a startup script to pull new images on boot.": "Incorrect. Startup scripts run on boot; doesn't ensure a 'rolling update' for running instances or manage the image version effectively."
            }
        },
        {
            "title": "Q137: Real-time User Event Collection for Mobile Analytics",
            "description": "Your mobile application generates a continuous stream of user behavior data (e.g., app opens, screen views, button clicks). You need to capture this data in real time and deliver it to a downstream analytics system without any data loss. Which service is best suited for reliable ingestion?",
            "options": [
                "Cloud Storage",
                "Cloud Pub/Sub",
                "Cloud SQL",
                "Cloud Bigtable"
            ],
            "correctAnswer": "Cloud Pub/Sub",
            "feedback": {
                "Cloud Storage": "Incorrect. For static file storage, not real-time stream ingestion.",
                "Cloud Pub/Sub": "Correct! Fully managed, highly scalable, and durable messaging service specifically designed for ingesting large volumes of real-time streaming data from mobile apps or IoT devices without data loss.",
                "Cloud SQL": "Incorrect. Transactional relational DB, not for high-volume stream ingestion.",
                "Cloud Bigtable": "Incorrect. For storing/querying high-throughput operational data, not the initial ingestion layer."
            }
        },
        {
            "title": "Q138: Multi-Region DNS Resolution for Global Applications",
            "description": "Your company has a global web application deployed across multiple Google Cloud regions. You need to ensure that users are directed to the closest available regional deployment for optimal latency. Which networking service should you use for global DNS resolution?",
            "options": [
                "Regional Cloud DNS",
                "Global Cloud DNS with routing policies",
                "Cloud Load Balancing (Regional)",
                "Cloud CDN"
            ],
            "correctAnswer": "Global Cloud DNS with routing policies",
            "feedback": {
                "Regional Cloud DNS": "Incorrect. Only serves within a single region.",
                "Global Cloud DNS with routing policies": "Correct. Global Cloud DNS (with a global external load balancer if applicable) supports various routing policies (e.g., geolocation, latency-based) to direct users to the optimal regional deployment for improved latency.",
                "Cloud Load Balancing (Regional)": "Incorrect. Distributes traffic within a region; needs a Global Load Balancer to act as the single entry point for global users.",
                "Cloud CDN": "Incorrect. For caching static content, not for global application routing."
            }
        },
        {
            "title": "Q139: Securing Access to Cloud Resources for Developers (Fine-Grained)",
            "description": "Your development team needs granular access to specific Google Cloud resources (e.g., read only access to a particular Cloud Storage bucket, ability to start/stop specific VMs, but not delete). You need to ensure access is minimized and easily auditable. Which IAM feature should you use?",
            "options": [
                "Grant them Project Editor role.",
                "Assign predefined roles or create custom roles.",
                "Use Identity-Aware Proxy (IAP).",
                "Grant them Project Owner role."
            ],
            "correctAnswer": "Assign predefined roles or create custom roles.",
            "feedback": {
                "Grant them Project Editor role.": "Incorrect. Grants broad permissions across the project, violating least privilege.",
                "Assign predefined roles or create custom roles.": "Correct! Predefined roles (e.g., storage.objectViewer, compute.instanceAdmin) or custom roles with precise permissions (least privilege) allow granular and auditable access control.",
                "Use Identity-Aware Proxy (IAP).": "Incorrect. For secure application access, not for granular resource permissions within IAM.",
                "Grant them Project Owner role.": "Incorrect. Grants all permissions, highly insecure."
            }
        },
        {
            "title": "Q140: Multi-Project Resource Management and Billing Control",
            "description": "Your organization has multiple business units, each managing its own set of Google Cloud projects. You need to enforce separate billing accounts for each business unit and ensure that resources are consistently named (e.g., 'prod-appname' for production VMs). Which resource hierarchy components help achieve this?",
            "options": [
                "Project Labels and Service Accounts.",
                "Organization, Folders, and Projects.",
                "Billing Accounts and Network Tags.",
                "Custom IAM roles and Shared VPC."
            ],
            "correctAnswer": "Organization, Folders, and Projects.",
            "feedback": {
                "Project Labels and Service Accounts.": "Incorrect. Labels are for tagging, not hierarchical organization. Service accounts are for identity.",
                "Organization, Folders, and Projects.": "Correct. The Organization node allows centralized management. Folders can group projects by business unit (allowing separate billing accounts). Projects contain resources. This hierarchy enables applying policies for consistent naming and billing separation.",
                "Billing Accounts and Network Tags.": "Incorrect. Billing accounts manage billing; network tags apply to VMs, not core hierarchical organization.",
                "Custom IAM roles and Shared VPC.": "Incorrect. IAM roles for permissions; Shared VPC for network sharing, not core hierarchical management and billing."
            }
        },
        {
            "title": "Q141: Data Ingestion for Event Stream Analytics (High Throughput)",
            "description": "Your application processes millions of user clicks per hour that need to be analyzed in real time to detect trends and anomalies. You need a highly scalable and reliable service to ingest this continuous stream of events. Which service should you use?",
            "options": [
                "Cloud Storage",
                "Cloud Pub/Sub",
                "Cloud SQL",
                "Cloud Bigtable"
            ],
            "correctAnswer": "Cloud Pub/Sub",
            "feedback": {
                "Cloud Storage": "Incorrect. For static file storage, not real-time stream ingestion.",
                "Cloud Pub/Sub": "Correct! Fully managed, highly scalable, and durable messaging service perfect for ingesting continuous streams of high-volume events like user clicks.",
                "Cloud SQL": "Incorrect. Transactional relational DB, not for high-throughput stream ingestion.",
                "Cloud Bigtable": "Incorrect. For storing/querying high-throughput operational data, not the initial ingestion layer."
            }
        },
        {
            "title": "Q142: Automating Development Environment Setup",
            "description": "Your development team needs to quickly and consistently provision new development environments on Compute Engine VMs, including a specific operating system, pre-installed software, and network configurations. You want to automate this process to minimize manual effort and errors. Which approach should you use?",
            "options": [
                "Manually create VMs via Cloud Console and configure them.",
                "Use gcloud CLI scripts to create and configure each VM.",
                "Create custom VM images and use instance templates.",
                "Use Cloud Functions to trigger VM creation."
            ],
            "correctAnswer": "Create custom VM images and use instance templates.",
            "feedback": {
                "Manually create VMs via Cloud Console and configure them.": "Incorrect. Not automated or consistent.",
                "Use gcloud CLI scripts to create and configure each VM.": "Incorrect. Imperative, less declarative and repeatable for complex setups than templates/images.",
                "Create custom VM images and use instance templates.": "Correct. Creating a custom (golden) image with the desired OS/software and using instance templates to deploy VMs ensures automated, consistent provisioning of development environments.",
                "Use Cloud Functions to trigger VM creation.": "Incorrect. Cloud Functions can trigger, but don't define the VM's configuration or consistency; they would still rely on templates/images."
            }
        },
        {
            "title": "Q143: Global Load Balancing for Web Applications (Latency-Based)",
            "description": "Your web application is accessed by users worldwide. You need to ensure that users are always directed to the geographically closest available instance of your application to minimize latency. Which load balancing solution provides this?",
            "options": [
                "Regional HTTP(S) Load Balancer",
                "Internal HTTP(S) Load Balancer",
                "External HTTP(S) Load Balancer (Global)",
                "Network Load Balancer (External)"
            ],
            "correctAnswer": "External HTTP(S) Load Balancer (Global)",
            "feedback": {
                "Regional HTTP(S) Load Balancer": "Incorrect. Only serves within a single region.",
                "Internal HTTP(S) Load Balancer": "Incorrect. For traffic within a VPC, not external users.",
                "External HTTP(S) Load Balancer (Global)": "Correct! A Global External HTTP(S) Load Balancer uses Google's global network and Anycast IP to route user requests to the geographically closest healthy backend, optimizing latency for global users.",
                "Network Load Balancer (External)": "Incorrect. Layer 4 load balancer, not optimized for HTTP/HTTPS or global latency routing like Layer 7."
            }
        },
        {
            "title": "Q144: Centralized Audit Logs for Compliance and Security",
            "description": "Your company has strict compliance requirements to monitor and audit all administrative activities and data access across their Google Cloud projects. They need a centralized, immutable record of these events for security investigations and audits. Which service enables this?",
            "options": [
                "Cloud Logging",
                "Cloud Monitoring",
                "Cloud Audit Logs",
                "Security Command Center"
            ],
            "correctAnswer": "Cloud Audit Logs",
            "feedback": {
                "Cloud Logging": "Incorrect. A broader logging service; Cloud Audit Logs is a specific type of log within it.",
                "Cloud Monitoring": "Incorrect. For metrics and operational health, not audit trails.",
                "Cloud Audit Logs": "Correct! Cloud Audit Logs automatically records administrative activities and data access events across all GCP services, providing a comprehensive and immutable audit trail.",
                "Security Command Center": "Incorrect. For security posture management, uses audit logs but doesn't generate them."
            }
        },
        {
            "title": "Q145: Choosing a Database for Real-time Ads Platform (High Read/Write)",
            "description": "You are developing a real-time advertising platform that needs to store and retrieve user profiles, ad inventory, and campaign data. This database will experience extremely high read and write throughput, with low latency. Data consistency is important, but not global ACID transactional. Which database should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Firestore"
            ],
            "correctAnswer": "Google Cloud Bigtable",
            "feedback": {
                "Google Cloud SQL": "Incorrect. A relational database would not scale to the extremely high read/write throughput required by a large-scale advertising platform.",
                "Google Cloud Spanner": "Incorrect. Spanner's strong global consistency is likely overkill and more expensive than necessary for this use case, which often prioritizes low latency and throughput over strict ACID transactions.",
                "Google Cloud Bigtable": "Correct! Bigtable is designed for massive-scale, low-latency, high-throughput workloads, which is exactly what an ad tech platform requires for serving profiles and ad data in real-time.",
                "Google Cloud Firestore": "Incorrect. While scalable, Firestore is generally used for mobile and web app backends and is not optimized for the extreme throughput and operational analytics patterns of an ad platform."
            }
        },
        {
            "title": "Q146: Data Migration from On-Prem DB to Cloud (Minimal Downtime, Large Scale)",
            "description": "Your company needs to migrate a large (multi-terabyte) on-premises relational database (e.g., MySQL) to Google Cloud SQL with minimal downtime. The database is actively used by production applications. Which migration method provides the least disruption?",
            "options": [
                "Export data to CSV and import into Cloud SQL.",
                "Use Database Migration Service (DMS) with continuous replication.",
                "Perform a full logical backup and restore to Cloud SQL.",
                "Manually create a new Cloud SQL instance and write scripts to sync data."
            ],
            "correctAnswer": "Use Database Migration Service (DMS) with continuous replication.",
            "feedback": {
                "Export data to CSV and import into Cloud SQL.": "Incorrect. This method would require significant application downtime while the export and import processes are running.",
                "Use Database Migration Service (DMS) with continuous replication.": "Correct! DMS is a serverless service designed for minimal downtime migrations. It performs an initial load and then uses continuous replication (Change Data Capture) to keep the target database in sync with the source, allowing you to cut over with only a few minutes of downtime.",
                "Perform a full logical backup and restore to Cloud SQL.": "Incorrect. Similar to a CSV export, this would involve taking the database offline for a long period to perform the backup and restore, resulting in significant downtime.",
                "Manually create a new Cloud SQL instance and write scripts to sync data.": "Incorrect. This is a manual, error-prone, and slow process that would not meet the minimal downtime requirement."
            }
        },
        {
            "title": "Q147: Orchestrating Daily ETL Workflows",
            "description": "Your data engineering team has a daily ETL (Extract, Transform, Load) process that involves pulling data from various sources, processing it with Dataflow, and loading the results into BigQuery. They need to schedule, monitor, and manage the dependencies of these batch jobs. Which service should they use?",
            "options": [
                "Google Cloud Dataflow",
                "Google Cloud Pub/Sub",
                "Google Cloud Composer",
                "Google Cloud Functions"
            ],
            "correctAnswer": "Google Cloud Composer",
            "feedback": {
                "Google Cloud Dataflow": "Incorrect. Dataflow is a service to execute one of the steps (the transformation), but it doesn't orchestrate the entire end-to-end workflow.",
                "Google Cloud Pub/Sub": "Incorrect. This is a messaging service, not a workflow orchestrator.",
                "Google Cloud Composer": "Correct! Cloud Composer, a managed Apache Airflow service, is specifically designed for orchestrating complex workflows. You can define tasks, set dependencies between them (e.g., run the Dataflow job only after the extraction is complete), and schedule the entire pipeline to run automatically.",
                "Google Cloud Functions": "Incorrect. Chaining functions together for a complex workflow is brittle and hard to manage. Composer is the purpose-built tool for this."
            }
        },
        {
            "title": "Q148: Cost-Effective Storage for Legal Hold Data (Very Rare Access)",
            "description": "Your legal department requires certain documents to be placed on a legal hold for potentially decades. Access to these documents is extremely rare (e.g., only if subpoenaed). Cost for storage must be minimized. Which Cloud Storage class should you choose?",
            "options": [
                "Standard",
                "Nearline",
                "Coldline",
                "Archive"
            ],
            "correctAnswer": "Archive",
            "feedback": {
                "Standard": "Incorrect. This would be prohibitively expensive for data that is almost never accessed.",
                "Nearline": "Incorrect. This is for data accessed monthly. Archive is far more cost-effective for this use case.",
                "Coldline": "Incorrect. This is for data accessed quarterly. Archive offers even lower storage costs.",
                "Archive": "Correct! Archive storage has the absolute lowest cost for data at rest, making it the perfect choice for long-term retention of data that is very rarely accessed."
            }
        },
        {
            "title": "Q149: Secure Connectivity to Private GKE Cluster from On-Prem",
            "description": "Your on-premises development machines need to connect securely to a private Google Kubernetes Engine (GKE) cluster in your VPC to manage deployments and access services. You need encrypted, high-bandwidth connectivity. Which networking solution should you implement?",
            "options": [
                "Cloud VPN",
                "Shared VPC",
                "Dedicated Interconnect",
                "Cloud NAT"
            ],
            "correctAnswer": "Dedicated Interconnect",
            "feedback": {
                "Cloud VPN": "Incorrect. While secure, standard Cloud VPN runs over the public internet and does not guarantee high bandwidth.",
                "Shared VPC": "Incorrect. This is for sharing a VPC network across projects and is irrelevant to on-premises connectivity.",
                "Dedicated Interconnect": "Correct! Dedicated Interconnect provides a direct, private, high-bandwidth physical connection between your on-premises network and your GCP VPC. This allows for secure and performant access to private GKE clusters.",
                "Cloud NAT": "Incorrect. Cloud NAT provides outbound internet access for private instances; it does not allow inbound connections from on-premises."
            }
        },
        {
            "title": "Q150: Containerization Strategy for New Microservices (Optimal Dev Experience)",
            "description": "Your team is developing a new application using a microservices architecture. They want a platform that provides a great developer experience, abstracting away infrastructure concerns, while still supporting containerization and automatic scaling. Which service should you choose?",
            "options": [
                "Google Compute Engine",
                "Google Kubernetes Engine (GKE)",
                "Cloud Run",
                "Google Cloud Functions"
            ],
            "correctAnswer": "Cloud Run",
            "feedback": {
                "Google Compute Engine": "Incorrect. This is IaaS and provides the least abstraction, requiring significant infrastructure management.",
                "Google Kubernetes Engine (GKE)": "Incorrect. While powerful, GKE exposes the full Kubernetes API and requires more operational knowledge than a serverless platform like Cloud Run.",
                "Cloud Run": "Correct! Cloud Run is a fully managed, serverless platform that runs stateless containers. It abstracts away all cluster management, scales to zero, and provides a very simple developer experience: just provide a container image and it runs.",
                "Google Cloud Functions": "Incorrect. This is for functions (FaaS), not for running general-purpose containerized microservices."
            }
        },
        {
            "title": "Q151: Real-time Data Ingestion from Webhooks",
            "description": "Your application receives a high volume of real-time event data via webhooks (HTTP POST requests) from third-party services. You need to ingest this data reliably and push it to a streaming analytics pipeline. Which Google Cloud service is best suited to receive and buffer these webhooks?",
            "options": [
                "Cloud Load Balancing",
                "Cloud Pub/Sub",
                "Cloud Functions",
                "Cloud Endpoints"
            ],
            "correctAnswer": "Cloud Functions",
            "feedback": {
                "Cloud Load Balancing": "Incorrect. A load balancer only directs traffic; it does not provide logic to process the webhook and push it to a pipeline.",
                "Cloud Pub/Sub": "Incorrect. Pub/Sub cannot directly receive HTTP POST requests as an endpoint. It needs a service in front of it to push messages to it.",
                "Cloud Functions": "Correct! A Cloud Function with an HTTP trigger provides a scalable, serverless endpoint that can receive the webhook POST request, perform any necessary validation or transformation, and then reliably publish the event to a Pub/Sub topic for downstream processing.",
                "Cloud Endpoints": "Incorrect. This is a service for managing and exposing your own APIs, not for ingesting incoming webhooks from third parties."
            }
        },
        {
            "title": "Q152: Centralized User Management for Workforce Identity",
            "description": "Your enterprise needs a unified identity management solution for its employees to access both Google Cloud resources and third-party SaaS applications. You want to provision and deprovision users, manage groups, and enforce security policies centrally. Which Google Cloud service should you use?",
            "options": [
                "Identity Platform",
                "Cloud IAM",
                "Cloud Identity",
                "Google Workspace"
            ],
            "correctAnswer": "Cloud Identity",
            "feedback": {
                "Identity Platform": "Incorrect. This is for managing customer identities for your applications (CIAM), not your workforce.",
                "Cloud IAM": "Incorrect. IAM controls *what* identities can do, but Cloud Identity is the service that *manages* the identities themselves.",
                "Cloud Identity": "Correct! Cloud Identity is Google's Identity-as-a-Service (IDaaS) offering. It provides a central place to manage your organization's users and groups, and it serves as the identity provider for accessing GCP and for setting up SSO with other SaaS apps.",
                "Google Workspace": "Incorrect. While Google Workspace includes Cloud Identity, Cloud Identity is also available as a standalone service specifically for this purpose."
            }
        },
        {
            "title": "Q153: Monitoring Application Latency in Distributed Systems",
            "description": "Your application consists of multiple microservices communicating across different Compute Engine instances. Users report slow response times. You need to pinpoint which service calls or operations are causing the latency. Which Google Cloud service should you use?",
            "options": [
                "Cloud Logging",
                "Cloud Monitoring",
                "Cloud Trace",
                "Cloud Audit Logs"
            ],
            "correctAnswer": "Cloud Trace",
            "feedback": {
                "Cloud Logging": "Incorrect. Logs can show errors but are not effective for visualizing latency across a distributed system.",
                "Cloud Monitoring": "Incorrect. Monitoring shows high-level metrics (e.g., 'the whole request took 2 seconds') but doesn't show the breakdown of that time across services.",
                "Cloud Trace": "Correct! Cloud Trace is specifically designed for distributed tracing. It samples requests, tracks them as they flow through multiple services, and generates detailed latency graphs that pinpoint exactly which service call or operation is the bottleneck.",
                "Cloud Audit Logs": "Incorrect. This is for auditing administrative actions, not for application performance monitoring."
            }
        },
        {
            "title": "Q154: Choosing a Database for Financial Transaction Ledger (Immutable)",
            "description": "Your company needs to implement an immutable financial transaction ledger where entries are strictly append-only and cannot be altered. The ledger needs to handle high write throughput and support eventual strong consistency. Which database would be most suitable?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Firestore"
            ],
            "correctAnswer": "Google Cloud Bigtable",
            "feedback": {
                "Google Cloud SQL": "Incorrect. A relational database allows for updates and deletes, so it is not inherently immutable. It would also struggle with extremely high write throughput.",
                "Google Cloud Spanner": "Incorrect. Spanner also allows updates and deletes. Its strong global consistency is likely overkill for an append-only ledger.",
                "Google Cloud Bigtable": "Correct! Bigtable's data model is well-suited for append-only patterns. By designing a row key that includes a timestamp, you can create a naturally ordered, immutable ledger that can handle massive write volumes.",
                "Google Firestore": "Incorrect. Firestore is a document database that allows for mutable documents and is not designed for the extreme write throughput of a large-scale ledger."
            }
        },
        {
            "title": "Q155: Automating Infrastructure Updates with Canary Deployments",
            "description": "You manage a critical production application on Google Kubernetes Engine (GKE). You need to deploy new versions of your application with minimal risk, gradually rolling out changes to a small subset of users before a full rollout. Which deployment strategy should you use?",
            "options": [
                "Rolling Update",
                "Blue/Green Deployment",
                "Canary Deployment",
                "In-place Update"
            ],
            "correctAnswer": "Canary Deployment",
            "feedback": {
                "Rolling Update": "Incorrect. A rolling update gradually replaces old pods with new ones, but all traffic is immediately sent to the new version as it comes online. It doesn't isolate a small subset of user traffic.",
                "Blue/Green Deployment": "Incorrect. This involves deploying a full new version alongside the old and then cutting over all traffic at once. It does not provide a gradual rollout to a subset of users.",
                "Canary Deployment": "Correct! A canary deployment is specifically designed to reduce risk by first deploying the new version to a small number of pods and directing a small percentage of live traffic to them. You can monitor this 'canary' release before proceeding with a full rollout.",
                "In-place Update": "Incorrect. This is not a standard Kubernetes deployment strategy and would be very risky."
            }
        },
        {
            "title": "Q156: Data Ingestion for Batch Analytics (FTP Source)",
            "description": "Your company receives daily large data files via SFTP from a partner. These files need to be ingested into Google Cloud Storage for subsequent batch processing. You need a reliable and automated way to pull these files into GCS. Which service should you use?",
            "options": [
                "gsutil rsync",
                "Storage Transfer Service",
                "Cloud Pub/Sub",
                "Cloud Functions"
            ],
            "correctAnswer": "Storage Transfer Service",
            "feedback": {
                "gsutil rsync": "Incorrect. This would require a running VM and custom scripting to schedule and manage the transfers, which is not a fully managed solution.",
                "Storage Transfer Service": "Correct! Storage Transfer Service is a fully managed service that can be configured to pull data from SFTP servers on a recurring schedule and transfer it reliably to Cloud Storage.",
                "Cloud Pub/Sub": "Incorrect. This is for real-time messaging, not for transferring files from an SFTP server.",
                "Cloud Functions": "Incorrect. While a function could be triggered to perform a transfer, it's not designed for large, long-running file transfers and would require significant custom code."
            }
        },
        {
            "title": "Q157: Centralized Billing and Budget Management",
            "description": "Your organization has multiple Google Cloud projects used by different departments. You need to gain centralized visibility into spending, set budgets per department, and receive alerts when spending thresholds are approached or exceeded. Which feature enables this?",
            "options": [
                "Project Labels",
                "Billing Accounts and Budgets",
                "Organization Policy Service",
                "Cloud Asset Inventory"
            ],
            "correctAnswer": "Billing Accounts and Budgets",
            "feedback": {
                "Project Labels": "Incorrect. Labels are useful for filtering costs in a report, but they don't allow you to set budgets or trigger alerts.",
                "Billing Accounts and Budgets": "Correct! You can link multiple projects (e.g., all projects for one department) to a single billing account or subaccount. Then, within the Cloud Billing console, you can create Budgets that scope to those projects and configure alert thresholds to notify you when costs are approaching the limit.",
                "Organization Policy Service": "Incorrect. This is for enforcing constraints on resource configurations, not for managing costs.",
                "Cloud Asset Inventory": "Incorrect. This service lets you view your resources, but it doesn't provide cost management or budgeting features."
            }
        },
        {
            "title": "Q158: Hybrid Cloud Network Security (Perimeter Control)",
            "description": "Your company is migrating sensitive applications to Google Cloud. You need to create a strong security perimeter around your cloud services (e.g., BigQuery, Cloud Storage) to prevent data exfiltration and control access from specific networks, including on-premises. Which service is designed for this?",
            "options": [
                "VPC Firewall Rules",
                "Shared VPC",
                "Identity-Aware Proxy (IAP)",
                "VPC Service Controls"
            ],
            "correctAnswer": "VPC Service Controls",
            "feedback": {
                "VPC Firewall Rules": "Incorrect. These control traffic to and from VMs, not access to the APIs of managed services like BigQuery and Cloud Storage.",
                "Shared VPC": "Incorrect. This is for sharing a network across projects and is not a security perimeter tool.",
                "Identity-Aware Proxy (IAP)": "Incorrect. IAP secures access to specific applications, but VPC Service Controls creates a broader network-based perimeter around a set of services.",
                "VPC Service Controls": "Correct! This is the exact purpose of VPC Service Controls. It creates a virtual perimeter that blocks data exfiltration by restricting access to managed service APIs to only authorized VPC networks and on-premises connections via a hybrid link."
            }
        },
        {
            "title": "Q159: Data Processing for Real-time Clickstream Analysis",
            "description": "Your e-commerce website generates a continuous stream of clickstream data. You need to analyze this data in real time to personalize user experiences and detect suspicious activity. Which services should you combine for this real-time processing and analysis?",
            "options": [
                "Cloud Storage and BigQuery",
                "Pub/Sub and Dataflow",
                "Cloud SQL and Dataproc",
                "Cloud Bigtable and Cloud Functions"
            ],
            "correctAnswer": "Pub/Sub and Dataflow",
            "feedback": {
                "Cloud Storage and BigQuery": "Incorrect. This is a batch processing pattern, not suitable for real-time analysis.",
                "Pub/Sub and Dataflow": "Correct! This is the standard architecture for this use case. Pub/Sub ingests the high-volume clickstream in real-time, and Dataflow provides a powerful, scalable engine for performing complex analysis (like sessionization or pattern detection) on the stream.",
                "Cloud SQL and Dataproc": "Incorrect. This combination is not designed for real-time stream processing.",
                "Cloud Bigtable and Cloud Functions": "Incorrect. While functions can process events, Dataflow is the service designed for building complex, stateful streaming pipelines required for this kind of analysis."
            }
        },
        {
            "title": "Q160: Disaster Recovery for Cloud SQL (Low RPO/RTO)",
            "description": "Your application's Cloud SQL for PostgreSQL database is highly critical and requires the lowest possible Recovery Point Objective (RPO) and Recovery Time Objective (RTO) in case of a regional disaster. What should you configure?",
            "options": [
                "Automated daily backups with point-in-time recovery.",
                "Cross-region read replica with manual failover.",
                "High Availability (HA) with cross-region replication.",
                "Export database regularly to Cloud Storage."
            ],
            "correctAnswer": "High Availability (HA) with cross-region replication.",
            "feedback": {
                "Automated daily backups with point-in-time recovery.": "Incorrect. This would result in an RPO of up to 24 hours and a high RTO due to the time required to restore.",
                "Cross-region read replica with manual failover.": "Incorrect. A standard read replica has asynchronous replication, which means there will be some data loss (non-zero RPO). Failover is also a manual process.",
                "High Availability (HA) with cross-region replication.": "Correct! An HA configuration provides a near-zero RPO for zonal failures. Combining this with a cross-region replica gives you a full disaster recovery solution that can be failed over to quickly, minimizing both RPO and RTO.",
                "Export database regularly to Cloud Storage.": "Incorrect. This is a manual backup method with the highest possible RPO and RTO."
            }
        },
        {
            "title": "Q161: Multi-Region Data Analytics with Data Locality",
            "description": "Your company has a global customer base and stores analytical data in BigQuery across multiple regions. You need to ensure that data processed for analysis respects data locality (i.e., data from Europe is processed in Europe, data from Asia is processed in Asia) for compliance and performance. How should you design your BigQuery setup?",
            "options": [
                "Use a single multi-regional dataset.",
                "Use regional datasets and query them from respective regions.",
                "Use a single global dataset.",
                "Use BigQuery Omni for cross-cloud analysis."
            ],
            "correctAnswer": "Use regional datasets and query them from respective regions.",
            "feedback": {
                "Use a single multi-regional dataset.": "Incorrect. A multi-region dataset (e.g., 'EU') stores data within that large geographic area but does not guarantee it will be processed in a specific country's data center, which might be required for compliance.",
                "Use regional datasets and query them from respective regions.": "Correct! By creating separate regional datasets (e.g., 'europe-west1', 'asia-northeast1'), you ensure that the data physically resides and is processed within that specific region, satisfying data locality requirements.",
                "Use a single global dataset.": "Incorrect. BigQuery does not have a 'global' dataset location type.",
                "Use BigQuery Omni for cross-cloud analysis.": "Incorrect. This is for analyzing data located in AWS or Azure, not for controlling data locality within Google Cloud."
            }
        },
        {
            "title": "Q162: Automating Image Building and Vulnerability Scanning in CI/CD",
            "description": "Your CI/CD pipeline builds Docker images for your GKE application. You need to ensure that newly built images are automatically scanned for vulnerabilities before they are pushed to a container registry. Which services should you integrate into your pipeline?",
            "options": [
                "Cloud Build and Artifact Registry.",
                "Cloud Build and Container Analysis.",
                "Cloud Functions and Cloud Storage.",
                "Cloud Deploy and Binary Authorization."
            ],
            "correctAnswer": "Cloud Build and Container Analysis.",
            "feedback": {
                "Cloud Build and Artifact Registry.": "Incorrect. Artifact Registry is where the images are stored, but it is the Container Analysis API that performs the actual scanning.",
                "Cloud Build and Container Analysis.": "Correct! Cloud Build automates the build process. When an image is pushed to Artifact Registry, it can automatically trigger Container Analysis to scan the image for known vulnerabilities, providing a security gate in your pipeline.",
                "Cloud Functions and Cloud Storage.": "Incorrect. These services are not designed for building or scanning container images.",
                "Cloud Deploy and Binary Authorization.": "Incorrect. Binary Authorization is a later step that *enforces* policies based on the results of a scan, but it does not perform the scan itself."
            }
        },
        {
            "title": "Q163: Global Application Frontend with Edge Caching",
            "description": "You are deploying a global web application. You want to serve static content (images, CSS, JS) with very low latency to users worldwide and reduce the load on your backend servers. Which service should you use for content delivery?",
            "options": [
                "External HTTP(S) Load Balancer",
                "Cloud CDN",
                "Cloud Storage",
                "Cloud Interconnect"
            ],
            "correctAnswer": "Cloud CDN",
            "feedback": {
                "External HTTP(S) Load Balancer": "Incorrect. The load balancer is necessary to route traffic, but it does not cache the content itself.",
                "Cloud CDN": "Correct! Cloud CDN (Content Delivery Network) is specifically designed for this. It integrates with the external load balancer to cache static content at Google's edge locations around the world, serving it directly to users from a nearby location for minimal latency.",
                "Cloud Storage": "Incorrect. While Cloud Storage is a great place to store the static content, you need Cloud CDN to distribute and cache it globally for low latency.",
                "Cloud Interconnect": "Incorrect. This is for private network connectivity, not for public content delivery."
            }
        },
        {
            "title": "Q164: Securing Access to Cloud Resources for External Partners",
            "description": "You need to grant a third-party partner secure access to a specific Cloud Storage bucket and a BigQuery dataset in your project. The partner uses their own Google accounts, and you want to ensure granular, least-privilege access. Which IAM feature should you use?",
            "options": [
                "Grant them Project Owner role.",
                "Add them to your Google Workspace directory.",
                "Grant IAM roles to their Google accounts in your project.",
                "Create service accounts for them."
            ],
            "correctAnswer": "Grant IAM roles to their Google accounts in your project.",
            "feedback": {
                "Grant them Project Owner role.": "Incorrect. This is extremely insecure and grants far too much permission.",
                "Add them to your Google Workspace directory.": "Incorrect. You should not add external users to your internal company directory. IAM allows you to grant access directly to their existing Google accounts.",
                "Grant IAM roles to their Google accounts in your project.": "Correct! You can add the partner's Google account email as a principal in your project's IAM policy and assign them specific, granular roles (like `roles/storage.objectViewer` or `roles/bigquery.dataViewer`) on the specific resources they need to access.",
                "Create service accounts for them.": "Incorrect. Service accounts are for applications, not for individual human users. Granting them a service account key would be a security risk."
            }
        },
        {
            "title": "Q165: Choosing a Database for Real-time Personalization Engine",
            "description": "You are building a real-time personalization engine for an e-commerce website. This engine needs to store dynamic user preferences and product recommendations, requiring very low-latency reads and writes, a flexible schema, and horizontal scalability for millions of users. Which database should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Firestore"
            ],
            "correctAnswer": "Google Cloud Firestore",
            "feedback": {
                "Google Cloud SQL": "Incorrect. A relational database's fixed schema would be too rigid for dynamic user preferences.",
                "Google Cloud Spanner": "Incorrect. Spanner is a relational database with a fixed schema, and its focus on global transactional consistency is likely unnecessary for this use case.",
                "Google Cloud Bigtable": "Incorrect. Bigtable is optimized for large analytical workloads, not for the document-like structure of user profiles and recommendations.",
                "Google Cloud Firestore": "Correct! Firestore is a NoSQL document database that is perfect for this. Its flexible schema can easily accommodate dynamic user preferences, it provides low-latency reads/writes, and it scales automatically to handle millions of users, making it ideal for personalization."
            }
        },
        {
            "title": "Q166: Data Ingestion for Archival of Application Logs",
            "description": "Your applications generate large volumes of logs that need to be collected, potentially filtered, and then archived to a cost-effective storage for long-term retention. Which service should you use to collect and route these logs?",
            "options": [
                "Cloud Storage",
                "Cloud Logging",
                "Cloud Pub/Sub",
                "Cloud Dataflow"
            ],
            "correctAnswer": "Cloud Logging",
            "feedback": {
                "Cloud Storage": "Incorrect. Cloud Storage is the *destination* for the archived logs, not the service that collects and routes them.",
                "Cloud Logging": "Correct! Cloud Logging is the centralized service for log ingestion. You can use the Ops Agent to collect logs from your applications and then create a Log Sink in Cloud Logging to filter and route these logs to a Cloud Storage bucket for long-term, cost-effective archival.",
                "Cloud Pub/Sub": "Incorrect. While Pub/Sub can be part of a logging pipeline, Cloud Logging is the primary service for collection and routing.",
                "Cloud Dataflow": "Incorrect. Dataflow is for processing data, not for the initial collection and routing of logs."
            }
        },
        {
            "title": "Q167: Orchestrating Data Workflows with Dependencies (Cloud-Native)",
            "description": "Your data pipeline involves several interdependent stages: data ingestion, cleaning, transformation, and loading into a data warehouse. These stages run on different GCP services. You need a fully managed service to define, schedule, and monitor these dependencies. Which service should you use?",
            "options": [
                "Custom scripts on Compute Engine",
                "Google Cloud Dataflow",
                "Google Cloud Composer",
                "Cloud Functions"
            ],
            "correctAnswer": "Google Cloud Composer",
            "feedback": {
                "Custom scripts on Compute Engine": "Incorrect. This would require you to build and manage your own scheduling, dependency, and monitoring logic, which is complex and not a managed solution.",
                "Google Cloud Dataflow": "Incorrect. Dataflow is a service to execute one of the steps (the transformation), but it doesn't orchestrate the entire end-to-end workflow across different services.",
                "Google Cloud Composer": "Correct! Cloud Composer, a managed Apache Airflow service, is specifically designed for orchestrating complex workflows. You can define tasks for each stage, set dependencies between them (e.g., run the ML model only after the transformation is complete), and schedule the entire pipeline to run automatically.",
                "Cloud Functions": "Incorrect. Chaining functions together for a complex workflow is brittle and hard to manage. Composer is the purpose-built tool for this."
            }
        },
        {
            "title": "Q168: Cost Optimization for Development VMs (Scheduled Off-Hours)",
            "description": "Your development team uses a set of Compute Engine VMs for daily work. To minimize costs, you want these VMs to automatically shut down at the end of the workday and start up again in the morning, Monday through Friday. Which solution enables this automation?",
            "options": [
                "Use Preemptible VMs.",
                "Create a Managed Instance Group with a scale-down policy.",
                "Schedule VM instance start/stop using Cloud Scheduler and gcloud commands.",
                "Use custom machine types."
            ],
            "correctAnswer": "Schedule VM instance start/stop using Cloud Scheduler and gcloud commands.",
            "feedback": {
                "Use Preemptible VMs.": "Incorrect. These can be terminated at any time and would be very disruptive to developers' work.",
                "Create a Managed Instance Group with a scale-down policy.": "Incorrect. Autoscaling is based on load (like CPU utilization), not a fixed time schedule.",
                "Schedule VM instance start/stop using Cloud Scheduler and gcloud commands.": "Correct! This is the best way to automate cost savings. Cloud Scheduler can trigger a Cloud Function or a Pub/Sub topic on a cron schedule to run `gcloud` commands that automatically stop and start the VMs.",
                "Use custom machine types.": "Incorrect. This helps right-size resources but does not save money when the VMs are idle."
            }
        },
        {
            "title": "Q169: Securing API Access with Rate Limiting and Monitoring",
            "description": "You have multiple internal and external APIs. You need a centralized solution to manage access, enforce rate limiting to prevent abuse, and collect detailed analytics on API usage. Which service should you use?",
            "options": [
                "Cloud Load Balancing",
                "API Gateway",
                "Cloud DNS",
                "Identity-Aware Proxy (IAP)"
            ],
            "correctAnswer": "API Gateway",
            "feedback": {
                "Cloud Load Balancing": "Incorrect. A load balancer distributes traffic but lacks key API management features like rate limiting, API key validation, and detailed usage analytics.",
                "API Gateway": "Correct! API Gateway is a fully managed service designed for this exact purpose. It provides a single, secure entry point for your APIs and allows you to centrally manage authentication, authorization, rate limiting, and monitoring.",
                "Cloud DNS": "Incorrect. This is a DNS service and is unrelated to API management.",
                "Identity-Aware Proxy (IAP)": "Incorrect. IAP is for securing user access to web applications, not for general-purpose API management."
            }
        },
        {
            "title": "Q170: Real-time Fraud Detection Pipeline",
            "description": "Your financial service generates continuous transaction data. You need to build a real-time fraud detection pipeline that ingests this data, processes it for patterns, and flags suspicious transactions for immediate review. Which combination of services should you use?",
            "options": [
                "Cloud Storage and BigQuery ML.",
                "Pub/Sub and Dataflow.",
                "Cloud SQL and Dataproc.",
                "Cloud Bigtable and Cloud Functions."
            ],
            "correctAnswer": "Pub/Sub and Dataflow.",
            "feedback": {
                "Cloud Storage and BigQuery ML.": "Incorrect. This is a batch processing pattern. You would analyze data after it has been stored, not in real-time as it arrives.",
                "Pub/Sub and Dataflow.": "Correct! This is the canonical architecture. Pub/Sub ingests the real-time transaction stream. Dataflow consumes the stream, allowing you to apply complex logic or machine learning models in real-time to detect fraudulent patterns as they happen.",
                "Cloud SQL and Dataproc.": "Incorrect. This combination is not suited for real-time stream processing.",
                "Cloud Bigtable and Cloud Functions.": "Incorrect. While a function could process a single event, Dataflow is the service designed for building complex, stateful streaming pipelines required for this kind of analysis."
            }
        },
        {
            "title": "Q171: Choosing a Database for Globally Distributed Gaming (High Read/Write, Low Latency)",
            "description": "You are developing a globally distributed multiplayer online game. You need a database that can handle extremely high read and write volumes with very low latency from players worldwide, while ensuring strong consistency across all game states. Which database should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Firestore"
            ],
            "correctAnswer": "Google Cloud Spanner",
            "feedback": {
                "Google Cloud SQL": "Incorrect. Cloud SQL is a regional database and cannot provide low-latency access or strong consistency on a global scale.",
                "Google Cloud Spanner": "Correct! Spanner is uniquely suited for this. It's a globally distributed, relational database that provides strong transactional consistency with low latency for reads and writes from anywhere in the world, which is critical for managing a consistent game state for all players.",
                "Google Cloud Bigtable": "Incorrect. Bigtable is designed for high throughput but does not provide the strong transactional consistency needed to manage complex, interrelated game states.",
                "Google Cloud Firestore": "Incorrect. While good for mobile apps, Firestore's consistency model is not as strong as Spanner's on a global scale, making it less suitable for a critical, competitive game state."
            }
        },
        {
            "title": "Q172: Automated Provisioning of Production Infrastructure",
            "description": "Your operations team wants to fully automate the provisioning and management of your production environment (networks, Compute Engine instances, databases, load balancers). They require a declarative approach that integrates with version control for repeatability and auditing. Which Google Cloud tool should they use?",
            "options": [
                "Cloud Console",
                "gcloud CLI",
                "Cloud Deployment Manager",
                "Cloud Shell"
            ],
            "correctAnswer": "Cloud Deployment Manager",
            "feedback": {
                "Cloud Console": "Incorrect. The console is for manual operations and does not support defining infrastructure as code.",
                "gcloud CLI": "Incorrect. The CLI uses imperative commands. Infrastructure as Code (IaC) requires a declarative approach where you define the desired state.",
                "Cloud Deployment Manager": "Correct! Deployment Manager is Google's native IaC service. It allows you to write declarative templates that define your infrastructure, which can be stored in Git and used to automate deployments consistently.",
                "Cloud Shell": "Incorrect. Cloud Shell is an interactive shell environment, not an IaC automation tool."
            }
        },
        {
            "title": "Q173: Centralized DNS for Hybrid Cloud Environments",
            "description": "Your company has internal applications running both on-premises and on Google Cloud. You need a centralized DNS solution that can resolve hostnames for both environments consistently and securely for internal services. Which Google Cloud DNS feature enables this?",
            "options": [
                "Public zones",
                "Managed private zones with forwarding",
                "DNSSEC",
                "DNS peering"
            ],
            "correctAnswer": "Managed private zones with forwarding",
            "feedback": {
                "Public zones": "Incorrect. Public zones are for resolving public domain names on the internet, not for internal, hybrid-cloud hostnames.",
                "Managed private zones with forwarding": "Correct! You can create a private zone in Cloud DNS for your cloud resources. Then, you can configure DNS forwarding to send any queries for your on-premises domains to your on-prem DNS servers, creating a unified resolution system.",
                "DNSSEC": "Incorrect. This is a security feature to prevent DNS spoofing and does not help with hybrid name resolution.",
                "DNS peering": "Incorrect. DNS peering is for allowing two different VPCs to resolve each other's private DNS names, not for connecting a VPC to an on-premises DNS system."
            }
        },
        {
            "title": "Q174: Securely Sharing Data with Third-Party via Signed URLs",
            "description": "You need to provide a third-party vendor with temporary, read-only access to a specific large file in a Cloud Storage bucket for a limited time. You do not want to grant them any IAM roles. Which method should you use?",
            "options": [
                "Make the file publicly readable.",
                "Share the file via gsutil cp.",
                "Generate a signed URL for the file.",
                "Grant roles/storage.objectViewer to the vendor's service account."
            ],
            "correctAnswer": "Generate a signed URL for the file.",
            "feedback": {
                "Make the file publicly readable.": "Incorrect. This is insecure and provides permanent public access to anyone with the link.",
                "Share the file via gsutil cp.": "Incorrect. This is a command to copy files; it is not an access control mechanism.",
                "Generate a signed URL for the file.": "Correct! A signed URL is a time-limited, cryptographically signed link that grants temporary access to a specific object. This is the perfect solution for secure, temporary sharing without creating user accounts or assigning IAM roles.",
                "Grant roles/storage.objectViewer to the vendor's service account.": "Incorrect. This would grant them a persistent IAM role, which violates the requirement of not granting any IAM roles."
            }
        },
        {
            "title": "Q175: Centralized Logging for Compliance Across Projects",
            "description": "Your organization needs to collect all audit logs and application logs from multiple GCP projects into a central BigQuery dataset for long-term retention and compliance analysis. Which service should you use to route these logs?",
            "options": [
                "Cloud Storage",
                "Cloud Logging",
                "Cloud Pub/Sub",
                "Cloud Audit Logs"
            ],
            "correctAnswer": "Cloud Logging",
            "feedback": {
                "Cloud Storage": "Incorrect. Is a destination for logs, not the routing service.",
                "Cloud Logging": "Correct! Cloud Logging (specifically Log Sinks configured at the Organization or Folder level) can centralize and route logs from multiple projects to destinations like BigQuery for analysis and archiving.",
                "Cloud Pub/Sub": "Incorrect. A messaging service, can transport logs but Cloud Logging is the dedicated routing service for BigQuery.",
                "Cloud Audit Logs": "Incorrect. A type of log, not a service for routing."
            }
        },
        {
            "title": "Q176: High Throughput Data Ingestion for Time-Series Database",
            "description": "Your application collects massive amounts of time-series sensor data that needs to be ingested at extremely high throughput and stored in a database that supports fast lookups by timestamp and device ID. Which database should you choose for this?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Firestore"
            ],
            "correctAnswer": "Google Cloud Bigtable",
            "feedback": {
                "Google Cloud SQL": "Incorrect. Relational, struggles with extreme scale and throughput for raw time-series data.",
                "Google Cloud Spanner": "Incorrect. For strong ACID transactions globally, not typically chosen for raw high-throughput time-series ingestion.",
                "Google Cloud Bigtable": "Correct! Optimized for very high write throughput and low-latency reads on time-series data. Its design with row keys (e.g., combining device ID and timestamp) is ideal for this use case.",
                "Google Cloud Firestore": "Incorrect. Document database, better for app data with flexible schema, not for extreme time-series throughput."
            }
        },
        {
            "title": "Q177: Automating Application Deployment to GKE (Managed Delivery)",
            "description": "Your development team uses GKE for their containerized applications. They want a fully managed service that automates the deployment process to multiple environments (dev, staging, prod) with capabilities for progressive rollouts (e.g., canary, blue/green) and rollbacks. Which service should you use?",
            "options": [
                "Cloud Build",
                "Cloud Deploy",
                "Cloud Functions",
                "Compute Engine"
            ],
            "correctAnswer": "Cloud Deploy",
            "feedback": {
                "Cloud Build": "Incorrect. For CI (building/testing images), not CD to GKE clusters.",
                "Cloud Deploy": "Correct! Fully managed continuous delivery service for GKE (and Cloud Run) providing multi-environment deployment, progressive rollouts (canary/blue-green), and rollback capabilities.",
                "Cloud Functions": "Incorrect. For event-driven functions, not managing GKE deployments.",
                "Compute Engine": "Incorrect. IaaS, doesn't provide managed deployment automation for GKE."
            }
        },
{
            "title": "Q178: Choosing a Database for Real-time Leaderboards (Simple Schema)",
            "description": "You are building a real-time leaderboard for a mobile game. The data involves player ID, score, and a timestamp. It needs to handle millions of writes per second and allow for fast retrieval of top scores. Which database is most suitable?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Firestore"
            ],
            "correctAnswer": "Google Cloud Bigtable",
            "feedback": {
                "Google Cloud SQL": "Incorrect. Relational, struggles with extreme high write throughput for leaderboards.",
                "Google Cloud Spanner": "Incorrect. Designed for strong ACID transactions, overkill and not cost-optimized for pure high-throughput leaderboard writes.",
                "Google Cloud Bigtable": "Correct! Optimized for very high write and read throughput for large datasets with simple key-value or wide-column access patterns, making it ideal for real-time leaderboards.",
                "Google Cloud Firestore": "Incorrect. Document database, typically for smaller transactions/app data, not extreme scale leaderboards."
            }
        },
        {
            "title": "Q179: Multi-Region Disaster Recovery for Cloud SQL (Active/Passive)",
            "description": "Your critical application uses Cloud SQL for MySQL. You need a disaster recovery strategy that ensures the lowest possible RPO and RTO in case of a regional outage, leveraging an active/passive setup across regions. What should you configure?",
            "options": [
                "Automated daily backups with point-in-time recovery.",
                "Cross-region read replica with manual failover.",
                "High Availability (HA) with cross-region replication.",
                "Export database regularly to Cloud Storage and restore manually."
            ],
            "correctAnswer": "High Availability (HA) with cross-region replication.",
            "feedback": {
                "Automated daily backups with point-in-time recovery.": "Incorrect. Higher RPO/RTO.",
                "Cross-region read replica with manual failover.": "Incorrect. Asynchronous (potential data loss) and manual failover (higher RTO).",
                "High Availability (HA) with cross-region replication.": "Correct! This setup (with a suitable failover mechanism or a separate instance in another region configured for HA) provides synchronous or near-synchronous replication for near-zero RPO and automatic/fast RTO in minutes, ideal for cross-region active/passive DR.",
                "Export database regularly to Cloud Storage and restore manually.": "Incorrect. Highest RPO and RTO."
            }
        },
        {
            "title": "Q180: Firewall Rule for Egress Traffic to Specific IP Range",
            "description": "You need to create a firewall rule in your GCP VPC network that allows outgoing HTTP traffic from your VMs only to a specific destination IP address range: 203.0.113.0/24. Which configuration should you use for the firewall rule?",
            "options": [
                "Direction: Ingress, Source IP ranges: 203.0.113.0/24, Protocols and ports: tcp:80",
                "Direction: Egress, Destination IP ranges: 203.0.113.0/24, Protocols and ports: tcp:80",
                "Direction: Ingress, Destination IP ranges: 203.0.113.0/24, Protocols and ports: tcp:80",
                "Direction: Egress, Source IP ranges: 203.0.113.0/24, Protocols and ports: tcp:80"
            ],
            "correctAnswer": "Direction: Egress, Destination IP ranges: 203.0.113.0/24, Protocols and ports: tcp:80",
            "feedback": {
                "Direction: Ingress, Source IP ranges: 203.0.113.0/24, Protocols and ports: tcp:80": "Incorrect. Ingress is incoming.",
                "Direction: Egress, Destination IP ranges: 203.0.113.0/24, Protocols and ports: tcp:80": "Correct! Egress for outgoing; Destination IP ranges specifies where the allowed traffic goes.",
                "Direction: Ingress, Destination IP ranges: 203.0.113.0/24, Protocols and ports: tcp:80": "Incorrect. Ingress is incoming.",
                "Direction: Egress, Source IP ranges: 203.0.113.0/24, Protocols and ports: tcp:80": "Incorrect. Source specifies origin, not destination for egress."
            }
        },
        {
            "title": "Q181: Cost Optimization for Infrequent ML Model Training",
            "description": "Your data science team uses Compute Engine instances to train machine learning models. These training jobs are run occasionally (e.g., once a week) and can be stopped and restarted without losing progress. You want to minimize the cost of these training instances. What type of Compute Engine instance should you recommend?",
            "options": [
                "Standard instances",
                "Preemptible instances",
                "Custom machine types",
                "Sole-tenant nodes"
            ],
            "correctAnswer": "Preemptible instances",
            "feedback": {
                "Standard instances": "Incorrect. Not cost-optimized for interruptible workloads.",
                "Preemptible instances": "Correct! Preemptible VMs are significantly cheaper and ideal for fault-tolerant, interruptible workloads like ML model training that can checkpoint progress.",
                "Custom machine types": "Incorrect. Helps right-size resources but doesn't offer the cost savings of preemptible for interruptible jobs.",
                "Sole-tenant nodes": "Incorrect. Very expensive, for dedicated hardware needs."
            }
        },
        {
            "title": "Q182: Centralized User Access Management for SaaS Applications",
            "description": "Your organization uses Cloud Identity for its workforce identities. You need to enable single sign-on (SSO) for your employees to access various third-party Software-as-a-Service (SaaS) applications. Which feature of Cloud Identity should you leverage?",
            "options": [
                "Custom Roles",
                "Directory Sync",
                "Identity Platform",
                "SAML-based SSO"
            ],
            "correctAnswer": "SAML-based SSO",
            "feedback": {
                "Custom Roles": "Incorrect. For managing access to GCP resources, not for SSO to SaaS apps.",
                "Directory Sync": "Incorrect. For synchronizing on-premises directories to Cloud Identity, not for SSO to SaaS apps.",
                "Identity Platform": "Incorrect. For customer-facing identity management.",
                "SAML-based SSO": "Correct! Cloud Identity supports SAML (Security Assertion Markup Language) 2.0, allowing you to configure SSO with many third-party SaaS applications using Cloud Identity as the Identity Provider (IdP)."
            }
        },
        {
            "title": "Q183: Monitoring Application Health and Logs (Specific Resource)",
            "description": "You are responsible for a specific microservice running on Compute Engine. You need to monitor its CPU utilization, memory usage, and collect its application logs to troubleshoot issues. Which Google Cloud service should you primarily use?",
            "options": [
                "Cloud Source Repositories",
                "Cloud Monitoring",
                "Cloud Trace",
                "Cloud Audit Logs"
            ],
            "correctAnswer": "Cloud Monitoring",
            "feedback": {
                "Cloud Source Repositories": "Incorrect. For code versioning.",
                "Cloud Monitoring": "Correct! Cloud Monitoring is the primary service for collecting and viewing metrics (CPU, memory) and logs for individual Compute Engine instances and applications.",
                "Cloud Trace": "Incorrect. For distributed tracing (latency), not general resource health.",
                "Cloud Audit Logs": "Incorrect. For auditing GCP resource access, not application performance/logs."
            }
        },
        {
            "title": "Q184: Data Migration from On-Prem Files to Cloud Storage (Scheduled)",
            "description": "Your company needs to regularly transfer large volumes of files from an on-premises file server to a Cloud Storage bucket on a scheduled basis (e.g., daily). You need an automated and managed solution. Which service should you use?",
            "options": [
                "gsutil rsync",
                "Storage Transfer Service",
                "Cloud Pub/Sub",
                "Cloud Functions"
            ],
            "correctAnswer": "Storage Transfer Service",
            "feedback": {
                "gsutil rsync": "Incorrect. Command-line tool, requires running on a VM and manual scripting for automation.",
                "Storage Transfer Service": "Correct! Supports scheduled and automated transfers from on-premises file systems (via agents) to Cloud Storage. It's fully managed and reliable for recurring large transfers.",
                "Cloud Pub/Sub": "Incorrect. For streaming data, not for scheduled file transfers.",
                "Cloud Functions": "Incorrect. Can trigger, but not the primary service for large-scale, automated file transfers from on-premises."
            }
        },
        {
            "title": "Q185: Choosing a Database for Financial Transaction History (Audit Trail)",
            "description": "Your financial application generates a continuous stream of transaction records that need to be stored as an immutable, append-only audit trail. This data is written at high volume and primarily accessed via range queries for historical analysis. Consistency is important but not globally distributed strong ACID. Which database should you choose?",
            "options": [
                "Google Cloud SQL",
                "Google Cloud Spanner",
                "Google Cloud Bigtable",
                "Google Cloud Firestore"
            ],
            "correctAnswer": "Google Cloud Bigtable",
            "feedback": {
                "Google Cloud SQL": "Incorrect. Relational, not optimized for extreme append-only write throughput or immutability.",
                "Google Cloud Spanner": "Incorrect. Designed for strong global ACID transactions; may be overkill/costly for a pure append-only audit trail.",
                "Google Cloud Bigtable": "Correct! Optimized for very high write throughput and low-latency range queries on time-series/append-only data (like an audit trail), making it suitable for this use case.",
                "Google Cloud Firestore": "Incorrect. Document database, typically for smaller app data, not extreme write throughput of a large-scale audit trail."
            }
        },
        {
            "title": "Q186: Automating VM Configuration After Launch (Declarative)",
            "description": "You need to deploy new Compute Engine VMs with specific software packages and configurations applied after the VM instances are launched, using a declarative approach. You want to ensure consistency and minimize manual intervention. Which tool should you primarily use for post-launch configuration?",
            "options": [
                "Startup scripts",
                "SSH commands",
                "Configuration management tools (e.g., Puppet, Ansible, Chef)",
                "Cloud Functions"
            ],
            "correctAnswer": "Configuration management tools (e.g., Puppet, Ansible, Chef)",
            "feedback": {
                "Startup scripts": "Incorrect. Imperative; run only once on boot. Less robust for complex, evolving configurations or managing state.",
                "SSH commands": "Incorrect. Manual, not automated or declarative.",
                "Configuration management tools (e.g., Puppet, Ansible, Chef)": "Correct! These tools are designed for declarative post-launch configuration, enforcing desired state consistently across a fleet of VMs, ideal for automating software installation and configuration.",
                "Cloud Functions": "Incorrect. For event-driven tasks; not typically used for declarative configuration management of VMs."
            }
        },
        {
            "title": "Q187: Global Application with Low Latency for Dynamic Content",
            "description": "Your global web application delivers dynamic, personalized content. You need to minimize latency for users worldwide by serving content from the closest possible edge location. Which GCP service would directly help optimize latency for this dynamic content?",
            "options": [
                "Cloud Storage",
                "Cloud CDN",
                "External HTTP(S) Load Balancer",
                "Cloud DNS"
            ],
            "correctAnswer": "External HTTP(S) Load Balancer",
            "feedback": {
                "Cloud Storage": "Incorrect. Storage for static files, doesn't optimize delivery of dynamic content.",
                "Cloud CDN": "Incorrect. Primarily for caching static content; less effective for truly dynamic/personalized content.",
                "External HTTP(S) Load Balancer": "Correct! A Global External HTTP(S) Load Balancer uses Google's global network and Anycast IP to route user requests to the geographically closest healthy backend. This minimizes latency for dynamic content (which Cloud CDN wouldn't cache).",
                "Cloud DNS": "Incorrect. Resolves domain names, but doesn't handle content delivery or routing to the closest backend itself."
            }
        },
        {
            "title": "Q188: Disaster Recovery for Stateful Application (Low RTO/RPO)",
            "description": "Your critical, stateful application requires very low RPO and RTO in case of an instance or zone failure within a region. It uses a relational database. Which combination of features should you use to achieve high availability and disaster recovery for its database state?",
            "options": [
                "Daily backups and point-in-time recovery for the database.",
                "Database replication to another region.",
                "High Availability (HA) configuration for the database.",
                "Storing state in local VM disks."
            ],
            "correctAnswer": "High Availability (HA) configuration for the database.",
            "feedback": {
                "Daily backups and point-in-time recovery for the database.": "Incorrect. Higher RPO (up to 24hr data loss) and RTO (restore time).",
                "Database replication to another region.": "Incorrect. This helps with regional disaster, but doesn't inherently provide very low RTO/RPO for instance/zone failures within a region without specific HA setup.",
                "High Availability (HA) configuration for the database.": "Correct! Database HA (e.g., Cloud SQL HA, Spanner instances across zones) provides automatic failover to a standby within the region (or across zones), resulting in very low RTO and RPO for instance/zone failures.",
                "Storing state in local VM disks.": "Incorrect. Causes data loss/high RTO on instance failure."
            }
        },
        {
            "title": "Q189: Network Access Control by Identity (Internal Microservices)",
            "description": "You have internal microservices running on Compute Engine VMs. You need to restrict which microservices can communicate with each other (e.g., Service A can call Service B, but not Service C). This access control must be based on the identity of the calling service. Which networking feature should you use?",
            "options": [
                "Network Tags in Firewall Rules",
                "Source IP ranges in Firewall Rules",
                "Service Accounts in Firewall Rules",
                "VPC Service Controls"
            ],
            "correctAnswer": "Service Accounts in Firewall Rules",
            "feedback": {
                "Network Tags in Firewall Rules": "Incorrect. Less granular than service accounts, based on VM labels, not identity.",
                "Source IP ranges in Firewall Rules": "Incorrect. Based on IP addresses, not identity.",
                "Service Accounts in Firewall Rules": "Correct! VPC Firewall Rules allow you to specify source and/or target service accounts. This enables identity-based network access control between VMs/microservices.",
                "VPC Service Controls": "Incorrect. For perimeter security around managed services, not internal VM-to-VM identity-based access."
            }
        },
        {
            "title": "Q190: Migrating a Legacy Application to Serverless (Minimal Code Change)",
            "description": "Your company has a legacy web application. You want to migrate it to a serverless platform on Google Cloud to minimize operational overhead. The application uses a custom web server framework. You want to achieve this with the least amount of code change. Which service should you choose?",
            "options": [
                "Google App Engine Standard",
                "Google Kubernetes Engine (GKE)",
                "Cloud Run",
                "Google Cloud Functions"
            ],
            "correctAnswer": "Cloud Run",
            "feedback": {
                "Google App Engine Standard": "Incorrect. Often requires significant code changes to fit its specific runtime environments and APIs.",
                "Google Kubernetes Engine (GKE)": "Incorrect. While containerized, GKE is not fully serverless and requires Kubernetes knowledge/management; higher ops overhead than Cloud Run.",
                "Cloud Run": "Correct! Cloud Run is a fully managed, serverless platform for containerized applications. It supports arbitrary container images (including those with custom web server frameworks) and scales automatically, requiring minimal code changes for migration.",
                "Google Cloud Functions": "Incorrect. For event-driven functions, typically requires significant code refactoring for a full web application."
            }
        },
        {
            "title": "Q191: Multi-Cloud Data Analytics",
            "description": "Your company has data stored in Amazon S3 and Azure Blob Storage, in addition to Google Cloud Storage. You want to perform complex analytical queries across all these data sources without moving data to a single cloud. Which Google Cloud service enables this?",
            "options": [
                "Cloud Dataflow",
                "Cloud Composer",
                "BigQuery Omni",
                "BigQuery ML"
            ],
            "correctAnswer": "BigQuery Omni",
            "feedback": {
                "Cloud Dataflow": "Incorrect. For data processing pipelines, requires data movement or specific connectors.",
                "Cloud Composer": "Incorrect. For workflow orchestration.",
                "BigQuery Omni": "Correct! BigQuery Omni allows you to run SQL queries on data residing in other clouds (AWS S3, Azure Blob Storage) directly from BigQuery, without physically moving the data to Google Cloud.",
                "BigQuery ML": "Incorrect. For machine learning within BigQuery, assumes data is already in BigQuery."
            }
        }
    ];


    // --- SCRIPT LOGIC ---
    document.addEventListener('DOMContentLoaded', () => {
        // --- State Management ---
        let currentQuestionIndex = 0;
        let userAnswers = new Array(quizData.length).fill(null);
        let score = 0;
        let shuffledOptionsMap = new Map();

        // --- DOM Elements ---
        const mainHeader = document.getElementById('main-header');
        const quizContainer = document.getElementById('quiz-container');
        const navigationContainer = document.getElementById('navigation-container');
        const resultsContainer = document.getElementById('results-container');
        const reviewContainer = document.getElementById('review-container');
        const reviewContent = document.getElementById('review-content');
        const reviewTitle = document.getElementById('review-title');
        const prevBtn = document.getElementById('prev-btn');
        const nextBtn = document.getElementById('next-btn');
        const finishBtn = document.getElementById('finish-btn');
        const currentQSpan = document.getElementById('current-q-span');
        const totalQSpan = document.getElementById('total-q-span');
        const restartBtn = document.getElementById('restart-btn');
        const reviewAllBtn = document.getElementById('review-all-btn');
        const reviewFocusedBtn = document.getElementById('review-focused-btn');
        const backToScoreBtn = document.getElementById('back-to-score-btn');
        const scoreText = document.getElementById('score-text');
        const scoreSummary = document.getElementById('score-summary');
        const progressBar = document.getElementById('progress-bar');

        // --- Icon SVGs ---
        const checkIconSVG = `<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" class="text-green-600"><path fill-rule="evenodd" d="M2.25 12c0-5.385 4.365-9.75 9.75-9.75s9.75 4.365 9.75 9.75-4.365 9.75-9.75 9.75S2.25 17.385 2.25 12zm13.36-1.814a.75.75 0 10-1.06-1.06L10.5 13.19l-1.72-1.72a.75.75 0 00-1.06 1.06l2.25 2.25a.75.75 0 001.06 0l4.5-4.5z" clip-rule="evenodd" /></svg>`;
        const crossIconSVG = `<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" class="text-red-600"><path fill-rule="evenodd" d="M12 2.25c-5.385 0-9.75 4.365-9.75 9.75s4.365 9.75 9.75 9.75 9.75-4.365 9.75-9.75S17.385 2.25 12 2.25zm-1.72 6.97a.75.75 0 011.06 0L12 9.94l.72-.72a.75.75 0 111.06 1.06L13.06 12l.72.72a.75.75 0 11-1.06 1.06L12 13.06l-.72.72a.75.75 0 01-1.06-1.06l.72-.72-.72-.72a.75.75 0 010-1.06z" clip-rule="evenodd" /></svg>`;

        function shuffleArray(array) {
            const shuffled = [...array];
            for (let i = shuffled.length - 1; i > 0; i--) {
                const j = Math.floor(Math.random() * (i + 1));
                [shuffled[i], shuffled[j]] = [shuffled[j], shuffled[i]];
            }
            return shuffled;
        }

        function loadQuestion() {
            const question = quizData[currentQuestionIndex];
            const isAnswered = userAnswers[currentQuestionIndex] !== null;
            const questionType = question.type || 'MULTIPLE_CHOICE';
            
            if (!shuffledOptionsMap.has(currentQuestionIndex)) {
                shuffledOptionsMap.set(currentQuestionIndex, shuffleArray(question.options));
            }
            const currentShuffledOptions = shuffledOptionsMap.get(currentQuestionIndex);

            let optionsHtml = '';
            currentShuffledOptions.forEach((option, index) => {
                let isSelected = false;
                if (questionType === 'CHECKBOX') {
                    isSelected = userAnswers[currentQuestionIndex] && userAnswers[currentQuestionIndex] !== 'SKIPPED' && userAnswers[currentQuestionIndex].includes(option);
                } else {
                    isSelected = userAnswers[currentQuestionIndex] === option;
                }
                
                const optionLetter = String.fromCharCode(65 + index);

                optionsHtml += `
                    <div class="quiz-option mt-4 p-4 rounded-lg flex items-center" data-option="${option}">
                        <input type="${questionType === 'CHECKBOX' ? 'checkbox' : 'radio'}" name="option" class="mr-4 pointer-events-none h-5 w-5 bg-transparent border-slate-300 text-blue-600 focus:ring-blue-500" ${isSelected ? 'checked' : ''} ${isAnswered ? 'disabled' : ''}>
                        <label class="flex-1 pointer-events-none"><span class="font-bold mr-2">${optionLetter}.</span>${option}</label><div class="status-icon ml-auto"></div>
                    </div>
                `;
            });

            quizContainer.innerHTML = `
                <div class="flex flex-col lg:flex-row lg:gap-8">
                    <div class="lg:w-5/12 lg:pr-8">
                        <h2 class="text-2xl lg:text-3xl font-bold">${question.title}</h2>
                        <p class="text-slate-600 mt-4 text-base lg:text-lg">${question.description}</p>
                    </div>
                    <div class="lg:w-7/12 mt-6 lg:mt-0">
                        <div id="options-container" class="${isAnswered ? 'answered' : ''}">
                            ${optionsHtml}
                        </div>
                        ${questionType === 'CHECKBOX' && !isAnswered ? '<button id="submit-btn" class="btn btn-primary mt-6 w-full py-3 px-4">Submit Answer</button>' : ''}
                    </div>
                </div>
                <div id="feedback-container" class="mt-6"></div>
            `;
            
            updateProgress();
            updateButtonStates();
            
            if (isAnswered && userAnswers[currentQuestionIndex] !== 'SKIPPED') {
                showFeedback();
            }

            addOptionListeners();
        }

        function addOptionListeners() {
            const question = quizData[currentQuestionIndex];
            const questionType = question.type || 'MULTIPLE_CHOICE';

            document.querySelectorAll('.quiz-option').forEach(optionEl => {
                optionEl.addEventListener('click', (e) => {
                    if (userAnswers[currentQuestionIndex] !== null) return;

                    if (questionType === 'MULTIPLE_CHOICE') {
                        const selectedOption = optionEl.dataset.option;
                        userAnswers[currentQuestionIndex] = selectedOption;
                        showFeedback();
                    } else { 
                        const checkbox = e.currentTarget.querySelector('input');
                        checkbox.checked = !checkbox.checked;
                    }
                });
            });

            if (questionType === 'CHECKBOX') {
                const submitBtn = document.getElementById('submit-btn');
                if (submitBtn) {
                    submitBtn.addEventListener('click', () => {
                        const selectedOptions = Array.from(document.querySelectorAll('.quiz-option input:checked')).map(input => input.parentElement.dataset.option);
                        if (selectedOptions.length > 0) {
                            userAnswers[currentQuestionIndex] = selectedOptions;
                            showFeedback();
                            submitBtn.style.display = 'none';
                        } else {
                            alert("Please select at least one option.");
                        }
                    });
                }
            }
        }

        function showFeedback() {
            const question = quizData[currentQuestionIndex];
            const userAnswer = userAnswers[currentQuestionIndex];
            const optionsContainer = document.getElementById('options-container');
            const feedbackContainer = document.getElementById('feedback-container');
            
            if (!optionsContainer || userAnswer === 'SKIPPED' || userAnswer === null) return;

            optionsContainer.classList.add('answered');
            optionsContainer.querySelectorAll('input').forEach(input => input.disabled = true);

            let isCorrect = false;
            if (question.type === 'CHECKBOX') {
                const correctAnswers = new Set(question.correctAnswers);
                const selectedAnswers = new Set(userAnswer || []);
                isCorrect = correctAnswers.size === selectedAnswers.size && [...correctAnswers].every(val => selectedAnswers.has(val));
            } else {
                isCorrect = userAnswer === question.correctAnswer;
            }

            document.querySelectorAll('.quiz-option').forEach(optionEl => {
                const optionText = optionEl.dataset.option;
                const statusIconEl = optionEl.querySelector('.status-icon');

                if (question.type === 'CHECKBOX') {
                    const isCorrectChoice = question.correctAnswers.includes(optionText);
                    const isSelected = (userAnswer || []).includes(optionText);

                    if (isCorrectChoice) {
                        optionEl.classList.add('correct');
                        if (isSelected) statusIconEl.innerHTML = checkIconSVG;
                    }
                    if (isSelected && !isCorrectChoice) {
                        optionEl.classList.add('incorrect');
                        statusIconEl.innerHTML = crossIconSVG;
                    }
                } else {
                    if (optionText === question.correctAnswer) {
                        optionEl.classList.add('correct');
                        statusIconEl.innerHTML = checkIconSVG;
                    } else if (optionText === userAnswer) {
                        optionEl.classList.add('incorrect');
                        statusIconEl.innerHTML = crossIconSVG;
                    }
                }
            });
            
            let feedbackHtml = `<p class="mt-2 text-sm font-semibold text-slate-700">Explanations:</p>`;
            
            const currentShuffledOptions = shuffledOptionsMap.get(currentQuestionIndex);
            currentShuffledOptions.forEach((option, index) => {
                const explanation = question.feedback[option] || "No explanation available.";
                let isCorrectOption = false;
                const optionLetter = String.fromCharCode(65 + index);

                if (question.type === 'CHECKBOX') {
                    isCorrectOption = question.correctAnswers.includes(option);
                } else {
                    isCorrectOption = (option === question.correctAnswer);
                }

                const feedbackItemHtml = `<span class="font-bold mr-2">${optionLetter}.</span> ${explanation}`;
                const feedbackClass = isCorrectOption 
                    ? 'bg-green-50 border-l-4 border-green-500 text-green-800' 
                    : 'bg-red-50 border-l-4 border-red-500 text-red-800';

                feedbackHtml += `<div class="mt-2 text-sm p-3 rounded ${feedbackClass}">${feedbackItemHtml}</div>`;
            });

            const feedbackColor = isCorrect ? 'bg-green-50/50 border-green-500' : 'bg-red-50/50 border-red-500';
            const feedbackTitle = isCorrect ? 'üéâ Correct! üéâ' : 'ü§î Not Quite...';
            
            feedbackContainer.innerHTML = `
                <div class="feedback-card p-6 border-l-4 rounded-r-lg ${feedbackColor}">
                    <h3 class="font-bold text-lg text-slate-800">${feedbackTitle}</h3>
                    <div class="feedback-explanations">${feedbackHtml}</div>
                </div>
            `;
            setTimeout(() => {
                const card = document.querySelector('.feedback-card');
                if(card) card.classList.add('show');
            }, 10);
        }

        function updateProgress() {
            const progressPercent = ((currentQuestionIndex) / quizData.length) * 100;
            progressBar.style.width = `${progressPercent}%`;
            currentQSpan.textContent = currentQuestionIndex + 1;
            totalQSpan.textContent = quizData.length;
        }

        function updateButtonStates() {
            prevBtn.disabled = currentQuestionIndex === 0;
            if (currentQuestionIndex === quizData.length - 1) {
                nextBtn.textContent = 'Finish Quiz';
            } else {
                nextBtn.textContent = 'Next';
            }
        }

        function showResults() {
            score = 0;
            userAnswers.forEach((userAnswer, index) => {
                if (userAnswer !== null && userAnswer !== 'SKIPPED') {
                    const question = quizData[index];
                    if (question.type === 'CHECKBOX') {
                        const correctAnswers = new Set(question.correctAnswers);
                        const selectedAnswers = new Set(userAnswer || []);
                        if (correctAnswers.size === selectedAnswers.size && [...correctAnswers].every(val => selectedAnswers.has(val))) {
                            score++;
                        }
                    } else {
                        if (userAnswer === question.correctAnswer) {
                            score++;
                        }
                    }
                }
            });

            const percentage = quizData.length > 0 ? Math.round((score / quizData.length) * 100) : 0;
            scoreText.textContent = `${percentage}%`;
            scoreSummary.textContent = `You scored ${score} out of ${quizData.length} total questions.`;

            quizContainer.classList.add('hidden');
            navigationContainer.classList.add('hidden');
            mainHeader.classList.add('hidden');
            resultsContainer.classList.remove('hidden');
        }

        function showReview(isFocused) {
            resultsContainer.classList.add('hidden');
            reviewContainer.classList.remove('hidden');
            reviewTitle.textContent = isFocused ? 'üéØ Focused Review (Incorrect & Skipped)' : 'üìã Review All Answers';
            
            let reviewHtml = '';
            quizData.forEach((question, index) => {
                const userAnswer = userAnswers[index];
                const wasAnswered = userAnswer !== null && userAnswer !== 'SKIPPED';
                const questionType = question.type || 'MULTIPLE_CHOICE';
                let isCorrect = false;

                if (wasAnswered) {
                    if (questionType === 'CHECKBOX') {
                        const correctAnswers = new Set(question.correctAnswers);
                        const selectedAnswers = new Set(userAnswer || []);
                        isCorrect = correctAnswers.size === selectedAnswers.size && [...correctAnswers].every(val => selectedAnswers.has(val));
                    } else {
                        isCorrect = userAnswer === question.correctAnswer;
                    }
                }

                // In focused mode, skip questions that were answered correctly
                if (isFocused && isCorrect) {
                    return;
                }
                
                const resultIcon = wasAnswered 
                    ? (isCorrect ? `<span class="text-green-600 font-bold ml-2">‚úî Correct</span>` : `<span class="text-red-600 font-bold ml-2">‚úñ Incorrect</span>`)
                    : `<span class="text-slate-500 font-bold ml-2">‚óè Skipped</span>`;

                let optionsReviewHtml = '<div class="mt-4 space-y-2">';
                question.options.forEach(option => {
                    const explanation = question.feedback[option] || "No explanation available.";
                    let optionClass = '';
                    let selectionIndicator = '';
                    let isCorrectChoice = false;

                    // Determine if the current option is a correct answer
                    if (questionType === 'CHECKBOX') {
                        isCorrectChoice = question.correctAnswers.includes(option);
                    } else { // MULTIPLE_CHOICE
                        isCorrectChoice = (option === question.correctAnswer);
                    }
                    
                    const isSelected = wasAnswered && (questionType === 'CHECKBOX' ? (userAnswer || []).includes(option) : userAnswer === option);

                    // Logic for showing options in different review modes
                    if (!isFocused || (isFocused && (isCorrectChoice || isSelected))) {
                        if (isCorrectChoice) {
                            optionClass = 'review-option-correct'; // Green for correct
                        } else {
                            optionClass = 'review-option-incorrect'; // Red for ALL incorrect answers
                        }

                        if (isSelected) {
                            selectionIndicator = `<span class="font-bold text-blue-700 ml-2">[Your Selection]</span>`;
                        }

                        optionsReviewHtml += `
                            <div class="review-option ${optionClass}">
                                <p class="font-semibold text-slate-800">${option} ${selectionIndicator}</p>
                                <p class="text-sm text-slate-600 mt-1">${explanation}</p>
                            </div>`;
                    }
                });
                optionsReviewHtml += '</div>';

                reviewHtml += `
                    <div class="border-t border-slate-200 pt-6">
                        <p class="font-semibold text-lg text-slate-800">${question.title}${resultIcon}</p>
                        <p class="text-slate-600 mt-2">${question.description}</p>
                        ${optionsReviewHtml}
                    </div>
                `;
            });

            if (isFocused && reviewHtml === '') {
                reviewHtml = `<p class="text-center text-lg text-green-700 font-semibold">Congratulations! You answered every question correctly!</p>`;
            }

            reviewContent.innerHTML = reviewHtml;
        }


        function restartQuiz() {
            currentQuestionIndex = 0;
            userAnswers.fill(null);
            score = 0;
            shuffledOptionsMap.clear();
            
            resultsContainer.classList.add('hidden');
            reviewContainer.classList.add('hidden');
            quizContainer.classList.remove('hidden');
            navigationContainer.classList.remove('hidden');
            mainHeader.classList.remove('hidden');
            
            loadQuestion();
        }

        // --- Event Listeners ---
        prevBtn.addEventListener('click', () => {
            if (currentQuestionIndex > 0) {
                currentQuestionIndex--;
                loadQuestion();
            }
        });

        nextBtn.addEventListener('click', () => {
            if (userAnswers[currentQuestionIndex] === null) {
                // Mark as skipped if no answer is provided for checkbox or radio before clicking next
                const questionType = quizData[currentQuestionIndex].type || 'MULTIPLE_CHOICE';
                if(questionType === 'CHECKBOX') {
                    const selectedOptions = Array.from(document.querySelectorAll('.quiz-option input:checked'));
                    if (selectedOptions.length === 0) {
                        userAnswers[currentQuestionIndex] = 'SKIPPED';
                    }
                } else {
                     userAnswers[currentQuestionIndex] = 'SKIPPED';
                }
            }

            if (currentQuestionIndex < quizData.length - 1) {
                currentQuestionIndex++;
                loadQuestion();
            } else {
                showResults();
            }
        });

        finishBtn.addEventListener('click', () => {
             if (confirm('Are you sure you want to finish the quiz? Your current progress will be scored.')) {
                showResults();
            }
        });
        
        restartBtn.addEventListener('click', restartQuiz);
        reviewAllBtn.addEventListener('click', () => showReview(false));
        reviewFocusedBtn.addEventListener('click', () => showReview(true));
        backToScoreBtn.addEventListener('click', () => {
            reviewContainer.classList.add('hidden');
            resultsContainer.classList.remove('hidden');
        });

        // --- Initial Load ---
        if (typeof quizData === 'undefined' || quizData.length === 0) {
            quizContainer.innerHTML = `<p class="text-red-500 font-bold text-center">Error: Could not load questions.</p>`;
            return;
        }
        loadQuestion();
    });
</script>
</body>
</html>
